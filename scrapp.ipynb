{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 2 - Limpieza y Transformaci√≥n de Datos con Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliotecas utilizadas\n",
    "\n",
    "!pip install scrapy\n",
    "\n",
    "\n",
    "!pip install ipykernel\n",
    "\n",
    "!pip install webdriver-manager\n",
    "\n",
    "!pip install nbconvert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T20:00:03.313839Z",
     "iopub.status.busy": "2025-03-22T20:00:03.313377Z",
     "iopub.status.idle": "2025-03-22T20:00:03.323071Z",
     "shell.execute_reply": "2025-03-22T20:00:03.321619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La carpeta 'datalake/1_LANDING_ZONE' ya existe.\n",
      "La carpeta 'datalake/2_REFINED_ZONE' ya existe.\n",
      "La carpeta 'datalake/3_CONSUMPTION_ZONE' ya existe.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Carpetas a crear\n",
    "folders = [\n",
    "    \"datalake/1_LANDING_ZONE\",\n",
    "    \"datalake/2_REFINED_ZONE\",\n",
    "    \"datalake/3_CONSUMPTION_ZONE\",   \n",
    "]\n",
    "\n",
    "# Crear carpetas\n",
    "for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        print(f\"Carpeta '{folder}' creada.\")\n",
    "    else:\n",
    "        print(f\"La carpeta '{folder}' ya existe.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Scrapy Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener informacion de las empresas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoria 1 - Bancos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T20:00:03.366348Z",
     "iopub.status.busy": "2025-03-22T20:00:03.366111Z",
     "iopub.status.idle": "2025-03-22T20:00:31.457454Z",
     "shell.execute_reply": "2025-03-22T20:00:31.455208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datos extra√≠dos correctamente:\n",
      "[\n",
      "    {\n",
      "        \"nombre\": \"Aplazame\",\n",
      "        \"ubicacion\": \"Calle Ulises 16-18, Madrid, Espa√±a\",\n",
      "        \"puntuacion\": 4.8,\n",
      "        \"pagina_web\": \"https://aplazame.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Bnext\",\n",
      "        \"ubicacion\": \"Calle de Zurbano, 71, Madrid, Espa√±a\",\n",
      "        \"puntuacion\": 1.5,\n",
      "        \"pagina_web\": \"https://www.bnext.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"CaixaBank\",\n",
      "        \"ubicacion\": \"Calle Pintor Sorolla, 2-4, Valencia, Espa√±a\",\n",
      "        \"puntuacion\": 1.2,\n",
      "        \"pagina_web\": \"https://www.caixabank.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Crealsa\",\n",
      "        \"ubicacion\": \"Carrer de Menorca 19, Planta 7¬™ (Edificio Aqua), Val√®ncia, Espa√±a\",\n",
      "        \"puntuacion\": 4.4,\n",
      "        \"pagina_web\": \"https://www.crealsa.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"BBVA Espa√±a\",\n",
      "        \"ubicacion\": \"Espa√±a\",\n",
      "        \"puntuacion\": 1.3,\n",
      "        \"pagina_web\": \"https://bbva.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Banco Sabadell\",\n",
      "        \"ubicacion\": \"Espa√±a\",\n",
      "        \"puntuacion\": 1.2,\n",
      "        \"pagina_web\": \"https://bancsabadell.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"ING Espa√±a\",\n",
      "        \"ubicacion\": \"Calle V√≠a de los Poblados, Madrid, Espa√±a\",\n",
      "        \"puntuacion\": 1.2,\n",
      "        \"pagina_web\": \"https://ing.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Cetelem Espa√±a\",\n",
      "        \"ubicacion\": \"Espa√±a\",\n",
      "        \"puntuacion\": 4.2,\n",
      "        \"pagina_web\": \"https://www.cetelem.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"EVO Banco\",\n",
      "        \"ubicacion\": \"Don Ram√≥n de la Cruz 84, Madrid, Espa√±a\",\n",
      "        \"puntuacion\": 3.1,\n",
      "        \"pagina_web\": \"https://evobanco.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Santander Espa√±a\",\n",
      "        \"ubicacion\": \"Espa√±a\",\n",
      "        \"puntuacion\": 1.3,\n",
      "        \"pagina_web\": \"https://www.bancosantander.es\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re, json, os\n",
    "\n",
    "# ----------------------------------\n",
    "# Configuraci√≥n de Chrome\n",
    "# ----------------------------------\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--window-size=1280,2000\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "URL = \"https://es.trustpilot.com/categories/bank?sort=reviews_count\"\n",
    "driver.get(URL)\n",
    "\n",
    "# Espera a que aparezcan las tarjetas de empresa\n",
    "wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a[name=\"business-unit-card\"]')))\n",
    "empresas = driver.find_elements(By.CSS_SELECTOR, 'a[name=\"business-unit-card\"]')[:10]\n",
    "\n",
    "datos = []\n",
    "\n",
    "# ----------------------------------\n",
    "# Helpers\n",
    "# ----------------------------------\n",
    "def limpiar_texto(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "def get_first_text(el, selector_list):\n",
    "    \"\"\"Prueba varios selectores y devuelve el primer innerText v√°lido.\"\"\"\n",
    "    for sel in selector_list:\n",
    "        try:\n",
    "            x = el.find_element(By.CSS_SELECTOR, sel)\n",
    "            txt = x.get_attribute(\"innerText\") or x.text\n",
    "            txt = limpiar_texto(txt)\n",
    "            if txt:\n",
    "                return txt\n",
    "        except:\n",
    "            continue\n",
    "    return \"\"\n",
    "\n",
    "def parse_puntuacion(txt: str):\n",
    "    \"\"\"\n",
    "    Convierte '4,8' o '4.8' a float 4.8.\n",
    "    Si llega '4,8/5' tambi√©n funciona.\n",
    "    \"\"\"\n",
    "    if not txt:\n",
    "        return \"N/A\"\n",
    "    m = re.search(r\"(\\d+[.,]\\d+|\\d+)\", txt)\n",
    "    if not m:\n",
    "        return \"N/A\"\n",
    "    num = m.group(1).replace(\",\", \".\")\n",
    "    try:\n",
    "        return float(num)\n",
    "    except:\n",
    "        return \"N/A\"\n",
    "\n",
    "# ----------------------------------\n",
    "# Scraping\n",
    "# ----------------------------------\n",
    "for empresa in empresas:\n",
    "    # --- NOMBRE ---\n",
    "    nombre = get_first_text(\n",
    "        empresa,\n",
    "        [\n",
    "            'p[class*=\"CDS_Typography_heading-\"]',\n",
    "            'p[class*=\"heading-\"]',\n",
    "            'p[class*=\"heading\"]',\n",
    "        ],\n",
    "    )\n",
    "    if not nombre:\n",
    "        nombre = \"N/A\"\n",
    "\n",
    "    # --- UBICACI√ìN ---\n",
    "    ubicacion = \"\"\n",
    "    # 1) Contenedor actual de ubicaci√≥n\n",
    "    try:\n",
    "        u = empresa.find_element(By.CSS_SELECTOR, 'div[class*=\"styles_businessLocation__\"] p')\n",
    "        ubicacion = limpiar_texto(u.get_attribute(\"innerText\") or u.text)\n",
    "    except:\n",
    "        ubicacion = \"\"\n",
    "\n",
    "    # 2) Fallback atributo data antiguo\n",
    "    if not ubicacion:\n",
    "        try:\n",
    "            uel = empresa.find_element(By.CSS_SELECTOR, 'span[data-business-location-typography=\"true\"]')\n",
    "            ubicacion = limpiar_texto(uel.get_attribute(\"innerText\") or uel.text)\n",
    "        except:\n",
    "            ubicacion = \"\"\n",
    "\n",
    "    # 3) Heur√≠stica general como √∫ltimo recurso\n",
    "    if not ubicacion:\n",
    "        try:\n",
    "            candidatos = empresa.find_elements(\n",
    "                By.CSS_SELECTOR,\n",
    "                'p[class*=\"CDS_Typography_body-\"], span[class*=\"CDS_Typography_body-\"], p[dir=\"auto\"]'\n",
    "            )\n",
    "            for c in candidatos:\n",
    "                t = limpiar_texto(c.get_attribute(\"innerText\") or c.text)\n",
    "                if t and (\n",
    "                    \",\" in t\n",
    "                    or re.search(r\"\\d\", t)\n",
    "                    or re.search(r\"(Espa√±a|Spain|Madrid|Barcelona|Valencia|Sevilla|Bilbao)\", t, re.I)\n",
    "                ):\n",
    "                    ubicacion = t\n",
    "                    break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if not ubicacion:\n",
    "        ubicacion = \"N/A\"\n",
    "\n",
    "    # --- PUNTUACI√ìN ---\n",
    "    puntuacion_txt = get_first_text(\n",
    "        empresa,\n",
    "        [\n",
    "            'span[class*=\"styles_trustScore__\"] span',\n",
    "            'span[class*=\"styles_trustScore__\"]',\n",
    "            'span[weight=\"heavy\"] span',\n",
    "            'span[weight=\"heavy\"]',\n",
    "        ],\n",
    "    )\n",
    "    puntuacion = parse_puntuacion(puntuacion_txt)\n",
    "\n",
    "    # --- P√ÅGINA WEB ---\n",
    "    pagina_web = get_first_text(\n",
    "        empresa,\n",
    "        [\n",
    "            'p[class*=\"styles_websiteUrlDisplayed__\"]',\n",
    "            'p[class*=\"styles_websiteUrlDisplayed\"]',\n",
    "            'p[class*=\"websiteUrl\"]',\n",
    "        ],\n",
    "    )\n",
    "    if pagina_web:\n",
    "        pagina_web = pagina_web.replace(\" \", \"\")\n",
    "        if not pagina_web.lower().startswith((\"http://\", \"https://\")):\n",
    "            pagina_web = \"https://\" + pagina_web\n",
    "    else:\n",
    "        pagina_web = \"N/A\"\n",
    "\n",
    "    datos.append(\n",
    "        {\n",
    "            \"nombre\": nombre,\n",
    "            \"ubicacion\": ubicacion,\n",
    "            \"puntuacion\": puntuacion,\n",
    "            \"pagina_web\": pagina_web,\n",
    "        }\n",
    "    )\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# ----------------------------------\n",
    "# Guardar JSON\n",
    "# ----------------------------------\n",
    "out_path = \"datalake/1_LANDING_ZONE/trustpilot_empresas_categoria1.json\"\n",
    "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(datos, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"‚úÖ Datos extra√≠dos correctamente:\")\n",
    "print(json.dumps(datos, indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoria 2 - Seguros de viajes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T20:00:31.462513Z",
     "iopub.status.busy": "2025-03-22T20:00:31.462228Z",
     "iopub.status.idle": "2025-03-22T20:01:00.572020Z",
     "shell.execute_reply": "2025-03-22T20:01:00.570812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datos extra√≠dos correctamente:\n",
      "[\n",
      "    {\n",
      "        \"nombre\": \"Heymondo Seguros de Viaje\",\n",
      "        \"ubicacion\": \"Calle Alaba, n√∫mero 140, 2¬∫ 4¬™, Barcelona, Espa√±a\",\n",
      "        \"puntuacion\": 4.5,\n",
      "        \"pagina_web\": \"https://heymondo.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Intermundial Seguros de viaje\",\n",
      "        \"ubicacion\": \"Calle de Ir√∫n, 7, Madrid, Espa√±a\",\n",
      "        \"puntuacion\": 4.5,\n",
      "        \"pagina_web\": \"https://intermundial.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"IATI Seguros\",\n",
      "        \"ubicacion\": \"Avinguda Diagonal 622, Barcelona, Espa√±a\",\n",
      "        \"puntuacion\": 4.4,\n",
      "        \"pagina_web\": \"https://iatiseguros.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Abbeygate Seguros Espa√±a\",\n",
      "        \"ubicacion\": \"Ctra Nacional 340, KM148.5, Estepona, Espa√±a\",\n",
      "        \"puntuacion\": 4.8,\n",
      "        \"pagina_web\": \"https://www.abbeygateinsure.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"MAPFRE Espa√±a\",\n",
      "        \"ubicacion\": \"Carretera de Pozuelo 52, Majadahonda, Espa√±a\",\n",
      "        \"puntuacion\": 1.5,\n",
      "        \"pagina_web\": \"https://mapfre.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Fit 2 Trip\",\n",
      "        \"ubicacion\": \"Irun 7, Madrid, Espa√±a\",\n",
      "        \"puntuacion\": 4.5,\n",
      "        \"pagina_web\": \"https://www.fit2trip.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Pelayo Mutua de Seguros\",\n",
      "        \"ubicacion\": \"Calle de Santa Engracia 67, Madrid, Espa√±a\",\n",
      "        \"puntuacion\": 1.1,\n",
      "        \"pagina_web\": \"https://pelayo.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Zurich\",\n",
      "        \"ubicacion\": \"Via Augusta 200, Barcelona, Espa√±a\",\n",
      "        \"puntuacion\": 1.1,\n",
      "        \"pagina_web\": \"https://zurich.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Chapka Seguros\",\n",
      "        \"ubicacion\": \"C/ Vel√°zquez, 86D, Madrid, Espa√±a\",\n",
      "        \"puntuacion\": 4.5,\n",
      "        \"pagina_web\": \"https://www.chapkadirect.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Divina Seguros\",\n",
      "        \"ubicacion\": \"Espa√±a\",\n",
      "        \"puntuacion\": 4.1,\n",
      "        \"pagina_web\": \"https://www.divinaseguros.com\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re, json, os\n",
    "\n",
    "# ----------------------------------\n",
    "# Configuraci√≥n de Chrome\n",
    "# ----------------------------------\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")     # headless moderno\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--window-size=1280,2000\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "URL = \"https://es.trustpilot.com/categories/travel_insurance_company?sort=reviews_count\"\n",
    "driver.get(URL)\n",
    "\n",
    "# Espera a que aparezcan las tarjetas de empresa\n",
    "wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a[name=\"business-unit-card\"]')))\n",
    "empresas = driver.find_elements(By.CSS_SELECTOR, 'a[name=\"business-unit-card\"]')[:10]  # top N\n",
    "datos = []\n",
    "\n",
    "# ----------------------------------\n",
    "# Helpers\n",
    "# ----------------------------------\n",
    "def limpiar_texto(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "def get_first_text(el, selector_list):\n",
    "    \"\"\"Prueba varios selectores y devuelve el primer innerText v√°lido.\"\"\"\n",
    "    for sel in selector_list:\n",
    "        try:\n",
    "            x = el.find_element(By.CSS_SELECTOR, sel)\n",
    "            txt = x.get_attribute(\"innerText\") or x.text\n",
    "            txt = limpiar_texto(txt)\n",
    "            if txt:\n",
    "                return txt\n",
    "        except:\n",
    "            continue\n",
    "    return \"\"\n",
    "\n",
    "def parse_puntuacion(txt: str):\n",
    "    \"\"\"\n",
    "    Convierte '4,8' o '4.8' a float 4.8.\n",
    "    Soporta '4,8/5' o con saltos de l√≠nea.\n",
    "    \"\"\"\n",
    "    if not txt:\n",
    "        return \"N/A\"\n",
    "    m = re.search(r\"(\\d+[.,]\\d+|\\d+)\", txt)\n",
    "    if not m:\n",
    "        return \"N/A\"\n",
    "    num = m.group(1).replace(\",\", \".\")\n",
    "    try:\n",
    "        return float(num)\n",
    "    except:\n",
    "        return \"N/A\"\n",
    "\n",
    "# ----------------------------------\n",
    "# Scraping\n",
    "# ----------------------------------\n",
    "for empresa in empresas:\n",
    "    # --- NOMBRE ---\n",
    "    nombre = get_first_text(\n",
    "        empresa,\n",
    "        [\n",
    "            'p[class*=\"CDS_Typography_heading-\"]',\n",
    "            'p[class*=\"heading-\"]',\n",
    "            'p[class*=\"heading\"]',\n",
    "        ],\n",
    "    ) or \"N/A\"\n",
    "\n",
    "    # --- UBICACI√ìN ---\n",
    "    ubicacion = \"\"\n",
    "    # 1) Contenedor t√≠pico de ubicaci√≥n\n",
    "    try:\n",
    "        u = empresa.find_element(By.CSS_SELECTOR, 'div[class*=\"styles_businessLocation__\"] p')\n",
    "        ubicacion = limpiar_texto(u.get_attribute(\"innerText\") or u.text)\n",
    "    except:\n",
    "        ubicacion = \"\"\n",
    "\n",
    "    # 2) Fallback antiguo por atributo data\n",
    "    if not ubicacion:\n",
    "        try:\n",
    "            uel = empresa.find_element(By.CSS_SELECTOR, 'span[data-business-location-typography=\"true\"]')\n",
    "            ubicacion = limpiar_texto(uel.get_attribute(\"innerText\") or uel.text)\n",
    "        except:\n",
    "            ubicacion = \"\"\n",
    "\n",
    "    # 3) Fallback gen√©rico (por si cambian el DOM)\n",
    "    if not ubicacion:\n",
    "        try:\n",
    "            candidatos = empresa.find_elements(\n",
    "                By.CSS_SELECTOR,\n",
    "                'p[class*=\"CDS_Typography_body-\"], span[class*=\"CDS_Typography_body-\"], p[dir=\"auto\"]'\n",
    "            )\n",
    "            for c in candidatos:\n",
    "                t = limpiar_texto(c.get_attribute(\"innerText\") or c.text)\n",
    "                # acepta pa√≠s/ciudad o textos con coma/n√∫mero\n",
    "                if t and (\n",
    "                    \",\" in t\n",
    "                    or re.search(r\"\\d\", t)\n",
    "                    or re.search(r\"(Espa√±a|Spain|Madrid|Barcelona|Valencia|Sevilla|Bilbao)\", t, re.I)\n",
    "                ):\n",
    "                    ubicacion = t\n",
    "                    break\n",
    "        except:\n",
    "            pass\n",
    "    if not ubicacion:\n",
    "        ubicacion = \"N/A\"\n",
    "\n",
    "    # --- PUNTUACI√ìN ---\n",
    "    puntuacion_txt = get_first_text(\n",
    "        empresa,\n",
    "        [\n",
    "            'span[class*=\"styles_trustScore__\"] span',\n",
    "            'span[class*=\"styles_trustScore__\"]',\n",
    "            'span[weight=\"heavy\"] span',\n",
    "            'span[weight=\"heavy\"]',\n",
    "        ],\n",
    "    )\n",
    "    puntuacion = parse_puntuacion(puntuacion_txt)\n",
    "\n",
    "    # --- P√ÅGINA WEB ---\n",
    "    pagina_web = get_first_text(\n",
    "        empresa,\n",
    "        [\n",
    "            'p[class*=\"styles_websiteUrlDisplayed__\"]',\n",
    "            'p[class*=\"styles_websiteUrlDisplayed\"]',\n",
    "            'p[class*=\"websiteUrl\"]',\n",
    "        ],\n",
    "    )\n",
    "    if pagina_web:\n",
    "        pagina_web = pagina_web.replace(\" \", \"\")\n",
    "        if not pagina_web.lower().startswith((\"http://\", \"https://\")):\n",
    "            pagina_web = \"https://\" + pagina_web\n",
    "    else:\n",
    "        pagina_web = \"N/A\"\n",
    "\n",
    "    datos.append(\n",
    "        {\n",
    "            \"nombre\": nombre,\n",
    "            \"ubicacion\": ubicacion,\n",
    "            \"puntuacion\": puntuacion,\n",
    "            \"pagina_web\": pagina_web,\n",
    "        }\n",
    "    )\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# ----------------------------------\n",
    "# Guardar JSON\n",
    "# ----------------------------------\n",
    "out_path = \"datalake/1_LANDING_ZONE/trustpilot_empresas_categoria2.json\"\n",
    "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(datos, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"‚úÖ Datos extra√≠dos correctamente:\")\n",
    "print(json.dumps(datos, indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoria 3 - Concesionario de autos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datos extra√≠dos correctamente:\n",
      "[\n",
      "    {\n",
      "        \"nombre\": \"compramostucoche.es\",\n",
      "        \"ubicacion\": \"Espa√±a\",\n",
      "        \"puntuacion\": 4.6,\n",
      "        \"pagina_web\": \"https://www.compramostucoche.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"OcasionPlus\",\n",
      "        \"ubicacion\": \"Avenida Juan Carlos I 30, Collado Villalba, Espa√±a\",\n",
      "        \"puntuacion\": 4.1,\n",
      "        \"pagina_web\": \"https://ocasionplus.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Autohero Espa√±a\",\n",
      "        \"ubicacion\": \"Calle Rosario Pino 14-16 Planta 1, Madrid, Espa√±a\",\n",
      "        \"puntuacion\": 4.2,\n",
      "        \"pagina_web\": \"https://autohero.com/es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Carwow ES\",\n",
      "        \"ubicacion\": \"C. de Serrano Anguita, 13, Madrid, Espa√±a\",\n",
      "        \"puntuacion\": 4.4,\n",
      "        \"pagina_web\": \"https://www.carwow.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Carplus Espa√±a\",\n",
      "        \"ubicacion\": \"Espa√±a\",\n",
      "        \"puntuacion\": 4.6,\n",
      "        \"pagina_web\": \"https://carplus.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Hr Motor\",\n",
      "        \"ubicacion\": \"Espa√±a\",\n",
      "        \"puntuacion\": 3.4,\n",
      "        \"pagina_web\": \"https://www.hrmotor.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Clicars\",\n",
      "        \"ubicacion\": \"Av. Laboral 10, Madrid, Espa√±a\",\n",
      "        \"puntuacion\": 4.3,\n",
      "        \"pagina_web\": \"https://www.clicars.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"CarNext.com Espa√±a\",\n",
      "        \"ubicacion\": \"Espa√±a\",\n",
      "        \"puntuacion\": 4.7,\n",
      "        \"pagina_web\": \"https://carnext.com/es-es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Autobiz SA\",\n",
      "        \"ubicacion\": \"Espa√±a\",\n",
      "        \"puntuacion\": 2.4,\n",
      "        \"pagina_web\": \"https://autobiz-ocasion.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Garage Club\",\n",
      "        \"ubicacion\": \"Carrer Catalunya, 37, Les Franqueses del Vall√©s (Barcelona), Espa√±a\",\n",
      "        \"puntuacion\": 3.9,\n",
      "        \"pagina_web\": \"https://garageclub.es\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re, json, os\n",
    "\n",
    "# ----------------------------------\n",
    "# Configuraci√≥n de Chrome\n",
    "# ----------------------------------\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")     # headless moderno\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--window-size=1280,2000\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "URL = \"https://es.trustpilot.com/categories/car_dealer?sort=reviews_count\"\n",
    "driver.get(URL)\n",
    "\n",
    "# Espera a que aparezcan las tarjetas de empresa\n",
    "wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a[name=\"business-unit-card\"]')))\n",
    "empresas = driver.find_elements(By.CSS_SELECTOR, 'a[name=\"business-unit-card\"]')[:10]  # top N\n",
    "datos = []\n",
    "\n",
    "# ----------------------------------\n",
    "# Helpers\n",
    "# ----------------------------------\n",
    "def limpiar_texto(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "def get_first_text(el, selector_list):\n",
    "    \"\"\"Prueba varios selectores y devuelve el primer innerText v√°lido.\"\"\"\n",
    "    for sel in selector_list:\n",
    "        try:\n",
    "            x = el.find_element(By.CSS_SELECTOR, sel)\n",
    "            txt = x.get_attribute(\"innerText\") or x.text\n",
    "            txt = limpiar_texto(txt)\n",
    "            if txt:\n",
    "                return txt\n",
    "        except:\n",
    "            continue\n",
    "    return \"\"\n",
    "\n",
    "def parse_puntuacion(txt: str):\n",
    "    \"\"\"\n",
    "    Convierte '4,8' o '4.8' a float 4.8.\n",
    "    Soporta '4,8/5' o con saltos de l√≠nea.\n",
    "    \"\"\"\n",
    "    if not txt:\n",
    "        return \"N/A\"\n",
    "    m = re.search(r\"(\\d+[.,]\\d+|\\d+)\", txt)\n",
    "    if not m:\n",
    "        return \"N/A\"\n",
    "    num = m.group(1).replace(\",\", \".\")\n",
    "    try:\n",
    "        return float(num)\n",
    "    except:\n",
    "        return \"N/A\"\n",
    "\n",
    "# ----------------------------------\n",
    "# Scraping\n",
    "# ----------------------------------\n",
    "for empresa in empresas:\n",
    "    # --- NOMBRE ---\n",
    "    nombre = get_first_text(\n",
    "        empresa,\n",
    "        [\n",
    "            'p[class*=\"CDS_Typography_heading-\"]',\n",
    "            'p[class*=\"heading-\"]',\n",
    "            'p[class*=\"heading\"]',\n",
    "        ],\n",
    "    ) or \"N/A\"\n",
    "\n",
    "    # --- UBICACI√ìN ---\n",
    "    ubicacion = \"\"\n",
    "    # 1) Contenedor t√≠pico de ubicaci√≥n\n",
    "    try:\n",
    "        u = empresa.find_element(By.CSS_SELECTOR, 'div[class*=\"styles_businessLocation__\"] p')\n",
    "        ubicacion = limpiar_texto(u.get_attribute(\"innerText\") or u.text)\n",
    "    except:\n",
    "        ubicacion = \"\"\n",
    "\n",
    "    # 2) Fallback atributo data antiguo\n",
    "    if not ubicacion:\n",
    "        try:\n",
    "            uel = empresa.find_element(By.CSS_SELECTOR, 'span[data-business-location-typography=\"true\"]')\n",
    "            ubicacion = limpiar_texto(uel.get_attribute(\"innerText\") or uel.text)\n",
    "        except:\n",
    "            ubicacion = \"\"\n",
    "\n",
    "    # 3) Fallback gen√©rico (por si cambian el DOM)\n",
    "    if not ubicacion:\n",
    "        try:\n",
    "            candidatos = empresa.find_elements(\n",
    "                By.CSS_SELECTOR,\n",
    "                'p[class*=\"CDS_Typography_body-\"], span[class*=\"CDS_Typography_body-\"], p[dir=\"auto\"]'\n",
    "            )\n",
    "            for c in candidatos:\n",
    "                t = limpiar_texto(c.get_attribute(\"innerText\") or c.text)\n",
    "                # acepta pa√≠s/ciudad o textos con coma/n√∫mero\n",
    "                if t and (\n",
    "                    \",\" in t\n",
    "                    or re.search(r\"\\d\", t)\n",
    "                    or re.search(r\"(Espa√±a|Spain|Madrid|Barcelona|Valencia|Sevilla|Bilbao)\", t, re.I)\n",
    "                ):\n",
    "                    ubicacion = t\n",
    "                    break\n",
    "        except:\n",
    "            pass\n",
    "    if not ubicacion:\n",
    "        ubicacion = \"N/A\"\n",
    "\n",
    "    # --- PUNTUACI√ìN ---\n",
    "    puntuacion_txt = get_first_text(\n",
    "        empresa,\n",
    "        [\n",
    "            'span[class*=\"styles_trustScore__\"] span',\n",
    "            'span[class*=\"styles_trustScore__\"]',\n",
    "            'span[weight=\"heavy\"] span',\n",
    "            'span[weight=\"heavy\"]',\n",
    "        ],\n",
    "    )\n",
    "    puntuacion = parse_puntuacion(puntuacion_txt)\n",
    "\n",
    "    # --- P√ÅGINA WEB ---\n",
    "    pagina_web = get_first_text(\n",
    "        empresa,\n",
    "        [\n",
    "            'p[class*=\"styles_websiteUrlDisplayed__\"]',\n",
    "            'p[class*=\"styles_websiteUrlDisplayed\"]',\n",
    "            'p[class*=\"websiteUrl\"]',\n",
    "        ],\n",
    "    )\n",
    "    if pagina_web:\n",
    "        pagina_web = pagina_web.replace(\" \", \"\")\n",
    "        if not pagina_web.lower().startswith((\"http://\", \"https://\")):\n",
    "            pagina_web = \"https://\" + pagina_web\n",
    "    else:\n",
    "        pagina_web = \"N/A\"\n",
    "\n",
    "    datos.append(\n",
    "        {\n",
    "            \"nombre\": nombre,\n",
    "            \"ubicacion\": ubicacion,\n",
    "            \"puntuacion\": puntuacion,\n",
    "            \"pagina_web\": pagina_web,\n",
    "        }\n",
    "    )\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# ----------------------------------\n",
    "# Guardar JSON\n",
    "# ----------------------------------\n",
    "out_path = \"datalake/1_LANDING_ZONE/trustpilot_empresas_categoria3.json\"\n",
    "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(datos, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"‚úÖ Datos extra√≠dos correctamente:\")\n",
    "print(json.dumps(datos, indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping de las reviews de las empresas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoria 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap - Paginado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Verificando nuevas rese√±as de Aplazame (aplazame.com)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/aplazame.com\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/aplazame.com?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/aplazame.com?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/aplazame.com?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/aplazame.com?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üîç Verificando nuevas rese√±as de Bnext (www.bnext.es)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.bnext.es\n",
      "‚úÖ 8 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.bnext.es?page=2\n",
      "üõë No hay rese√±as nuevas en p√°gina 2. Deteniendo scraping para esta empresa.\n",
      "\n",
      "üîç Verificando nuevas rese√±as de CaixaBank (www.caixabank.es)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.caixabank.es\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.caixabank.es?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.caixabank.es?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.caixabank.es?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.caixabank.es?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üîç Verificando nuevas rese√±as de Crealsa (www.crealsa.es)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.crealsa.es\n",
      "‚úÖ 4 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.crealsa.es?page=2\n",
      "üõë No hay rese√±as nuevas en p√°gina 2. Deteniendo scraping para esta empresa.\n",
      "\n",
      "üîç Verificando nuevas rese√±as de BBVA Espa√±a (bbva.es)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/bbva.es\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/bbva.es?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/bbva.es?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/bbva.es?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/bbva.es?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üîç Verificando nuevas rese√±as de Banco Sabadell (bancsabadell.com)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/bancsabadell.com\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/bancsabadell.com?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/bancsabadell.com?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/bancsabadell.com?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/bancsabadell.com?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üîç Verificando nuevas rese√±as de ING Espa√±a (ing.es)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/ing.es\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/ing.es?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/ing.es?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/ing.es?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/ing.es?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üîç Verificando nuevas rese√±as de Cetelem Espa√±a (www.cetelem.es)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.cetelem.es\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.cetelem.es?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.cetelem.es?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.cetelem.es?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.cetelem.es?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üîç Verificando nuevas rese√±as de EVO Banco (evobanco.com)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/evobanco.com\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/evobanco.com?page=2\n",
      "‚úÖ 11 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/evobanco.com?page=3\n",
      "üõë No hay rese√±as nuevas en p√°gina 3. Deteniendo scraping para esta empresa.\n",
      "\n",
      "üîç Verificando nuevas rese√±as de Santander Espa√±a (www.bancosantander.es)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.bancosantander.es\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.bancosantander.es?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.bancosantander.es?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.bancosantander.es?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.bancosantander.es?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üéâ Dataset actualizado guardado en 'datalake/1_LANDING_ZONE/dataset_categoria1.csv'\n",
      "üì¶ Archivo JSON guardado en 'datalake/1_LANDING_ZONE/reviews_trustpilot_empresas_categoria1.json'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ------------- CONFIG -------------\n",
    "DEBUG = False                 # True = imprime y guarda evidencia (screenshot + html)\n",
    "MAX_PAGES = 5               # m√°ximo de p√°ginas por empresa\n",
    "EMPRESAS_JSON = 'datalake/1_LANDING_ZONE/trustpilot_empresas_categoria1.json'\n",
    "DATASET_CSV   = 'datalake/1_LANDING_ZONE/dataset_categoria1.csv'\n",
    "SALIDA_JSON   = 'datalake/1_LANDING_ZONE/reviews_trustpilot_empresas_categoria1.json'\n",
    "\n",
    "# ------------- HELPERS -------------\n",
    "def norm_text(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "def parse_estrellas_from_alt(alt_txt: str):\n",
    "    \"\"\"\n",
    "    'Valorada con 4 estrellas sobre 5' -> '4'\n",
    "    \"\"\"\n",
    "    if not alt_txt:\n",
    "        return \"N/A\"\n",
    "    m = re.search(r\"(\\d)\\s*estrellas\", alt_txt, flags=re.I)\n",
    "    return m.group(1) if m else \"N/A\"\n",
    "\n",
    "def normalizar_dominio(pagina_web: str) -> str:\n",
    "    \"\"\"\n",
    "    Espera 'www.midominio.es' o 'midominio.es'.\n",
    "    Quita http(s):// si viene, y slashes al final.\n",
    "    \"\"\"\n",
    "    if not pagina_web:\n",
    "        return \"\"\n",
    "    pw = pagina_web.strip()\n",
    "    pw = re.sub(r\"^https?://\", \"\", pw, flags=re.I)\n",
    "    pw = pw.strip(\"/\")\n",
    "    return pw\n",
    "\n",
    "def extraer_review_uid(card):\n",
    "    \"\"\"\n",
    "    Intenta extraer el id de la rese√±a desde el href del t√≠tulo: '/reviews/<id>'\n",
    "    Devuelve el segmento <id> o '' si no existe.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        a = card.find_element(By.CSS_SELECTOR, 'a[data-review-title-typography=\"true\"]')\n",
    "        href = a.get_attribute(\"href\")\n",
    "        if href:\n",
    "            m = re.search(r\"/reviews/([^/?#]+)\", href)\n",
    "            if m:\n",
    "                return m.group(1)\n",
    "    except:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "def aceptar_cookies_si_aparecen(driver, wait):\n",
    "    \"\"\"\n",
    "    Cierra la barra de cookies de Trustpilot si aparece (bot√≥n 'Entendido' o similar).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # bot√≥n con texto 'Entendido'\n",
    "        btn = WebDriverWait(driver, 3).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[.//span[normalize-space(text())='Entendido'] or normalize-space(text())='Entendido']\"))\n",
    "        )\n",
    "        btn.click()\n",
    "        time.sleep(0.5)\n",
    "        return True\n",
    "    except:\n",
    "        # otros textos t√≠picos\n",
    "        try:\n",
    "            btn2 = WebDriverWait(driver, 3).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[normalize-space(text())='Aceptar']\"))\n",
    "            )\n",
    "            btn2.click()\n",
    "            time.sleep(0.5)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "# ------------- CARGA EMPRESAS -------------\n",
    "with open(EMPRESAS_JSON, 'r', encoding='utf-8') as f:\n",
    "    empresas_data = json.load(f)\n",
    "\n",
    "# ------------- CARGA / CREA DATASET -------------\n",
    "if os.path.exists(DATASET_CSV):\n",
    "    existing_df = pd.read_csv(DATASET_CSV)\n",
    "else:\n",
    "    existing_df = pd.DataFrame(columns=[\"id_rese√±a\", \"Fecha\", \"T√≠tulo\", \"Contenido\", \"Empresa\", \"Calificaci√≥n\"])\n",
    "\n",
    "existing_df['Fecha'] = pd.to_datetime(existing_df['Fecha'], errors='coerce')\n",
    "\n",
    "# ------------- SELENIUM -------------\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_argument(\"--window-size=1366,2400\")\n",
    "options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    \"(KHTML, like Gecko) Chrome/118.0.5993.70 Safari/537.36\"\n",
    ")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "wait = WebDriverWait(driver, 25)\n",
    "\n",
    "resultados = []\n",
    "reviews_by_company = {}\n",
    "\n",
    "try:\n",
    "    for empresa in empresas_data:\n",
    "        nombre_empresa = empresa.get(\"nombre\", \"N/A\")\n",
    "        pagina_web = normalizar_dominio(empresa.get(\"pagina_web\", \"\"))\n",
    "        if not pagina_web:\n",
    "            print(f\"‚ö†Ô∏è Empresa sin 'pagina_web': {nombre_empresa}. Omitida.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüîç Verificando nuevas rese√±as de {nombre_empresa} ({pagina_web})...\")\n",
    "\n",
    "        # Hist√≥rico de esa empresa\n",
    "        old_df = existing_df[existing_df[\"Empresa\"] == nombre_empresa].copy()\n",
    "        if not old_df.empty:\n",
    "            old_df[\"T√≠tulo\"] = old_df[\"T√≠tulo\"].fillna(\"\").str.strip()\n",
    "            old_df[\"Fecha\"] = pd.to_datetime(old_df[\"Fecha\"], errors=\"coerce\")\n",
    "\n",
    "        rese√±as_nuevas = []\n",
    "\n",
    "        for page in range(1, MAX_PAGES + 1):\n",
    "            review_url = f\"https://es.trustpilot.com/review/{pagina_web}\" + (f\"?page={page}\" if page > 1 else \"\")\n",
    "            print(f\"üåê Accediendo a: {review_url}\")\n",
    "            driver.get(review_url)\n",
    "\n",
    "            # Aceptar cookies si estorban\n",
    "            aceptar_cookies_si_aparecen(driver, wait)\n",
    "\n",
    "            # Espera a que al menos haya 1 card de rese√±a en la p√°gina\n",
    "            try:\n",
    "                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-testid=\"service-review-card-v2\"]')))\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è No se encontraron cards en p√°gina {page}: {e}\")\n",
    "                # guarda para debug y rompe\n",
    "                if DEBUG:\n",
    "                    ss = f\"_debug_reviews_{nombre_empresa}_p{page}_nocards.png\"\n",
    "                    driver.save_screenshot(ss)\n",
    "                    with open(f\"_debug_reviews_{nombre_empresa}_p{page}_nocards.html\", \"w\", encoding=\"utf-8\") as fh:\n",
    "                        fh.write(driver.page_source)\n",
    "                    print(f\"üíæ Guard√© evidencia sin cards: {ss}\")\n",
    "                break\n",
    "\n",
    "            # Forzar render con scrolls suaves (contenido lazy)\n",
    "            for _ in range(3):\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(1.2)\n",
    "\n",
    "            # Cards reales (B√öSQUEDA GLOBAL, no dentro de un contenedor espec√≠fico)\n",
    "            cards = driver.find_elements(By.CSS_SELECTOR, 'div[data-testid=\"service-review-card-v2\"]')\n",
    "\n",
    "            if DEBUG:\n",
    "                print(f\"üîé Encontradas {len(cards)} cards en p√°gina {page}\")\n",
    "                try:\n",
    "                    ss_name = f\"_debug_reviews_{nombre_empresa}_p{page}.png\"\n",
    "                    html_name = f\"_debug_reviews_{nombre_empresa}_p{page}.html\"\n",
    "                    driver.save_screenshot(ss_name)\n",
    "                    with open(html_name, \"w\", encoding=\"utf-8\") as fh:\n",
    "                        fh.write(driver.page_source)\n",
    "                    print(f\"üíæ Guard√© screenshot y HTML: {ss_name} / {html_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è No pude guardar debug files: {e}\")\n",
    "\n",
    "            nuevas_esta_pagina = []\n",
    "\n",
    "            for i, card in enumerate(cards, start=1):\n",
    "                try:\n",
    "                    # Fecha (ISO en <time datetime=\"...\">)\n",
    "                    try:\n",
    "                        time_el = card.find_element(By.CSS_SELECTOR, 'time[datetime]')\n",
    "                        fecha = pd.to_datetime(time_el.get_attribute(\"datetime\"), errors='coerce')\n",
    "                    except:\n",
    "                        fecha = pd.NaT\n",
    "\n",
    "                    # T√≠tulo\n",
    "                    try:\n",
    "                        h2 = card.find_element(By.CSS_SELECTOR, 'h2[data-service-review-title-typography=\"true\"]')\n",
    "                        titulo = norm_text(h2.get_attribute(\"innerText\") or h2.text)\n",
    "                    except:\n",
    "                        titulo = \"N/A\"\n",
    "\n",
    "                    # Contenido\n",
    "                    try:\n",
    "                        p = card.find_element(By.CSS_SELECTOR, 'p[data-service-review-text-typography=\"true\"]')\n",
    "                        contenido = norm_text(p.get_attribute(\"innerText\") or p.text)\n",
    "                    except:\n",
    "                        contenido = \"Sin contenido\"\n",
    "\n",
    "                    # Calificaci√≥n (solo n√∫mero)\n",
    "                    try:\n",
    "                        img = card.find_element(By.CSS_SELECTOR, 'img[alt*=\"Valorada con\"]')\n",
    "                        estrellas = parse_estrellas_from_alt(img.get_attribute(\"alt\"))\n",
    "                    except:\n",
    "                        estrellas = \"N/A\"\n",
    "\n",
    "                    # UID de la review (por si luego quieres deduplicar por id real)\n",
    "                    review_uid = extraer_review_uid(card)\n",
    "\n",
    "                    if DEBUG and i <= 3:\n",
    "                        print(f\"  ‚Ä¢ Card #{i}: fecha={fecha}, estrellas={estrellas}, t√≠tulo='{titulo[:60]}', uid='{review_uid}'\")\n",
    "\n",
    "                    # DEDUP: por compatibilidad, T√≠tulo + Fecha\n",
    "                    existe = (\n",
    "                        not old_df[\n",
    "                            (old_df[\"T√≠tulo\"].str.strip() == titulo) &\n",
    "                            (old_df[\"Fecha\"] == fecha)\n",
    "                        ].empty\n",
    "                    )\n",
    "\n",
    "                    if not existe:\n",
    "                        nuevas_esta_pagina.append({\n",
    "                            \"Empresa\": nombre_empresa,\n",
    "                            \"T√≠tulo\": titulo,\n",
    "                            \"Contenido\": contenido,\n",
    "                            \"Fecha\": fecha,\n",
    "                            \"Calificaci√≥n\": estrellas\n",
    "                        })\n",
    "\n",
    "                except Exception as e_card:\n",
    "                    if DEBUG:\n",
    "                        print(f\"  ‚Ä¢ Aviso: card con error: {e_card}\")\n",
    "                    continue\n",
    "\n",
    "            if nuevas_esta_pagina:\n",
    "                rese√±as_nuevas.extend(nuevas_esta_pagina)\n",
    "                print(f\"‚úÖ {len(nuevas_esta_pagina)} rese√±as nuevas en p√°gina {page}. Continuando...\")\n",
    "            else:\n",
    "                print(f\"üõë No hay rese√±as nuevas en p√°gina {page}. Deteniendo scraping para esta empresa.\")\n",
    "                break\n",
    "\n",
    "        # Consolidar por empresa\n",
    "        if rese√±as_nuevas:\n",
    "            df_nuevas = pd.DataFrame(rese√±as_nuevas)\n",
    "            df_nuevas['Fecha'] = pd.to_datetime(df_nuevas['Fecha'], errors='coerce')\n",
    "            combined_df = pd.concat([old_df, df_nuevas], ignore_index=True)\n",
    "            combined_df.drop_duplicates(subset=[\"T√≠tulo\", \"Fecha\"], keep='first', inplace=True)\n",
    "            combined_df.sort_values(by=\"Fecha\", ascending=False, inplace=True)\n",
    "            combined_df.reset_index(drop=True, inplace=True)\n",
    "            combined_df.fillna(\"\", inplace=True)\n",
    "\n",
    "            # id_rese√±a por empresa (mantenemos tu formato)\n",
    "            total = len(combined_df)\n",
    "            combined_df[\"id_rese√±a\"] = [f\"{nombre_empresa.replace(' ', '')}_N{total - idx}\" for idx in range(total)]\n",
    "\n",
    "            reviews_by_company[nombre_empresa] = combined_df.to_dict(orient=\"records\")\n",
    "            resultados.append(combined_df)\n",
    "        else:\n",
    "            if not old_df.empty:\n",
    "                reviews_by_company[nombre_empresa] = old_df.to_dict(orient=\"records\")\n",
    "            print(f\"üì≠ No se detectaron nuevas rese√±as para {nombre_empresa}.\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# ------------- SALIDA CSV -------------\n",
    "if resultados:\n",
    "    final_df = pd.concat([existing_df] + resultados, ignore_index=True)\n",
    "    final_df.drop_duplicates(subset=[\"T√≠tulo\", \"Fecha\", \"Empresa\"], keep='first', inplace=True)\n",
    "    final_df.fillna(\"\", inplace=True)\n",
    "\n",
    "    # Recalcular ids por empresa en orden de fecha desc\n",
    "    final_df[\"Fecha\"] = pd.to_datetime(final_df[\"Fecha\"], errors=\"coerce\")\n",
    "    final_df.sort_values(by=[\"Empresa\", \"Fecha\"], ascending=[True, False], inplace=True)\n",
    "\n",
    "    final_df[\"id_rese√±a\"] = final_df.groupby(\"Empresa\").cumcount(ascending=False) + 1\n",
    "    final_df[\"id_rese√±a\"] = final_df.apply(\n",
    "        lambda row: f\"{row['Empresa'].replace(' ', '')}_N{row['id_rese√±a']}\", axis=1\n",
    "    )\n",
    "\n",
    "    final_df = final_df[[\"id_rese√±a\", \"Fecha\", \"T√≠tulo\", \"Contenido\", \"Empresa\", \"Calificaci√≥n\"]]\n",
    "\n",
    "    # Ordenar por n√∫mero de id descendente dentro de cada empresa\n",
    "    final_df[\"id_num\"] = final_df[\"id_rese√±a\"].str.extract(r'_N(\\d+)').astype(int)\n",
    "    final_df.sort_values(by=[\"Empresa\", \"id_num\"], ascending=[True, False], inplace=True)\n",
    "    final_df.drop(columns=[\"id_num\"], inplace=True)\n",
    "\n",
    "    os.makedirs(os.path.dirname(DATASET_CSV), exist_ok=True)\n",
    "    final_df.to_csv(DATASET_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\nüéâ Dataset actualizado guardado en '{DATASET_CSV}'\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No hubo actualizaciones, el dataset se mantiene igual.\")\n",
    "\n",
    "# ------------- SALIDA JSON (anidado por empresa) -------------\n",
    "for empresa in empresas_data:\n",
    "    nombre_empresa = empresa.get(\"nombre\", \"N/A\")\n",
    "    empresa[\"rese√±as\"] = reviews_by_company.get(nombre_empresa, [])\n",
    "\n",
    "os.makedirs(os.path.dirname(SALIDA_JSON), exist_ok=True)\n",
    "with open(SALIDA_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(empresas_data, f, ensure_ascii=False, indent=4, default=str)\n",
    "\n",
    "print(f\"üì¶ Archivo JSON guardado en '{SALIDA_JSON}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoria 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap - Paginado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Verificando nuevas rese√±as de Heymondo Seguros de Viaje (heymondo.es)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/heymondo.es\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/heymondo.es?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/heymondo.es?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/heymondo.es?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/heymondo.es?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üîç Verificando nuevas rese√±as de Intermundial Seguros de viaje (intermundial.es)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/intermundial.es\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/intermundial.es?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/intermundial.es?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/intermundial.es?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/intermundial.es?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üîç Verificando nuevas rese√±as de IATI Seguros (iatiseguros.com)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/iatiseguros.com\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/iatiseguros.com?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/iatiseguros.com?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/iatiseguros.com?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/iatiseguros.com?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üîç Verificando nuevas rese√±as de Abbeygate Seguros Espa√±a (www.abbeygateinsure.com)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.abbeygateinsure.com\n",
      "‚úÖ 2 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.abbeygateinsure.com?page=2\n",
      "üõë No hay rese√±as nuevas en p√°gina 2. Deteniendo scraping para esta empresa.\n",
      "\n",
      "üîç Verificando nuevas rese√±as de MAPFRE Espa√±a (mapfre.es)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/mapfre.es\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/mapfre.es?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/mapfre.es?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/mapfre.es?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/mapfre.es?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üîç Verificando nuevas rese√±as de Fit 2 Trip (www.fit2trip.es)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.fit2trip.es\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.fit2trip.es?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.fit2trip.es?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.fit2trip.es?page=4\n",
      "üõë No hay rese√±as nuevas en p√°gina 4. Deteniendo scraping para esta empresa.\n",
      "\n",
      "üîç Verificando nuevas rese√±as de Pelayo Mutua de Seguros (pelayo.com)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/pelayo.com\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/pelayo.com?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/pelayo.com?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/pelayo.com?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/pelayo.com?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üîç Verificando nuevas rese√±as de Zurich (zurich.es)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/zurich.es\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/zurich.es?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/zurich.es?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/zurich.es?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/zurich.es?page=5\n",
      "‚úÖ 14 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üîç Verificando nuevas rese√±as de Chapka Seguros (www.chapkadirect.es)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.chapkadirect.es\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.chapkadirect.es?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.chapkadirect.es?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.chapkadirect.es?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.chapkadirect.es?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üîç Verificando nuevas rese√±as de Divina Seguros (www.divinaseguros.com)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.divinaseguros.com\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.divinaseguros.com?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.divinaseguros.com?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.divinaseguros.com?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.divinaseguros.com?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üéâ Dataset actualizado guardado en 'datalake/1_LANDING_ZONE/dataset_categoria2.csv'\n",
      "üì¶ Archivo JSON guardado en 'datalake/1_LANDING_ZONE/reviews_trustpilot_empresas_categoria2.json'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ------------- CONFIG -------------\n",
    "DEBUG = False                 # True = imprime y guarda evidencia (screenshot + html)\n",
    "MAX_PAGES = 5                 # m√°ximo de p√°ginas por empresa\n",
    "EMPRESAS_JSON = 'datalake/1_LANDING_ZONE/trustpilot_empresas_categoria2.json'\n",
    "DATASET_CSV   = 'datalake/1_LANDING_ZONE/dataset_categoria2.csv'\n",
    "SALIDA_JSON   = 'datalake/1_LANDING_ZONE/reviews_trustpilot_empresas_categoria2.json'\n",
    "\n",
    "# ------------- HELPERS -------------\n",
    "def norm_text(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "def parse_estrellas_from_alt(alt_txt: str):\n",
    "    \"\"\"\n",
    "    'Valorada con 4 estrellas sobre 5' -> '4'\n",
    "    \"\"\"\n",
    "    if not alt_txt:\n",
    "        return \"N/A\"\n",
    "    m = re.search(r\"(\\d)\\s*estrellas\", alt_txt, flags=re.I)\n",
    "    return m.group(1) if m else \"N/A\"\n",
    "\n",
    "def normalizar_dominio(pagina_web: str) -> str:\n",
    "    \"\"\"\n",
    "    Espera 'www.midominio.es' o 'midominio.es'.\n",
    "    Quita http(s):// si viene, y slashes al final.\n",
    "    \"\"\"\n",
    "    if not pagina_web:\n",
    "        return \"\"\n",
    "    pw = pagina_web.strip()\n",
    "    pw = re.sub(r\"^https?://\", \"\", pw, flags=re.I)\n",
    "    pw = pw.strip(\"/\")\n",
    "    return pw\n",
    "\n",
    "def extraer_review_uid(card):\n",
    "    \"\"\"\n",
    "    Intenta extraer el id de la rese√±a desde el href del t√≠tulo: '/reviews/<id>'\n",
    "    Devuelve el segmento <id> o '' si no existe.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        a = card.find_element(By.CSS_SELECTOR, 'a[data-review-title-typography=\"true\"]')\n",
    "        href = a.get_attribute(\"href\")\n",
    "        if href:\n",
    "            m = re.search(r\"/reviews/([^/?#]+)\", href)\n",
    "            if m:\n",
    "                return m.group(1)\n",
    "    except:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "def aceptar_cookies_si_aparecen(driver, wait):\n",
    "    \"\"\"\n",
    "    Cierra la barra de cookies de Trustpilot si aparece (bot√≥n 'Entendido' o similar).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        btn = WebDriverWait(driver, 3).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[.//span[normalize-space(text())='Entendido'] or normalize-space(text())='Entendido']\"))\n",
    "        )\n",
    "        btn.click()\n",
    "        time.sleep(0.5)\n",
    "        return True\n",
    "    except:\n",
    "        try:\n",
    "            btn2 = WebDriverWait(driver, 3).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[normalize-space(text())='Aceptar']\"))\n",
    "            )\n",
    "            btn2.click()\n",
    "            time.sleep(0.5)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "# ------------- CARGA EMPRESAS -------------\n",
    "with open(EMPRESAS_JSON, 'r', encoding='utf-8') as f:\n",
    "    empresas_data = json.load(f)\n",
    "\n",
    "# ------------- CARGA / CREA DATASET -------------\n",
    "if os.path.exists(DATASET_CSV):\n",
    "    existing_df = pd.read_csv(DATASET_CSV)\n",
    "else:\n",
    "    existing_df = pd.DataFrame(columns=[\"id_rese√±a\", \"Fecha\", \"T√≠tulo\", \"Contenido\", \"Empresa\", \"Calificaci√≥n\"])\n",
    "\n",
    "existing_df['Fecha'] = pd.to_datetime(existing_df['Fecha'], errors='coerce')\n",
    "\n",
    "# ------------- SELENIUM -------------\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_argument(\"--window-size=1366,2400\")\n",
    "options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    \"(KHTML, like Gecko) Chrome/118.0.5993.70 Safari/537.36\"\n",
    ")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "wait = WebDriverWait(driver, 25)\n",
    "\n",
    "resultados = []\n",
    "reviews_by_company = {}\n",
    "\n",
    "try:\n",
    "    for empresa in empresas_data:\n",
    "        nombre_empresa = empresa.get(\"nombre\", \"N/A\")\n",
    "        pagina_web = normalizar_dominio(empresa.get(\"pagina_web\", \"\"))\n",
    "        if not pagina_web:\n",
    "            print(f\"‚ö†Ô∏è Empresa sin 'pagina_web': {nombre_empresa}. Omitida.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüîç Verificando nuevas rese√±as de {nombre_empresa} ({pagina_web})...\")\n",
    "\n",
    "        # Hist√≥rico de esa empresa\n",
    "        old_df = existing_df[existing_df[\"Empresa\"] == nombre_empresa].copy()\n",
    "        if not old_df.empty:\n",
    "            old_df[\"T√≠tulo\"] = old_df[\"T√≠tulo\"].fillna(\"\").str.strip()\n",
    "            old_df[\"Fecha\"] = pd.to_datetime(old_df[\"Fecha\"], errors=\"coerce\")\n",
    "\n",
    "        rese√±as_nuevas = []\n",
    "\n",
    "        for page in range(1, MAX_PAGES + 1):\n",
    "            review_url = f\"https://es.trustpilot.com/review/{pagina_web}\" + (f\"?page={page}\" if page > 1 else \"\")\n",
    "            print(f\"üåê Accediendo a: {review_url}\")\n",
    "            driver.get(review_url)\n",
    "\n",
    "            # Aceptar cookies si estorban\n",
    "            aceptar_cookies_si_aparecen(driver, wait)\n",
    "\n",
    "            # Espera a que al menos haya 1 card de rese√±a en la p√°gina\n",
    "            try:\n",
    "                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-testid=\"service-review-card-v2\"]')))\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è No se encontraron cards en p√°gina {page}: {e}\")\n",
    "                if DEBUG:\n",
    "                    ss = f\"_debug_reviews_{nombre_empresa}_p{page}_nocards.png\"\n",
    "                    driver.save_screenshot(ss)\n",
    "                    with open(f\"_debug_reviews_{nombre_empresa}_p{page}_nocards.html\", \"w\", encoding=\"utf-8\") as fh:\n",
    "                        fh.write(driver.page_source)\n",
    "                    print(f\"üíæ Guard√© evidencia sin cards: {ss}\")\n",
    "                break\n",
    "\n",
    "            # Forzar render con scrolls suaves (contenido lazy)\n",
    "            for _ in range(3):\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(1.2)\n",
    "\n",
    "            # Cards reales (b√∫squeda global)\n",
    "            cards = driver.find_elements(By.CSS_SELECTOR, 'div[data-testid=\"service-review-card-v2\"]')\n",
    "\n",
    "            if DEBUG:\n",
    "                print(f\"üîé Encontradas {len(cards)} cards en p√°gina {page}\")\n",
    "                try:\n",
    "                    ss_name = f\"_debug_reviews_{nombre_empresa}_p{page}.png\"\n",
    "                    html_name = f\"_debug_reviews_{nombre_empresa}_p{page}.html\"\n",
    "                    driver.save_screenshot(ss_name)\n",
    "                    with open(html_name, \"w\", encoding=\"utf-8\") as fh:\n",
    "                        fh.write(driver.page_source)\n",
    "                    print(f\"üíæ Guard√© screenshot y HTML: {ss_name} / {html_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è No pude guardar debug files: {e}\")\n",
    "\n",
    "            nuevas_esta_pagina = []\n",
    "\n",
    "            for i, card in enumerate(cards, start=1):\n",
    "                try:\n",
    "                    # Fecha\n",
    "                    try:\n",
    "                        time_el = card.find_element(By.CSS_SELECTOR, 'time[datetime]')\n",
    "                        fecha = pd.to_datetime(time_el.get_attribute(\"datetime\"), errors='coerce')\n",
    "                    except:\n",
    "                        fecha = pd.NaT\n",
    "\n",
    "                    # T√≠tulo\n",
    "                    try:\n",
    "                        h2 = card.find_element(By.CSS_SELECTOR, 'h2[data-service-review-title-typography=\"true\"]')\n",
    "                        titulo = norm_text(h2.get_attribute(\"innerText\") or h2.text)\n",
    "                    except:\n",
    "                        titulo = \"N/A\"\n",
    "\n",
    "                    # Contenido\n",
    "                    try:\n",
    "                        p = card.find_element(By.CSS_SELECTOR, 'p[data-service-review-text-typography=\"true\"]')\n",
    "                        contenido = norm_text(p.get_attribute(\"innerText\") or p.text)\n",
    "                    except:\n",
    "                        contenido = \"Sin contenido\"\n",
    "\n",
    "                    # Calificaci√≥n (solo n√∫mero)\n",
    "                    try:\n",
    "                        img = card.find_element(By.CSS_SELECTOR, 'img[alt*=\"Valorada con\"]')\n",
    "                        estrellas = parse_estrellas_from_alt(img.get_attribute(\"alt\"))\n",
    "                    except:\n",
    "                        estrellas = \"N/A\"\n",
    "\n",
    "                    # UID de la review (opcional)\n",
    "                    review_uid = extraer_review_uid(card)\n",
    "\n",
    "                    if DEBUG and i <= 3:\n",
    "                        print(f\"  ‚Ä¢ Card #{i}: fecha={fecha}, estrellas={estrellas}, t√≠tulo='{titulo[:60]}', uid='{review_uid}'\")\n",
    "\n",
    "                    # DEDUP: T√≠tulo + Fecha\n",
    "                    existe = (\n",
    "                        not old_df[\n",
    "                            (old_df[\"T√≠tulo\"].str.strip() == titulo) &\n",
    "                            (old_df[\"Fecha\"] == fecha)\n",
    "                        ].empty\n",
    "                    )\n",
    "\n",
    "                    if not existe:\n",
    "                        nuevas_esta_pagina.append({\n",
    "                            \"Empresa\": nombre_empresa,\n",
    "                            \"T√≠tulo\": titulo,\n",
    "                            \"Contenido\": contenido,\n",
    "                            \"Fecha\": fecha,\n",
    "                            \"Calificaci√≥n\": estrellas\n",
    "                        })\n",
    "\n",
    "                except Exception as e_card:\n",
    "                    if DEBUG:\n",
    "                        print(f\"  ‚Ä¢ Aviso: card con error: {e_card}\")\n",
    "                    continue\n",
    "\n",
    "            if nuevas_esta_pagina:\n",
    "                rese√±as_nuevas.extend(nuevas_esta_pagina)\n",
    "                print(f\"‚úÖ {len(nuevas_esta_pagina)} rese√±as nuevas en p√°gina {page}. Continuando...\")\n",
    "            else:\n",
    "                print(f\"üõë No hay rese√±as nuevas en p√°gina {page}. Deteniendo scraping para esta empresa.\")\n",
    "                break\n",
    "\n",
    "        # Consolidar por empresa\n",
    "        if rese√±as_nuevas:\n",
    "            df_nuevas = pd.DataFrame(rese√±as_nuevas)\n",
    "            df_nuevas['Fecha'] = pd.to_datetime(df_nuevas['Fecha'], errors='coerce')\n",
    "            combined_df = pd.concat([old_df, df_nuevas], ignore_index=True)\n",
    "            combined_df.drop_duplicates(subset=[\"T√≠tulo\", \"Fecha\"], keep='first', inplace=True)\n",
    "            combined_df.sort_values(by=\"Fecha\", ascending=False, inplace=True)\n",
    "            combined_df.reset_index(drop=True, inplace=True)\n",
    "            combined_df.fillna(\"\", inplace=True)\n",
    "\n",
    "            total = len(combined_df)\n",
    "            combined_df[\"id_rese√±a\"] = [f\"{nombre_empresa.replace(' ', '')}_N{total - idx}\" for idx in range(total)]\n",
    "\n",
    "            reviews_by_company[nombre_empresa] = combined_df.to_dict(orient=\"records\")\n",
    "            resultados.append(combined_df)\n",
    "        else:\n",
    "            if not old_df.empty:\n",
    "                reviews_by_company[nombre_empresa] = old_df.to_dict(orient=\"records\")\n",
    "            print(f\"üì≠ No se detectaron nuevas rese√±as para {nombre_empresa}.\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# ------------- SALIDA CSV -------------\n",
    "if resultados:\n",
    "    final_df = pd.concat([existing_df] + resultados, ignore_index=True)\n",
    "    final_df.drop_duplicates(subset=[\"T√≠tulo\", \"Fecha\", \"Empresa\"], keep='first', inplace=True)\n",
    "    final_df.fillna(\"\", inplace=True)\n",
    "\n",
    "    final_df[\"Fecha\"] = pd.to_datetime(final_df[\"Fecha\"], errors=\"coerce\")\n",
    "    final_df.sort_values(by=[\"Empresa\", \"Fecha\"], ascending=[True, False], inplace=True)\n",
    "\n",
    "    final_df[\"id_rese√±a\"] = final_df.groupby(\"Empresa\").cumcount(ascending=False) + 1\n",
    "    final_df[\"id_rese√±a\"] = final_df.apply(\n",
    "        lambda row: f\"{row['Empresa'].replace(' ', '')}_N{row['id_rese√±a']}\", axis=1\n",
    "    )\n",
    "\n",
    "    final_df = final_df[[\"id_rese√±a\", \"Fecha\", \"T√≠tulo\", \"Contenido\", \"Empresa\", \"Calificaci√≥n\"]]\n",
    "\n",
    "    final_df[\"id_num\"] = final_df[\"id_rese√±a\"].str.extract(r'_N(\\d+)').astype(int)\n",
    "    final_df.sort_values(by=[\"Empresa\", \"id_num\"], ascending=[True, False], inplace=True)\n",
    "    final_df.drop(columns=[\"id_num\"], inplace=True)\n",
    "\n",
    "    os.makedirs(os.path.dirname(DATASET_CSV), exist_ok=True)\n",
    "    final_df.to_csv(DATASET_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\nüéâ Dataset actualizado guardado en '{DATASET_CSV}'\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No hubo actualizaciones, el dataset se mantiene igual.\")\n",
    "\n",
    "# ------------- SALIDA JSON (anidado por empresa) -------------\n",
    "for empresa in empresas_data:\n",
    "    nombre_empresa = empresa.get(\"nombre\", \"N/A\")\n",
    "    empresa[\"rese√±as\"] = reviews_by_company.get(nombre_empresa, [])\n",
    "\n",
    "os.makedirs(os.path.dirname(SALIDA_JSON), exist_ok=True)\n",
    "with open(SALIDA_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(empresas_data, f, ensure_ascii=False, indent=4, default=str)\n",
    "\n",
    "print(f\"üì¶ Archivo JSON guardado en '{SALIDA_JSON}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoria 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Verificando nuevas rese√±as de compramostucoche.es (www.compramostucoche.es)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.compramostucoche.es\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.compramostucoche.es?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.compramostucoche.es?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.compramostucoche.es?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.compramostucoche.es?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üîç Verificando nuevas rese√±as de OcasionPlus (ocasionplus.com)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/ocasionplus.com\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/ocasionplus.com?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/ocasionplus.com?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/ocasionplus.com?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/ocasionplus.com?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üîç Verificando nuevas rese√±as de Autohero Espa√±a (autohero.com/es)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/autohero.com/es\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/autohero.com/es?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/autohero.com/es?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/autohero.com/es?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/autohero.com/es?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üîç Verificando nuevas rese√±as de Carwow ES (www.carwow.es)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.carwow.es\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.carwow.es?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.carwow.es?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.carwow.es?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.carwow.es?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üîç Verificando nuevas rese√±as de Carplus Espa√±a (carplus.es)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/carplus.es\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/carplus.es?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/carplus.es?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/carplus.es?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/carplus.es?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üîç Verificando nuevas rese√±as de Hr Motor (www.hrmotor.com)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.hrmotor.com\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.hrmotor.com?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.hrmotor.com?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.hrmotor.com?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.hrmotor.com?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üîç Verificando nuevas rese√±as de Clicars (www.clicars.com)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.clicars.com\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.clicars.com?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.clicars.com?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.clicars.com?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/www.clicars.com?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üîç Verificando nuevas rese√±as de CarNext.com Espa√±a (carnext.com/es-es)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/carnext.com/es-es\n",
      "üõë No hay rese√±as nuevas en p√°gina 1. Deteniendo scraping para esta empresa.\n",
      "üì≠ No se detectaron nuevas rese√±as para CarNext.com Espa√±a.\n",
      "\n",
      "üîç Verificando nuevas rese√±as de Autobiz SA (autobiz-ocasion.es)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/autobiz-ocasion.es\n",
      "‚úÖ 4 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/autobiz-ocasion.es?page=2\n",
      "üõë No hay rese√±as nuevas en p√°gina 2. Deteniendo scraping para esta empresa.\n",
      "\n",
      "üîç Verificando nuevas rese√±as de Garage Club (garageclub.es)...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/garageclub.es\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 1. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/garageclub.es?page=2\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 2. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/garageclub.es?page=3\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 3. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/garageclub.es?page=4\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 4. Continuando...\n",
      "üåê Accediendo a: https://es.trustpilot.com/review/garageclub.es?page=5\n",
      "‚úÖ 20 rese√±as nuevas en p√°gina 5. Continuando...\n",
      "\n",
      "üéâ Dataset actualizado guardado en 'datalake/1_LANDING_ZONE/dataset_categoria3.csv'\n",
      "üì¶ Archivo JSON guardado en 'datalake/1_LANDING_ZONE/reviews_trustpilot_empresas_categoria3.json'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ------------- CONFIG -------------\n",
    "DEBUG = False                 # True = imprime y guarda evidencia (screenshot + html)\n",
    "MAX_PAGES = 5                 # m√°ximo de p√°ginas por empresa\n",
    "EMPRESAS_JSON = 'datalake/1_LANDING_ZONE/trustpilot_empresas_categoria3.json'\n",
    "DATASET_CSV   = 'datalake/1_LANDING_ZONE/dataset_categoria3.csv'\n",
    "SALIDA_JSON   = 'datalake/1_LANDING_ZONE/reviews_trustpilot_empresas_categoria3.json'\n",
    "\n",
    "# ------------- HELPERS -------------\n",
    "def norm_text(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "def parse_estrellas_from_alt(alt_txt: str):\n",
    "    \"\"\"\n",
    "    'Valorada con 4 estrellas sobre 5' -> '4'\n",
    "    \"\"\"\n",
    "    if not alt_txt:\n",
    "        return \"N/A\"\n",
    "    m = re.search(r\"(\\d)\\s*estrellas\", alt_txt, flags=re.I)\n",
    "    return m.group(1) if m else \"N/A\"\n",
    "\n",
    "def normalizar_dominio(pagina_web: str) -> str:\n",
    "    \"\"\"\n",
    "    Espera 'www.midominio.es' o 'midominio.es'.\n",
    "    Quita http(s):// si viene, y slashes al final.\n",
    "    \"\"\"\n",
    "    if not pagina_web:\n",
    "        return \"\"\n",
    "    pw = pagina_web.strip()\n",
    "    pw = re.sub(r\"^https?://\", \"\", pw, flags=re.I)\n",
    "    pw = pw.strip(\"/\")\n",
    "    return pw\n",
    "\n",
    "def extraer_review_uid(card):\n",
    "    \"\"\"\n",
    "    Intenta extraer el id de la rese√±a desde el href del t√≠tulo: '/reviews/<id>'\n",
    "    Devuelve el segmento <id> o '' si no existe.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        a = card.find_element(By.CSS_SELECTOR, 'a[data-review-title-typography=\"true\"]')\n",
    "        href = a.get_attribute(\"href\")\n",
    "        if href:\n",
    "            m = re.search(r\"/reviews/([^/?#]+)\", href)\n",
    "            if m:\n",
    "                return m.group(1)\n",
    "    except:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "def aceptar_cookies_si_aparecen(driver, wait):\n",
    "    \"\"\"\n",
    "    Cierra la barra de cookies de Trustpilot si aparece (bot√≥n 'Entendido' o similar).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        btn = WebDriverWait(driver, 3).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[.//span[normalize-space(text())='Entendido'] or normalize-space(text())='Entendido']\"))\n",
    "        )\n",
    "        btn.click()\n",
    "        time.sleep(0.5)\n",
    "        return True\n",
    "    except:\n",
    "        try:\n",
    "            btn2 = WebDriverWait(driver, 3).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[normalize-space(text())='Aceptar']\"))\n",
    "            )\n",
    "            btn2.click()\n",
    "            time.sleep(0.5)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "# ------------- CARGA EMPRESAS -------------\n",
    "with open(EMPRESAS_JSON, 'r', encoding='utf-8') as f:\n",
    "    empresas_data = json.load(f)\n",
    "\n",
    "# ------------- CARGA / CREA DATASET -------------\n",
    "if os.path.exists(DATASET_CSV):\n",
    "    existing_df = pd.read_csv(DATASET_CSV)\n",
    "else:\n",
    "    existing_df = pd.DataFrame(columns=[\"id_rese√±a\", \"Fecha\", \"T√≠tulo\", \"Contenido\", \"Empresa\", \"Calificaci√≥n\"])\n",
    "\n",
    "existing_df['Fecha'] = pd.to_datetime(existing_df['Fecha'], errors='coerce')\n",
    "\n",
    "# ------------- SELENIUM -------------\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_argument(\"--window-size=1366,2400\")\n",
    "options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    \"(KHTML, like Gecko) Chrome/118.0.5993.70 Safari/537.36\"\n",
    ")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "wait = WebDriverWait(driver, 25)\n",
    "\n",
    "resultados = []\n",
    "reviews_by_company = {}\n",
    "\n",
    "try:\n",
    "    for empresa in empresas_data:\n",
    "        nombre_empresa = empresa.get(\"nombre\", \"N/A\")\n",
    "        pagina_web = normalizar_dominio(empresa.get(\"pagina_web\", \"\"))\n",
    "        if not pagina_web:\n",
    "            print(f\"‚ö†Ô∏è Empresa sin 'pagina_web': {nombre_empresa}. Omitida.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüîç Verificando nuevas rese√±as de {nombre_empresa} ({pagina_web})...\")\n",
    "\n",
    "        # Hist√≥rico de esa empresa\n",
    "        old_df = existing_df[existing_df[\"Empresa\"] == nombre_empresa].copy()\n",
    "        if not old_df.empty:\n",
    "            old_df[\"T√≠tulo\"] = old_df[\"T√≠tulo\"].fillna(\"\").str.strip()\n",
    "            old_df[\"Fecha\"] = pd.to_datetime(old_df[\"Fecha\"], errors=\"coerce\")\n",
    "\n",
    "        rese√±as_nuevas = []\n",
    "\n",
    "        for page in range(1, MAX_PAGES + 1):\n",
    "            review_url = f\"https://es.trustpilot.com/review/{pagina_web}\" + (f\"?page={page}\" if page > 1 else \"\")\n",
    "            print(f\"üåê Accediendo a: {review_url}\")\n",
    "            driver.get(review_url)\n",
    "\n",
    "            # Aceptar cookies si estorban\n",
    "            aceptar_cookies_si_aparecen(driver, wait)\n",
    "\n",
    "            # Espera a que al menos haya 1 card de rese√±a en la p√°gina\n",
    "            try:\n",
    "                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-testid=\"service-review-card-v2\"]')))\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è No se encontraron cards en p√°gina {page}: {e}\")\n",
    "                if DEBUG:\n",
    "                    ss = f\"_debug_reviews_{nombre_empresa}_p{page}_nocards.png\"\n",
    "                    driver.save_screenshot(ss)\n",
    "                    with open(f\"_debug_reviews_{nombre_empresa}_p{page}_nocards.html\", \"w\", encoding=\"utf-8\") as fh:\n",
    "                        fh.write(driver.page_source)\n",
    "                    print(f\"üíæ Guard√© evidencia sin cards: {ss}\")\n",
    "                break\n",
    "\n",
    "            # Forzar render con scrolls suaves (contenido lazy)\n",
    "            for _ in range(3):\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(1.2)\n",
    "\n",
    "            # Cards reales (b√∫squeda global)\n",
    "            cards = driver.find_elements(By.CSS_SELECTOR, 'div[data-testid=\"service-review-card-v2\"]')\n",
    "\n",
    "            if DEBUG:\n",
    "                print(f\"üîé Encontradas {len(cards)} cards en p√°gina {page}\")\n",
    "                try:\n",
    "                    ss_name = f\"_debug_reviews_{nombre_empresa}_p{page}.png\"\n",
    "                    html_name = f\"_debug_reviews_{nombre_empresa}_p{page}.html\"\n",
    "                    driver.save_screenshot(ss_name)\n",
    "                    with open(html_name, \"w\", encoding=\"utf-8\") as fh:\n",
    "                        fh.write(driver.page_source)\n",
    "                    print(f\"üíæ Guard√© screenshot y HTML: {ss_name} / {html_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è No pude guardar debug files: {e}\")\n",
    "\n",
    "            nuevas_esta_pagina = []\n",
    "\n",
    "            for i, card in enumerate(cards, start=1):\n",
    "                try:\n",
    "                    # Fecha\n",
    "                    try:\n",
    "                        time_el = card.find_element(By.CSS_SELECTOR, 'time[datetime]')\n",
    "                        fecha = pd.to_datetime(time_el.get_attribute(\"datetime\"), errors='coerce')\n",
    "                    except:\n",
    "                        fecha = pd.NaT\n",
    "\n",
    "                    # T√≠tulo\n",
    "                    try:\n",
    "                        h2 = card.find_element(By.CSS_SELECTOR, 'h2[data-service-review-title-typography=\"true\"]')\n",
    "                        titulo = norm_text(h2.get_attribute(\"innerText\") or h2.text)\n",
    "                    except:\n",
    "                        titulo = \"N/A\"\n",
    "\n",
    "                    # Contenido\n",
    "                    try:\n",
    "                        p = card.find_element(By.CSS_SELECTOR, 'p[data-service-review-text-typography=\"true\"]')\n",
    "                        contenido = norm_text(p.get_attribute(\"innerText\") or p.text)\n",
    "                    except:\n",
    "                        contenido = \"Sin contenido\"\n",
    "\n",
    "                    # Calificaci√≥n (solo n√∫mero)\n",
    "                    try:\n",
    "                        img = card.find_element(By.CSS_SELECTOR, 'img[alt*=\"Valorada con\"]')\n",
    "                        estrellas = parse_estrellas_from_alt(img.get_attribute(\"alt\"))\n",
    "                    except:\n",
    "                        estrellas = \"N/A\"\n",
    "\n",
    "                    # UID de la review (opcional)\n",
    "                    review_uid = extraer_review_uid(card)\n",
    "\n",
    "                    if DEBUG and i <= 3:\n",
    "                        print(f\"  ‚Ä¢ Card #{i}: fecha={fecha}, estrellas={estrellas}, t√≠tulo='{titulo[:60]}', uid='{review_uid}'\")\n",
    "\n",
    "                    # DEDUP: T√≠tulo + Fecha\n",
    "                    existe = (\n",
    "                        not old_df[\n",
    "                            (old_df[\"T√≠tulo\"].str.strip() == titulo) &\n",
    "                            (old_df[\"Fecha\"] == fecha)\n",
    "                        ].empty\n",
    "                    )\n",
    "\n",
    "                    if not existe:\n",
    "                        nuevas_esta_pagina.append({\n",
    "                            \"Empresa\": nombre_empresa,\n",
    "                            \"T√≠tulo\": titulo,\n",
    "                            \"Contenido\": contenido,\n",
    "                            \"Fecha\": fecha,\n",
    "                            \"Calificaci√≥n\": estrellas\n",
    "                        })\n",
    "\n",
    "                except Exception as e_card:\n",
    "                    if DEBUG:\n",
    "                        print(f\"  ‚Ä¢ Aviso: card con error: {e_card}\")\n",
    "                    continue\n",
    "\n",
    "            if nuevas_esta_pagina:\n",
    "                rese√±as_nuevas.extend(nuevas_esta_pagina)\n",
    "                print(f\"‚úÖ {len(nuevas_esta_pagina)} rese√±as nuevas en p√°gina {page}. Continuando...\")\n",
    "            else:\n",
    "                print(f\"üõë No hay rese√±as nuevas en p√°gina {page}. Deteniendo scraping para esta empresa.\")\n",
    "                break\n",
    "\n",
    "        # Consolidar por empresa\n",
    "        if rese√±as_nuevas:\n",
    "            df_nuevas = pd.DataFrame(rese√±as_nuevas)\n",
    "            df_nuevas['Fecha'] = pd.to_datetime(df_nuevas['Fecha'], errors='coerce')\n",
    "            combined_df = pd.concat([old_df, df_nuevas], ignore_index=True)\n",
    "            combined_df.drop_duplicates(subset=[\"T√≠tulo\", \"Fecha\"], keep='first', inplace=True)\n",
    "            combined_df.sort_values(by=\"Fecha\", ascending=False, inplace=True)\n",
    "            combined_df.reset_index(drop=True, inplace=True)\n",
    "            combined_df.fillna(\"\", inplace=True)\n",
    "\n",
    "            total = len(combined_df)\n",
    "            combined_df[\"id_rese√±a\"] = [f\"{nombre_empresa.replace(' ', '')}_N{total - idx}\" for idx in range(total)]\n",
    "\n",
    "            reviews_by_company[nombre_empresa] = combined_df.to_dict(orient=\"records\")\n",
    "            resultados.append(combined_df)\n",
    "        else:\n",
    "            if not old_df.empty:\n",
    "                reviews_by_company[nombre_empresa] = old_df.to_dict(orient=\"records\")\n",
    "            print(f\"üì≠ No se detectaron nuevas rese√±as para {nombre_empresa}.\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# ------------- SALIDA CSV -------------\n",
    "if resultados:\n",
    "    final_df = pd.concat([existing_df] + resultados, ignore_index=True)\n",
    "    final_df.drop_duplicates(subset=[\"T√≠tulo\", \"Fecha\", \"Empresa\"], keep='first', inplace=True)\n",
    "    final_df.fillna(\"\", inplace=True)\n",
    "\n",
    "    final_df[\"Fecha\"] = pd.to_datetime(final_df[\"Fecha\"], errors=\"coerce\")\n",
    "    final_df.sort_values(by=[\"Empresa\", \"Fecha\"], ascending=[True, False], inplace=True)\n",
    "\n",
    "    final_df[\"id_rese√±a\"] = final_df.groupby(\"Empresa\").cumcount(ascending=False) + 1\n",
    "    final_df[\"id_rese√±a\"] = final_df.apply(\n",
    "        lambda row: f\"{row['Empresa'].replace(' ', '')}_N{row['id_rese√±a']}\", axis=1\n",
    "    )\n",
    "\n",
    "    final_df = final_df[[\"id_rese√±a\", \"Fecha\", \"T√≠tulo\", \"Contenido\", \"Empresa\", \"Calificaci√≥n\"]]\n",
    "\n",
    "    final_df[\"id_num\"] = final_df[\"id_rese√±a\"].str.extract(r'_N(\\d+)').astype(int)\n",
    "    final_df.sort_values(by=[\"Empresa\", \"id_num\"], ascending=[True, False], inplace=True)\n",
    "    final_df.drop(columns=[\"id_num\"], inplace=True)\n",
    "\n",
    "    os.makedirs(os.path.dirname(DATASET_CSV), exist_ok=True)\n",
    "    final_df.to_csv(DATASET_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\nüéâ Dataset actualizado guardado en '{DATASET_CSV}'\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No hubo actualizaciones, el dataset se mantiene igual.\")\n",
    "\n",
    "# ------------- SALIDA JSON (anidado por empresa) -------------\n",
    "for empresa in empresas_data:\n",
    "    nombre_empresa = empresa.get(\"nombre\", \"N/A\")\n",
    "    empresa[\"rese√±as\"] = reviews_by_company.get(nombre_empresa, [])\n",
    "\n",
    "os.makedirs(os.path.dirname(SALIDA_JSON), exist_ok=True)\n",
    "with open(SALIDA_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(empresas_data, f, ensure_ascii=False, indent=4, default=str)\n",
    "\n",
    "print(f\"üì¶ Archivo JSON guardado en '{SALIDA_JSON}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Unir los dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV fusionado guardado como dataset_reviews.csv\n",
      "JSON fusionado guardado como dataset_reviews.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Cargar los datasets CSV\n",
    "df1 = pd.read_csv(\"datalake/1_LANDING_ZONE/dataset_categoria1.csv\")\n",
    "df2 = pd.read_csv(\"datalake/1_LANDING_ZONE/dataset_categoria2.csv\")\n",
    "df3 = pd.read_csv(\"datalake/1_LANDING_ZONE/dataset_categoria3.csv\")  # Nuevo dataset\n",
    "\n",
    "# Agregar columna 'categoria'\n",
    "df1[\"categoria\"] = \"Banco\"\n",
    "df2[\"categoria\"] = \"Seguros de viaje\"\n",
    "df3[\"categoria\"] = \"Concesionario de autos\"  # Nueva categor√≠a\n",
    "\n",
    "# Reordenar las columnas para que 'categoria' aparezca al inicio\n",
    "def reorder_columns(df):\n",
    "    return df[[\"categoria\"] + [col for col in df.columns if col != \"categoria\"]]\n",
    "\n",
    "df1 = reorder_columns(df1)\n",
    "df2 = reorder_columns(df2)\n",
    "df3 = reorder_columns(df3)\n",
    "\n",
    "# Combinar todos los DataFrames\n",
    "merged_csv = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "# Guardar el resultado en un nuevo archivo CSV\n",
    "merged_csv.to_csv(\"datalake/1_LANDING_ZONE/dataset_reviews.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"CSV fusionado guardado como dataset_reviews.csv\")\n",
    "\n",
    "################### JSON ###################\n",
    "\n",
    "# Cargar los archivos JSON de entrada\n",
    "with open(\"datalake/1_LANDING_ZONE/reviews_trustpilot_empresas_categoria1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data1 = json.load(f)\n",
    "with open(\"datalake/1_LANDING_ZONE/reviews_trustpilot_empresas_categoria2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data2 = json.load(f)\n",
    "with open(\"datalake/1_LANDING_ZONE/reviews_trustpilot_empresas_categoria3.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data3 = json.load(f)\n",
    "\n",
    "# Agregar la clave 'categoria' a cada empresa\n",
    "for company in data1:\n",
    "    company[\"categoria\"] = \"Bancos\"\n",
    "for company in data2:\n",
    "    company[\"categoria\"] = \"Seguro de viajes\"\n",
    "for company in data3:\n",
    "    company[\"categoria\"] = \"Concesionario de autos\"\n",
    "\n",
    "# Combinar las listas de empresas\n",
    "merged_data = data1 + data2 + data3\n",
    "\n",
    "# Reordenar las claves para que 'categoria' aparezca al inicio\n",
    "ordered_merged_data = []\n",
    "for company in merged_data:\n",
    "    ordered_company = OrderedDict()\n",
    "    ordered_company[\"categoria\"] = company.get(\"categoria\", \"\")\n",
    "    for key in company:\n",
    "        if key != \"categoria\":\n",
    "            ordered_company[key] = company[key]\n",
    "    ordered_merged_data.append(ordered_company)\n",
    "\n",
    "# Guardar el JSON combinado en un nuevo archivo\n",
    "with open(\"datalake/1_LANDING_ZONE/dataset_reviews.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ordered_merged_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"JSON fusionado guardado como dataset_reviews.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Data Cleaning & Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos limpios y validados guardados en datalake/2_REFINED_ZONE/dataset_reviews_limpio.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from datetime import datetime, timezone\n",
    "import pytz\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Definir rutas de archivos\n",
    "input_file = \"datalake/1_LANDING_ZONE/dataset_reviews.json\"\n",
    "output_file = \"datalake/2_REFINED_ZONE/dataset_reviews_limpio.json\"\n",
    "\n",
    "# Configurar zona horaria de Bolivia (America/La_Paz)\n",
    "bolivia_tz = pytz.timezone(\"America/La_Paz\")\n",
    "\n",
    "def clean_puntuacion(puntuacion):\n",
    "    \"\"\"\n",
    "    Devuelve la puntuaci√≥n como float (>0) o \"\" si no es v√°lida.\n",
    "    - Soporta float/int directamente.\n",
    "    - Si es str, elimina 'TrustScore', cambia coma por punto y castea a float.\n",
    "    \"\"\"\n",
    "    if puntuacion is None:\n",
    "        return \"\"\n",
    "    # Si ya es num√©rico\n",
    "    if isinstance(puntuacion, (int, float)):\n",
    "        try:\n",
    "            val = float(puntuacion)\n",
    "            return val if val > 0 else \"\"\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    # Si es texto\n",
    "    if isinstance(puntuacion, str):\n",
    "        cleaned = puntuacion.replace(\"TrustScore\", \"\").strip().replace(\",\", \".\")\n",
    "        # Si vienen otras palabras, intenta extraer el primer n√∫mero\n",
    "        m = re.search(r\"(\\d+(?:\\.\\d+)?)\", cleaned)\n",
    "        if m:\n",
    "            try:\n",
    "                val = float(m.group(1))\n",
    "                return val if val > 0 else \"\"\n",
    "            except ValueError:\n",
    "                return \"\"\n",
    "        try:\n",
    "            val = float(cleaned)\n",
    "            return val if val > 0 else \"\"\n",
    "        except ValueError:\n",
    "            return \"\"\n",
    "    # Cualquier otro tipo\n",
    "    return \"\"\n",
    "\n",
    "def convert_datetime(dt_str):\n",
    "    \"\"\"\n",
    "    Convierte a zona Bolivia y retorna (fecha_local, hora_local, iso_bolivia).\n",
    "    Acepta strings ISO como '2025-11-08 13:51:34+00:00' o '2025-11-08T13:51:34+00:00'.\n",
    "    Si es naive, asume UTC.\n",
    "    \"\"\"\n",
    "    if not dt_str:\n",
    "        return \"\", \"\", \"\"\n",
    "    s = str(dt_str)\n",
    "    # Normaliza separador\n",
    "    if \" \" in s and \"T\" not in s:\n",
    "        s = s.replace(\" \", \"T\", 1)\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(s)\n",
    "    except Exception:\n",
    "        return \"\", \"\", \"\"\n",
    "    # Si es naive, asumimos UTC\n",
    "    if dt.tzinfo is None:\n",
    "        dt = dt.replace(tzinfo=timezone.utc)\n",
    "    try:\n",
    "        dt_bolivia = dt.astimezone(bolivia_tz)\n",
    "        fecha_local = dt_bolivia.strftime(\"%Y-%m-%d\")\n",
    "        hora_local = dt_bolivia.strftime(\"%H:%M:%S\")\n",
    "        return fecha_local, hora_local, dt_bolivia.isoformat()\n",
    "    except Exception:\n",
    "        return \"\", \"\", \"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Min√∫sculas, quita acentos, remueve caracteres no alfanum√©ricos (y emojis),\n",
    "    colapsa espacios.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Cargar el archivo JSON de entrada\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Procesar cada empresa en el dataset\n",
    "for company in data:\n",
    "    # Asegurar que los campos de nivel empresa sean textos (cuando apliquen)\n",
    "    for key in [\"categoria\", \"nombre\", \"ubicacion\", \"pagina_web\"]:\n",
    "        if key in company and company[key] is not None:\n",
    "            company[key] = str(company[key]).strip()\n",
    "\n",
    "    # Si la ubicaci√≥n es \"N/A\", reemplazar por \"sin ubicacion\"\n",
    "    if company.get(\"ubicacion\", \"\").strip().upper() == \"N/A\":\n",
    "        company[\"ubicacion\"] = \"sin ubicacion\"\n",
    "\n",
    "    # Limpiar el campo 'puntuacion' (ahora puede ser float)\n",
    "    if \"puntuacion\" in company:\n",
    "        company[\"puntuacion\"] = clean_puntuacion(company[\"puntuacion\"])\n",
    "\n",
    "    # Procesar las rese√±as\n",
    "    if \"rese√±as\" in company and isinstance(company[\"rese√±as\"], list):\n",
    "        for review in company[\"rese√±as\"]:\n",
    "            # Normaliza campos base a string\n",
    "            for key in [\"id_rese√±a\", \"T√≠tulo\", \"Contenido\", \"Empresa\"]:\n",
    "                if key in review and review[key] is not None:\n",
    "                    review[key] = str(review[key]).strip()\n",
    "\n",
    "            # Limpiar T√≠tulo y Contenido\n",
    "            if \"T√≠tulo\" in review:\n",
    "                review[\"T√≠tulo\"] = clean_text(review[\"T√≠tulo\"])\n",
    "            if \"Contenido\" in review:\n",
    "                review[\"Contenido\"] = clean_text(review[\"Contenido\"])\n",
    "\n",
    "            # Convertir fecha a zona Bolivia\n",
    "            if \"Fecha\" in review and review[\"Fecha\"]:\n",
    "                fecha_local, hora_local, iso_bolivia = convert_datetime(review[\"Fecha\"])\n",
    "                # Usa min√∫sculas para consistencia con tu dashboard\n",
    "                review[\"fecha_local\"] = fecha_local\n",
    "                review[\"hora_local\"] = hora_local\n",
    "                # Si te sirve conservar el ISO local, puedes guardarlo en otro campo:\n",
    "                # review[\"fecha_iso_bolivia\"] = iso_bolivia\n",
    "\n",
    "            # Validar \"Calificaci√≥n\": entero > 0, tolerando 'N/A', '', etc.\n",
    "            if \"Calificaci√≥n\" in review:\n",
    "                try:\n",
    "                    cal = int(str(review[\"Calificaci√≥n\"]).strip())\n",
    "                    review[\"Calificaci√≥n\"] = cal if cal > 0 else \"\"\n",
    "                except Exception:\n",
    "                    review[\"Calificaci√≥n\"] = \"\"\n",
    "\n",
    "            # Eliminar campos no requeridos\n",
    "            review.pop(\"Fecha\", None)     # ya convertida a fecha_local/hora_local\n",
    "            review.pop(\"Empresa\", None)   # evitar redundancia\n",
    "\n",
    "# Guardar los datos limpios en formato JSON\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4, default=str)\n",
    "\n",
    "print(f\"Datos limpios y validados guardados en {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subir en la base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Data Lake Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!streamlit run datalake/3_CONSUMPTION_ZONE/app.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd C:\\Users\\DANIEL\\Downloads\\M7_PROYECTO_DANIEL_SANCHEZ\\M7_PROYECTO_DANIEL_SANCHEZ\n",
    "#streamlit run datalake\\3_CONSUMPTION_ZONE\\app.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
