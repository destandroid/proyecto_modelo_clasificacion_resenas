{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 2 - Limpieza y TransformaciÃ³n de Datos con Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliotecas utilizadas\n",
    "\n",
    "!pip install scrapy\n",
    "\n",
    "\n",
    "!pip install ipykernel\n",
    "\n",
    "!pip install webdriver-manager\n",
    "\n",
    "!pip install nbconvert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ VERSIONES DE LIBRERÃAS\n",
      "\n",
      "LibrerÃ­a                       VersiÃ³n\n",
      "-------------------------------------------------------\n",
      "streamlit                      1.51.0\n",
      "pandas                         2.3.3\n",
      "plotly                         6.4.0\n",
      "nltk                           3.9.2\n",
      "wordcloud                      1.9.4\n",
      "numpy                          2.2.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DANIEL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers                   4.56.2\n",
      "selenium                       4.38.0\n",
      "webdriver_manager              4.0.2\n",
      "pytz                           2025.2\n",
      "sqlalchemy                     2.0.44\n",
      "os                             Incluida en Python\n",
      "json                           2.0.9\n",
      "re                             2.2.1\n",
      "time                           Incluida en Python\n",
      "collections                    Incluida en Python\n",
      "datetime                       Incluida en Python\n",
      "unicodedata                    Incluida en Python\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "libraries = [\n",
    "    \"streamlit\",\n",
    "    \"pandas\",\n",
    "    \"plotly\",\n",
    "    \"nltk\",\n",
    "    \"wordcloud\",\n",
    "    \"numpy\",\n",
    "    \"transformers\",\n",
    "    \"selenium\",\n",
    "    \"webdriver_manager\",\n",
    "    \"pytz\",\n",
    "    \"sqlalchemy\",\n",
    "    \"os\",\n",
    "    \"json\",\n",
    "    \"re\",\n",
    "    \"time\",\n",
    "    \"collections\",\n",
    "    \"datetime\",\n",
    "    \"unicodedata\",\n",
    "]\n",
    "\n",
    "def get_version(package_name):\n",
    "    try:\n",
    "        base_pkg = package_name.split(\".\")[0]\n",
    "        module = importlib.import_module(base_pkg)\n",
    "        return getattr(module, \"__version__\", \"Incluida en Python\")\n",
    "    except Exception:\n",
    "        return \"VersiÃ³n no disponible\"\n",
    "\n",
    "print(\"ğŸ“„ VERSIONES DE LIBRERÃAS\\n\")\n",
    "print(f\"{'LibrerÃ­a':30} {'VersiÃ³n'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for lib in libraries:\n",
    "    print(f\"{lib:30} {get_version(lib)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T20:00:03.313839Z",
     "iopub.status.busy": "2025-03-22T20:00:03.313377Z",
     "iopub.status.idle": "2025-03-22T20:00:03.323071Z",
     "shell.execute_reply": "2025-03-22T20:00:03.321619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La carpeta 'datalake/1_LANDING_ZONE' ya existe.\n",
      "La carpeta 'datalake/2_REFINED_ZONE' ya existe.\n",
      "La carpeta 'datalake/3_CONSUMPTION_ZONE' ya existe.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Carpetas a crear\n",
    "folders = [\n",
    "    \"datalake/1_LANDING_ZONE\",\n",
    "    \"datalake/2_REFINED_ZONE\",\n",
    "    \"datalake/3_CONSUMPTION_ZONE\",   \n",
    "]\n",
    "\n",
    "# Crear carpetas\n",
    "for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        print(f\"Carpeta '{folder}' creada.\")\n",
    "    else:\n",
    "        print(f\"La carpeta '{folder}' ya existe.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Scrapy Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener informacion de las empresas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoria 1 - Bancos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T20:00:03.366348Z",
     "iopub.status.busy": "2025-03-22T20:00:03.366111Z",
     "iopub.status.idle": "2025-03-22T20:00:31.457454Z",
     "shell.execute_reply": "2025-03-22T20:00:31.455208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Datos extraÃ­dos correctamente:\n",
      "[\n",
      "    {\n",
      "        \"nombre\": \"Aplazame\",\n",
      "        \"ubicacion\": \"Calle Ulises 16-18, Madrid, EspaÃ±a\",\n",
      "        \"puntuacion\": 4.8,\n",
      "        \"pagina_web\": \"https://aplazame.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Bnext\",\n",
      "        \"ubicacion\": \"Calle de Zurbano, 71, Madrid, EspaÃ±a\",\n",
      "        \"puntuacion\": 1.5,\n",
      "        \"pagina_web\": \"https://www.bnext.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"CaixaBank\",\n",
      "        \"ubicacion\": \"Calle Pintor Sorolla, 2-4, Valencia, EspaÃ±a\",\n",
      "        \"puntuacion\": 1.2,\n",
      "        \"pagina_web\": \"https://www.caixabank.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Crealsa\",\n",
      "        \"ubicacion\": \"Carrer de Menorca 19, Planta 7Âª (Edificio Aqua), ValÃ¨ncia, EspaÃ±a\",\n",
      "        \"puntuacion\": 4.4,\n",
      "        \"pagina_web\": \"https://www.crealsa.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"BBVA EspaÃ±a\",\n",
      "        \"ubicacion\": \"EspaÃ±a\",\n",
      "        \"puntuacion\": 1.3,\n",
      "        \"pagina_web\": \"https://bbva.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Banco Sabadell\",\n",
      "        \"ubicacion\": \"EspaÃ±a\",\n",
      "        \"puntuacion\": 1.2,\n",
      "        \"pagina_web\": \"https://bancsabadell.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"ING EspaÃ±a\",\n",
      "        \"ubicacion\": \"Calle VÃ­a de los Poblados, Madrid, EspaÃ±a\",\n",
      "        \"puntuacion\": 1.2,\n",
      "        \"pagina_web\": \"https://ing.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Cetelem EspaÃ±a\",\n",
      "        \"ubicacion\": \"EspaÃ±a\",\n",
      "        \"puntuacion\": 4.2,\n",
      "        \"pagina_web\": \"https://www.cetelem.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"EVO Banco\",\n",
      "        \"ubicacion\": \"Don RamÃ³n de la Cruz 84, Madrid, EspaÃ±a\",\n",
      "        \"puntuacion\": 3.1,\n",
      "        \"pagina_web\": \"https://evobanco.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Santander EspaÃ±a\",\n",
      "        \"ubicacion\": \"EspaÃ±a\",\n",
      "        \"puntuacion\": 1.3,\n",
      "        \"pagina_web\": \"https://www.bancosantander.es\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re, json, os\n",
    "\n",
    "# ----------------------------------\n",
    "# ConfiguraciÃ³n de Chrome\n",
    "# ----------------------------------\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--window-size=1280,2000\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "URL = \"https://es.trustpilot.com/categories/bank?sort=reviews_count\"\n",
    "driver.get(URL)\n",
    "\n",
    "# Espera a que aparezcan las tarjetas de empresa\n",
    "wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a[name=\"business-unit-card\"]')))\n",
    "empresas = driver.find_elements(By.CSS_SELECTOR, 'a[name=\"business-unit-card\"]')[:10]\n",
    "\n",
    "datos = []\n",
    "\n",
    "# ----------------------------------\n",
    "# Helpers\n",
    "# ----------------------------------\n",
    "def limpiar_texto(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "def get_first_text(el, selector_list):\n",
    "    \"\"\"Prueba varios selectores y devuelve el primer innerText vÃ¡lido.\"\"\"\n",
    "    for sel in selector_list:\n",
    "        try:\n",
    "            x = el.find_element(By.CSS_SELECTOR, sel)\n",
    "            txt = x.get_attribute(\"innerText\") or x.text\n",
    "            txt = limpiar_texto(txt)\n",
    "            if txt:\n",
    "                return txt\n",
    "        except:\n",
    "            continue\n",
    "    return \"\"\n",
    "\n",
    "def parse_puntuacion(txt: str):\n",
    "    \"\"\"\n",
    "    Convierte '4,8' o '4.8' a float 4.8.\n",
    "    Si llega '4,8/5' tambiÃ©n funciona.\n",
    "    \"\"\"\n",
    "    if not txt:\n",
    "        return \"N/A\"\n",
    "    m = re.search(r\"(\\d+[.,]\\d+|\\d+)\", txt)\n",
    "    if not m:\n",
    "        return \"N/A\"\n",
    "    num = m.group(1).replace(\",\", \".\")\n",
    "    try:\n",
    "        return float(num)\n",
    "    except:\n",
    "        return \"N/A\"\n",
    "\n",
    "# ----------------------------------\n",
    "# Scraping\n",
    "# ----------------------------------\n",
    "for empresa in empresas:\n",
    "    # --- NOMBRE ---\n",
    "    nombre = get_first_text(\n",
    "        empresa,\n",
    "        [\n",
    "            'p[class*=\"CDS_Typography_heading-\"]',\n",
    "            'p[class*=\"heading-\"]',\n",
    "            'p[class*=\"heading\"]',\n",
    "        ],\n",
    "    )\n",
    "    if not nombre:\n",
    "        nombre = \"N/A\"\n",
    "\n",
    "    # --- UBICACIÃ“N ---\n",
    "    ubicacion = \"\"\n",
    "    # 1) Contenedor actual de ubicaciÃ³n\n",
    "    try:\n",
    "        u = empresa.find_element(By.CSS_SELECTOR, 'div[class*=\"styles_businessLocation__\"] p')\n",
    "        ubicacion = limpiar_texto(u.get_attribute(\"innerText\") or u.text)\n",
    "    except:\n",
    "        ubicacion = \"\"\n",
    "\n",
    "    # 2) Fallback atributo data antiguo\n",
    "    if not ubicacion:\n",
    "        try:\n",
    "            uel = empresa.find_element(By.CSS_SELECTOR, 'span[data-business-location-typography=\"true\"]')\n",
    "            ubicacion = limpiar_texto(uel.get_attribute(\"innerText\") or uel.text)\n",
    "        except:\n",
    "            ubicacion = \"\"\n",
    "\n",
    "    # 3) HeurÃ­stica general como Ãºltimo recurso\n",
    "    if not ubicacion:\n",
    "        try:\n",
    "            candidatos = empresa.find_elements(\n",
    "                By.CSS_SELECTOR,\n",
    "                'p[class*=\"CDS_Typography_body-\"], span[class*=\"CDS_Typography_body-\"], p[dir=\"auto\"]'\n",
    "            )\n",
    "            for c in candidatos:\n",
    "                t = limpiar_texto(c.get_attribute(\"innerText\") or c.text)\n",
    "                if t and (\n",
    "                    \",\" in t\n",
    "                    or re.search(r\"\\d\", t)\n",
    "                    or re.search(r\"(EspaÃ±a|Spain|Madrid|Barcelona|Valencia|Sevilla|Bilbao)\", t, re.I)\n",
    "                ):\n",
    "                    ubicacion = t\n",
    "                    break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if not ubicacion:\n",
    "        ubicacion = \"N/A\"\n",
    "\n",
    "    # --- PUNTUACIÃ“N ---\n",
    "    puntuacion_txt = get_first_text(\n",
    "        empresa,\n",
    "        [\n",
    "            'span[class*=\"styles_trustScore__\"] span',\n",
    "            'span[class*=\"styles_trustScore__\"]',\n",
    "            'span[weight=\"heavy\"] span',\n",
    "            'span[weight=\"heavy\"]',\n",
    "        ],\n",
    "    )\n",
    "    puntuacion = parse_puntuacion(puntuacion_txt)\n",
    "\n",
    "    # --- PÃGINA WEB ---\n",
    "    pagina_web = get_first_text(\n",
    "        empresa,\n",
    "        [\n",
    "            'p[class*=\"styles_websiteUrlDisplayed__\"]',\n",
    "            'p[class*=\"styles_websiteUrlDisplayed\"]',\n",
    "            'p[class*=\"websiteUrl\"]',\n",
    "        ],\n",
    "    )\n",
    "    if pagina_web:\n",
    "        pagina_web = pagina_web.replace(\" \", \"\")\n",
    "        if not pagina_web.lower().startswith((\"http://\", \"https://\")):\n",
    "            pagina_web = \"https://\" + pagina_web\n",
    "    else:\n",
    "        pagina_web = \"N/A\"\n",
    "\n",
    "    datos.append(\n",
    "        {\n",
    "            \"nombre\": nombre,\n",
    "            \"ubicacion\": ubicacion,\n",
    "            \"puntuacion\": puntuacion,\n",
    "            \"pagina_web\": pagina_web,\n",
    "        }\n",
    "    )\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# ----------------------------------\n",
    "# Guardar JSON\n",
    "# ----------------------------------\n",
    "out_path = \"datalake/1_LANDING_ZONE/trustpilot_empresas_categoria1.json\"\n",
    "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(datos, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"âœ… Datos extraÃ­dos correctamente:\")\n",
    "print(json.dumps(datos, indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoria 2 - Seguros de viajes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T20:00:31.462513Z",
     "iopub.status.busy": "2025-03-22T20:00:31.462228Z",
     "iopub.status.idle": "2025-03-22T20:01:00.572020Z",
     "shell.execute_reply": "2025-03-22T20:01:00.570812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Datos extraÃ­dos correctamente:\n",
      "[\n",
      "    {\n",
      "        \"nombre\": \"Heymondo Seguros de Viaje\",\n",
      "        \"ubicacion\": \"Calle Alaba, nÃºmero 140, 2Âº 4Âª, Barcelona, EspaÃ±a\",\n",
      "        \"puntuacion\": 4.5,\n",
      "        \"pagina_web\": \"https://heymondo.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Intermundial Seguros de viaje\",\n",
      "        \"ubicacion\": \"Calle de IrÃºn, 7, Madrid, EspaÃ±a\",\n",
      "        \"puntuacion\": 4.6,\n",
      "        \"pagina_web\": \"https://intermundial.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"IATI Seguros\",\n",
      "        \"ubicacion\": \"Avinguda Diagonal 622, Barcelona, EspaÃ±a\",\n",
      "        \"puntuacion\": 4.4,\n",
      "        \"pagina_web\": \"https://iatiseguros.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Abbeygate Seguros EspaÃ±a\",\n",
      "        \"ubicacion\": \"Ctra Nacional 340, KM148.5, Estepona, EspaÃ±a\",\n",
      "        \"puntuacion\": 4.8,\n",
      "        \"pagina_web\": \"https://www.abbeygateinsure.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"MAPFRE EspaÃ±a\",\n",
      "        \"ubicacion\": \"Carretera de Pozuelo 52, Majadahonda, EspaÃ±a\",\n",
      "        \"puntuacion\": 1.5,\n",
      "        \"pagina_web\": \"https://mapfre.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Fit 2 Trip\",\n",
      "        \"ubicacion\": \"Irun 7, Madrid, EspaÃ±a\",\n",
      "        \"puntuacion\": 4.5,\n",
      "        \"pagina_web\": \"https://www.fit2trip.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Pelayo Mutua de Seguros\",\n",
      "        \"ubicacion\": \"Calle de Santa Engracia 67, Madrid, EspaÃ±a\",\n",
      "        \"puntuacion\": 1.1,\n",
      "        \"pagina_web\": \"https://pelayo.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Zurich\",\n",
      "        \"ubicacion\": \"Via Augusta 200, Barcelona, EspaÃ±a\",\n",
      "        \"puntuacion\": 1.1,\n",
      "        \"pagina_web\": \"https://zurich.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Chapka Seguros\",\n",
      "        \"ubicacion\": \"C/ VelÃ¡zquez, 86D, Madrid, EspaÃ±a\",\n",
      "        \"puntuacion\": 4.5,\n",
      "        \"pagina_web\": \"https://www.chapkadirect.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Divina Seguros\",\n",
      "        \"ubicacion\": \"EspaÃ±a\",\n",
      "        \"puntuacion\": 4.0,\n",
      "        \"pagina_web\": \"https://www.divinaseguros.com\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re, json, os\n",
    "\n",
    "# ----------------------------------\n",
    "# ConfiguraciÃ³n de Chrome\n",
    "# ----------------------------------\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")     # headless moderno\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--window-size=1280,2000\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "URL = \"https://es.trustpilot.com/categories/travel_insurance_company?sort=reviews_count\"\n",
    "driver.get(URL)\n",
    "\n",
    "# Espera a que aparezcan las tarjetas de empresa\n",
    "wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a[name=\"business-unit-card\"]')))\n",
    "empresas = driver.find_elements(By.CSS_SELECTOR, 'a[name=\"business-unit-card\"]')[:10]  # top N\n",
    "datos = []\n",
    "\n",
    "# ----------------------------------\n",
    "# Helpers\n",
    "# ----------------------------------\n",
    "def limpiar_texto(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "def get_first_text(el, selector_list):\n",
    "    \"\"\"Prueba varios selectores y devuelve el primer innerText vÃ¡lido.\"\"\"\n",
    "    for sel in selector_list:\n",
    "        try:\n",
    "            x = el.find_element(By.CSS_SELECTOR, sel)\n",
    "            txt = x.get_attribute(\"innerText\") or x.text\n",
    "            txt = limpiar_texto(txt)\n",
    "            if txt:\n",
    "                return txt\n",
    "        except:\n",
    "            continue\n",
    "    return \"\"\n",
    "\n",
    "def parse_puntuacion(txt: str):\n",
    "    \"\"\"\n",
    "    Convierte '4,8' o '4.8' a float 4.8.\n",
    "    Soporta '4,8/5' o con saltos de lÃ­nea.\n",
    "    \"\"\"\n",
    "    if not txt:\n",
    "        return \"N/A\"\n",
    "    m = re.search(r\"(\\d+[.,]\\d+|\\d+)\", txt)\n",
    "    if not m:\n",
    "        return \"N/A\"\n",
    "    num = m.group(1).replace(\",\", \".\")\n",
    "    try:\n",
    "        return float(num)\n",
    "    except:\n",
    "        return \"N/A\"\n",
    "\n",
    "# ----------------------------------\n",
    "# Scraping\n",
    "# ----------------------------------\n",
    "for empresa in empresas:\n",
    "    # --- NOMBRE ---\n",
    "    nombre = get_first_text(\n",
    "        empresa,\n",
    "        [\n",
    "            'p[class*=\"CDS_Typography_heading-\"]',\n",
    "            'p[class*=\"heading-\"]',\n",
    "            'p[class*=\"heading\"]',\n",
    "        ],\n",
    "    ) or \"N/A\"\n",
    "\n",
    "    # --- UBICACIÃ“N ---\n",
    "    ubicacion = \"\"\n",
    "    # 1) Contenedor tÃ­pico de ubicaciÃ³n\n",
    "    try:\n",
    "        u = empresa.find_element(By.CSS_SELECTOR, 'div[class*=\"styles_businessLocation__\"] p')\n",
    "        ubicacion = limpiar_texto(u.get_attribute(\"innerText\") or u.text)\n",
    "    except:\n",
    "        ubicacion = \"\"\n",
    "\n",
    "    # 2) Fallback antiguo por atributo data\n",
    "    if not ubicacion:\n",
    "        try:\n",
    "            uel = empresa.find_element(By.CSS_SELECTOR, 'span[data-business-location-typography=\"true\"]')\n",
    "            ubicacion = limpiar_texto(uel.get_attribute(\"innerText\") or uel.text)\n",
    "        except:\n",
    "            ubicacion = \"\"\n",
    "\n",
    "    # 3) Fallback genÃ©rico (por si cambian el DOM)\n",
    "    if not ubicacion:\n",
    "        try:\n",
    "            candidatos = empresa.find_elements(\n",
    "                By.CSS_SELECTOR,\n",
    "                'p[class*=\"CDS_Typography_body-\"], span[class*=\"CDS_Typography_body-\"], p[dir=\"auto\"]'\n",
    "            )\n",
    "            for c in candidatos:\n",
    "                t = limpiar_texto(c.get_attribute(\"innerText\") or c.text)\n",
    "                # acepta paÃ­s/ciudad o textos con coma/nÃºmero\n",
    "                if t and (\n",
    "                    \",\" in t\n",
    "                    or re.search(r\"\\d\", t)\n",
    "                    or re.search(r\"(EspaÃ±a|Spain|Madrid|Barcelona|Valencia|Sevilla|Bilbao)\", t, re.I)\n",
    "                ):\n",
    "                    ubicacion = t\n",
    "                    break\n",
    "        except:\n",
    "            pass\n",
    "    if not ubicacion:\n",
    "        ubicacion = \"N/A\"\n",
    "\n",
    "    # --- PUNTUACIÃ“N ---\n",
    "    puntuacion_txt = get_first_text(\n",
    "        empresa,\n",
    "        [\n",
    "            'span[class*=\"styles_trustScore__\"] span',\n",
    "            'span[class*=\"styles_trustScore__\"]',\n",
    "            'span[weight=\"heavy\"] span',\n",
    "            'span[weight=\"heavy\"]',\n",
    "        ],\n",
    "    )\n",
    "    puntuacion = parse_puntuacion(puntuacion_txt)\n",
    "\n",
    "    # --- PÃGINA WEB ---\n",
    "    pagina_web = get_first_text(\n",
    "        empresa,\n",
    "        [\n",
    "            'p[class*=\"styles_websiteUrlDisplayed__\"]',\n",
    "            'p[class*=\"styles_websiteUrlDisplayed\"]',\n",
    "            'p[class*=\"websiteUrl\"]',\n",
    "        ],\n",
    "    )\n",
    "    if pagina_web:\n",
    "        pagina_web = pagina_web.replace(\" \", \"\")\n",
    "        if not pagina_web.lower().startswith((\"http://\", \"https://\")):\n",
    "            pagina_web = \"https://\" + pagina_web\n",
    "    else:\n",
    "        pagina_web = \"N/A\"\n",
    "\n",
    "    datos.append(\n",
    "        {\n",
    "            \"nombre\": nombre,\n",
    "            \"ubicacion\": ubicacion,\n",
    "            \"puntuacion\": puntuacion,\n",
    "            \"pagina_web\": pagina_web,\n",
    "        }\n",
    "    )\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# ----------------------------------\n",
    "# Guardar JSON\n",
    "# ----------------------------------\n",
    "out_path = \"datalake/1_LANDING_ZONE/trustpilot_empresas_categoria2.json\"\n",
    "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(datos, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"âœ… Datos extraÃ­dos correctamente:\")\n",
    "print(json.dumps(datos, indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoria 3 - Concesionario de autos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Datos extraÃ­dos correctamente:\n",
      "[\n",
      "    {\n",
      "        \"nombre\": \"compramostucoche.es\",\n",
      "        \"ubicacion\": \"EspaÃ±a\",\n",
      "        \"puntuacion\": 4.6,\n",
      "        \"pagina_web\": \"https://www.compramostucoche.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"OcasionPlus\",\n",
      "        \"ubicacion\": \"Avenida Juan Carlos I 30, Collado Villalba, EspaÃ±a\",\n",
      "        \"puntuacion\": 4.1,\n",
      "        \"pagina_web\": \"https://ocasionplus.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Autohero EspaÃ±a\",\n",
      "        \"ubicacion\": \"Calle Rosario Pino 14-16 Planta 1, Madrid, EspaÃ±a\",\n",
      "        \"puntuacion\": 4.2,\n",
      "        \"pagina_web\": \"https://autohero.com/es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Carwow ES\",\n",
      "        \"ubicacion\": \"C. de Serrano Anguita, 13, Madrid, EspaÃ±a\",\n",
      "        \"puntuacion\": 4.4,\n",
      "        \"pagina_web\": \"https://www.carwow.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Carplus EspaÃ±a\",\n",
      "        \"ubicacion\": \"EspaÃ±a\",\n",
      "        \"puntuacion\": 4.6,\n",
      "        \"pagina_web\": \"https://carplus.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Hr Motor\",\n",
      "        \"ubicacion\": \"EspaÃ±a\",\n",
      "        \"puntuacion\": 3.4,\n",
      "        \"pagina_web\": \"https://www.hrmotor.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Clicars\",\n",
      "        \"ubicacion\": \"Av. Laboral 10, Madrid, EspaÃ±a\",\n",
      "        \"puntuacion\": 4.3,\n",
      "        \"pagina_web\": \"https://www.clicars.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"CarNext.com EspaÃ±a\",\n",
      "        \"ubicacion\": \"EspaÃ±a\",\n",
      "        \"puntuacion\": 4.7,\n",
      "        \"pagina_web\": \"https://carnext.com/es-es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Autobiz SA\",\n",
      "        \"ubicacion\": \"EspaÃ±a\",\n",
      "        \"puntuacion\": 2.4,\n",
      "        \"pagina_web\": \"https://autobiz-ocasion.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Garage Club\",\n",
      "        \"ubicacion\": \"Carrer Catalunya, 37, Les Franqueses del VallÃ©s (Barcelona), EspaÃ±a\",\n",
      "        \"puntuacion\": 4.0,\n",
      "        \"pagina_web\": \"https://garageclub.es\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re, json, os\n",
    "\n",
    "# ----------------------------------\n",
    "# ConfiguraciÃ³n de Chrome\n",
    "# ----------------------------------\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")     # headless moderno\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--window-size=1280,2000\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "URL = \"https://es.trustpilot.com/categories/car_dealer?sort=reviews_count\"\n",
    "driver.get(URL)\n",
    "\n",
    "# Espera a que aparezcan las tarjetas de empresa\n",
    "wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a[name=\"business-unit-card\"]')))\n",
    "empresas = driver.find_elements(By.CSS_SELECTOR, 'a[name=\"business-unit-card\"]')[:10]  # top N\n",
    "datos = []\n",
    "\n",
    "# ----------------------------------\n",
    "# Helpers\n",
    "# ----------------------------------\n",
    "def limpiar_texto(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "def get_first_text(el, selector_list):\n",
    "    \"\"\"Prueba varios selectores y devuelve el primer innerText vÃ¡lido.\"\"\"\n",
    "    for sel in selector_list:\n",
    "        try:\n",
    "            x = el.find_element(By.CSS_SELECTOR, sel)\n",
    "            txt = x.get_attribute(\"innerText\") or x.text\n",
    "            txt = limpiar_texto(txt)\n",
    "            if txt:\n",
    "                return txt\n",
    "        except:\n",
    "            continue\n",
    "    return \"\"\n",
    "\n",
    "def parse_puntuacion(txt: str):\n",
    "    \"\"\"\n",
    "    Convierte '4,8' o '4.8' a float 4.8.\n",
    "    Soporta '4,8/5' o con saltos de lÃ­nea.\n",
    "    \"\"\"\n",
    "    if not txt:\n",
    "        return \"N/A\"\n",
    "    m = re.search(r\"(\\d+[.,]\\d+|\\d+)\", txt)\n",
    "    if not m:\n",
    "        return \"N/A\"\n",
    "    num = m.group(1).replace(\",\", \".\")\n",
    "    try:\n",
    "        return float(num)\n",
    "    except:\n",
    "        return \"N/A\"\n",
    "\n",
    "# ----------------------------------\n",
    "# Scraping\n",
    "# ----------------------------------\n",
    "for empresa in empresas:\n",
    "    # --- NOMBRE ---\n",
    "    nombre = get_first_text(\n",
    "        empresa,\n",
    "        [\n",
    "            'p[class*=\"CDS_Typography_heading-\"]',\n",
    "            'p[class*=\"heading-\"]',\n",
    "            'p[class*=\"heading\"]',\n",
    "        ],\n",
    "    ) or \"N/A\"\n",
    "\n",
    "    # --- UBICACIÃ“N ---\n",
    "    ubicacion = \"\"\n",
    "    # 1) Contenedor tÃ­pico de ubicaciÃ³n\n",
    "    try:\n",
    "        u = empresa.find_element(By.CSS_SELECTOR, 'div[class*=\"styles_businessLocation__\"] p')\n",
    "        ubicacion = limpiar_texto(u.get_attribute(\"innerText\") or u.text)\n",
    "    except:\n",
    "        ubicacion = \"\"\n",
    "\n",
    "    # 2) Fallback atributo data antiguo\n",
    "    if not ubicacion:\n",
    "        try:\n",
    "            uel = empresa.find_element(By.CSS_SELECTOR, 'span[data-business-location-typography=\"true\"]')\n",
    "            ubicacion = limpiar_texto(uel.get_attribute(\"innerText\") or uel.text)\n",
    "        except:\n",
    "            ubicacion = \"\"\n",
    "\n",
    "    # 3) Fallback genÃ©rico (por si cambian el DOM)\n",
    "    if not ubicacion:\n",
    "        try:\n",
    "            candidatos = empresa.find_elements(\n",
    "                By.CSS_SELECTOR,\n",
    "                'p[class*=\"CDS_Typography_body-\"], span[class*=\"CDS_Typography_body-\"], p[dir=\"auto\"]'\n",
    "            )\n",
    "            for c in candidatos:\n",
    "                t = limpiar_texto(c.get_attribute(\"innerText\") or c.text)\n",
    "                # acepta paÃ­s/ciudad o textos con coma/nÃºmero\n",
    "                if t and (\n",
    "                    \",\" in t\n",
    "                    or re.search(r\"\\d\", t)\n",
    "                    or re.search(r\"(EspaÃ±a|Spain|Madrid|Barcelona|Valencia|Sevilla|Bilbao)\", t, re.I)\n",
    "                ):\n",
    "                    ubicacion = t\n",
    "                    break\n",
    "        except:\n",
    "            pass\n",
    "    if not ubicacion:\n",
    "        ubicacion = \"N/A\"\n",
    "\n",
    "    # --- PUNTUACIÃ“N ---\n",
    "    puntuacion_txt = get_first_text(\n",
    "        empresa,\n",
    "        [\n",
    "            'span[class*=\"styles_trustScore__\"] span',\n",
    "            'span[class*=\"styles_trustScore__\"]',\n",
    "            'span[weight=\"heavy\"] span',\n",
    "            'span[weight=\"heavy\"]',\n",
    "        ],\n",
    "    )\n",
    "    puntuacion = parse_puntuacion(puntuacion_txt)\n",
    "\n",
    "    # --- PÃGINA WEB ---\n",
    "    pagina_web = get_first_text(\n",
    "        empresa,\n",
    "        [\n",
    "            'p[class*=\"styles_websiteUrlDisplayed__\"]',\n",
    "            'p[class*=\"styles_websiteUrlDisplayed\"]',\n",
    "            'p[class*=\"websiteUrl\"]',\n",
    "        ],\n",
    "    )\n",
    "    if pagina_web:\n",
    "        pagina_web = pagina_web.replace(\" \", \"\")\n",
    "        if not pagina_web.lower().startswith((\"http://\", \"https://\")):\n",
    "            pagina_web = \"https://\" + pagina_web\n",
    "    else:\n",
    "        pagina_web = \"N/A\"\n",
    "\n",
    "    datos.append(\n",
    "        {\n",
    "            \"nombre\": nombre,\n",
    "            \"ubicacion\": ubicacion,\n",
    "            \"puntuacion\": puntuacion,\n",
    "            \"pagina_web\": pagina_web,\n",
    "        }\n",
    "    )\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# ----------------------------------\n",
    "# Guardar JSON\n",
    "# ----------------------------------\n",
    "out_path = \"datalake/1_LANDING_ZONE/trustpilot_empresas_categoria3.json\"\n",
    "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(datos, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"âœ… Datos extraÃ­dos correctamente:\")\n",
    "print(json.dumps(datos, indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoria 4 - Tienda de ropa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Datos extraÃ­dos correctamente:\n",
      "[\n",
      "    {\n",
      "        \"nombre\": \"Bershka\",\n",
      "        \"ubicacion\": \"EspaÃ±a\",\n",
      "        \"puntuacion\": 3.1,\n",
      "        \"pagina_web\": \"https://www.bershka.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"latostadora\",\n",
      "        \"ubicacion\": \"EspaÃ±a\",\n",
      "        \"puntuacion\": 4.6,\n",
      "        \"pagina_web\": \"https://latostadora.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Hawkers\",\n",
      "        \"ubicacion\": \"EspaÃ±a\",\n",
      "        \"puntuacion\": 4.3,\n",
      "        \"pagina_web\": \"https://hawkersco.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Dressinn\",\n",
      "        \"ubicacion\": \"Calle Pirineus 9, CelrÃ , EspaÃ±a\",\n",
      "        \"puntuacion\": 4.0,\n",
      "        \"pagina_web\": \"https://dressinn.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"ZARA\",\n",
      "        \"ubicacion\": \"EspaÃ±a\",\n",
      "        \"puntuacion\": 1.3,\n",
      "        \"pagina_web\": \"https://www.zara.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"TWOTHIRDS\",\n",
      "        \"ubicacion\": \"Carrer de Morales 21-27, Local K, , Barcelona, EspaÃ±a\",\n",
      "        \"puntuacion\": 4.5,\n",
      "        \"pagina_web\": \"https://twothirds.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Streetprorunning\",\n",
      "        \"ubicacion\": \"EspaÃ±a\",\n",
      "        \"puntuacion\": 4.7,\n",
      "        \"pagina_web\": \"https://streetprorunning.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Scuffers\",\n",
      "        \"ubicacion\": \"Pozuelo de AlarcÃ³n, Madrid, EspaÃ±a\",\n",
      "        \"puntuacion\": 3.6,\n",
      "        \"pagina_web\": \"https://scuffers.es\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Funidelia\",\n",
      "        \"ubicacion\": \"EspaÃ±a\",\n",
      "        \"puntuacion\": 4.3,\n",
      "        \"pagina_web\": \"https://funidelia.com\"\n",
      "    },\n",
      "    {\n",
      "        \"nombre\": \"Vinted ES\",\n",
      "        \"ubicacion\": \"EspaÃ±a\",\n",
      "        \"puntuacion\": 4.2,\n",
      "        \"pagina_web\": \"https://vinted.es\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re, json, os\n",
    "\n",
    "# ----------------------------------\n",
    "# ConfiguraciÃ³n de Chrome\n",
    "# ----------------------------------\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")     # headless moderno\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--window-size=1280,2000\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "URL = \"https://es.trustpilot.com/categories/clothing_store?sort=reviews_count\"\n",
    "driver.get(URL)\n",
    "\n",
    "# Espera a que aparezcan las tarjetas de empresa\n",
    "wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a[name=\"business-unit-card\"]')))\n",
    "empresas = driver.find_elements(By.CSS_SELECTOR, 'a[name=\"business-unit-card\"]')[:10]  # top N\n",
    "datos = []\n",
    "\n",
    "# ----------------------------------\n",
    "# Helpers\n",
    "# ----------------------------------\n",
    "def limpiar_texto(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "def get_first_text(el, selector_list):\n",
    "    \"\"\"Prueba varios selectores y devuelve el primer innerText vÃ¡lido.\"\"\"\n",
    "    for sel in selector_list:\n",
    "        try:\n",
    "            x = el.find_element(By.CSS_SELECTOR, sel)\n",
    "            txt = x.get_attribute(\"innerText\") or x.text\n",
    "            txt = limpiar_texto(txt)\n",
    "            if txt:\n",
    "                return txt\n",
    "        except:\n",
    "            continue\n",
    "    return \"\"\n",
    "\n",
    "def parse_puntuacion(txt: str):\n",
    "    \"\"\"\n",
    "    Convierte '4,8' o '4.8' a float 4.8.\n",
    "    Soporta '4,8/5' o con saltos de lÃ­nea.\n",
    "    \"\"\"\n",
    "    if not txt:\n",
    "        return \"N/A\"\n",
    "    m = re.search(r\"(\\d+[.,]\\d+|\\d+)\", txt)\n",
    "    if not m:\n",
    "        return \"N/A\"\n",
    "    num = m.group(1).replace(\",\", \".\")\n",
    "    try:\n",
    "        return float(num)\n",
    "    except:\n",
    "        return \"N/A\"\n",
    "\n",
    "# ----------------------------------\n",
    "# Scraping\n",
    "# ----------------------------------\n",
    "for empresa in empresas:\n",
    "    # --- NOMBRE ---\n",
    "    nombre = get_first_text(\n",
    "        empresa,\n",
    "        [\n",
    "            'p[class*=\"CDS_Typography_heading-\"]',\n",
    "            'p[class*=\"heading-\"]',\n",
    "            'p[class*=\"heading\"]',\n",
    "        ],\n",
    "    ) or \"N/A\"\n",
    "\n",
    "    # --- UBICACIÃ“N ---\n",
    "    ubicacion = \"\"\n",
    "    # 1) Contenedor tÃ­pico de ubicaciÃ³n\n",
    "    try:\n",
    "        u = empresa.find_element(By.CSS_SELECTOR, 'div[class*=\"styles_businessLocation__\"] p')\n",
    "        ubicacion = limpiar_texto(u.get_attribute(\"innerText\") or u.text)\n",
    "    except:\n",
    "        ubicacion = \"\"\n",
    "\n",
    "    # 2) Fallback atributo data antiguo\n",
    "    if not ubicacion:\n",
    "        try:\n",
    "            uel = empresa.find_element(By.CSS_SELECTOR, 'span[data-business-location-typography=\"true\"]')\n",
    "            ubicacion = limpiar_texto(uel.get_attribute(\"innerText\") or uel.text)\n",
    "        except:\n",
    "            ubicacion = \"\"\n",
    "\n",
    "    # 3) Fallback genÃ©rico (por si cambian el DOM)\n",
    "    if not ubicacion:\n",
    "        try:\n",
    "            candidatos = empresa.find_elements(\n",
    "                By.CSS_SELECTOR,\n",
    "                'p[class*=\"CDS_Typography_body-\"], span[class*=\"CDS_Typography_body-\"], p[dir=\"auto\"]'\n",
    "            )\n",
    "            for c in candidatos:\n",
    "                t = limpiar_texto(c.get_attribute(\"innerText\") or c.text)\n",
    "                # acepta paÃ­s/ciudad o textos con coma/nÃºmero\n",
    "                if t and (\n",
    "                    \",\" in t\n",
    "                    or re.search(r\"\\d\", t)\n",
    "                    or re.search(r\"(EspaÃ±a|Spain|Madrid|Barcelona|Valencia|Sevilla|Bilbao)\", t, re.I)\n",
    "                ):\n",
    "                    ubicacion = t\n",
    "                    break\n",
    "        except:\n",
    "            pass\n",
    "    if not ubicacion:\n",
    "        ubicacion = \"N/A\"\n",
    "\n",
    "    # --- PUNTUACIÃ“N ---\n",
    "    puntuacion_txt = get_first_text(\n",
    "        empresa,\n",
    "        [\n",
    "            'span[class*=\"styles_trustScore__\"] span',\n",
    "            'span[class*=\"styles_trustScore__\"]',\n",
    "            'span[weight=\"heavy\"] span',\n",
    "            'span[weight=\"heavy\"]',\n",
    "        ],\n",
    "    )\n",
    "    puntuacion = parse_puntuacion(puntuacion_txt)\n",
    "\n",
    "    # --- PÃGINA WEB ---\n",
    "    pagina_web = get_first_text(\n",
    "        empresa,\n",
    "        [\n",
    "            'p[class*=\"styles_websiteUrlDisplayed__\"]',\n",
    "            'p[class*=\"styles_websiteUrlDisplayed\"]',\n",
    "            'p[class*=\"websiteUrl\"]',\n",
    "        ],\n",
    "    )\n",
    "    if pagina_web:\n",
    "        pagina_web = pagina_web.replace(\" \", \"\")\n",
    "        if not pagina_web.lower().startswith((\"http://\", \"https://\")):\n",
    "            pagina_web = \"https://\" + pagina_web\n",
    "    else:\n",
    "        pagina_web = \"N/A\"\n",
    "\n",
    "    datos.append(\n",
    "        {\n",
    "            \"nombre\": nombre,\n",
    "            \"ubicacion\": ubicacion,\n",
    "            \"puntuacion\": puntuacion,\n",
    "            \"pagina_web\": pagina_web,\n",
    "        }\n",
    "    )\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# ----------------------------------\n",
    "# Guardar JSON\n",
    "# ----------------------------------\n",
    "out_path = \"datalake/1_LANDING_ZONE/trustpilot_empresas_categoria4.json\"\n",
    "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(datos, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"âœ… Datos extraÃ­dos correctamente:\")\n",
    "print(json.dumps(datos, indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping de las reviews de las empresas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoria 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap - Paginado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Aplazame (aplazame.com)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/aplazame.com\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/aplazame.com?page=2\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 2. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/aplazame.com?page=3\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 3. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/aplazame.com?page=4\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 4. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/aplazame.com?page=5\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 5. Continuando...\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Bnext (www.bnext.es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.bnext.es\n",
      "âœ… 1 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.bnext.es?page=2\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 2. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de CaixaBank (www.caixabank.es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.caixabank.es\n",
      "âœ… 4 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.caixabank.es?page=2\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 2. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Crealsa (www.crealsa.es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.crealsa.es\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 1. Deteniendo scraping para esta empresa.\n",
      "ğŸ“­ No se detectaron nuevas reseÃ±as para Crealsa.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de BBVA EspaÃ±a (bbva.es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/bbva.es\n",
      "âœ… 8 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/bbva.es?page=2\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 2. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Banco Sabadell (bancsabadell.com)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/bancsabadell.com\n",
      "âœ… 6 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/bancsabadell.com?page=2\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 2. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de ING EspaÃ±a (ing.es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/ing.es\n",
      "âœ… 6 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/ing.es?page=2\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 2. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Cetelem EspaÃ±a (www.cetelem.es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.cetelem.es\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.cetelem.es?page=2\n",
      "âœ… 1 reseÃ±as nuevas en pÃ¡gina 2. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.cetelem.es?page=3\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 3. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de EVO Banco (evobanco.com)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/evobanco.com\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 1. Deteniendo scraping para esta empresa.\n",
      "ğŸ“­ No se detectaron nuevas reseÃ±as para EVO Banco.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Santander EspaÃ±a (www.bancosantander.es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.bancosantander.es\n",
      "âœ… 3 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.bancosantander.es?page=2\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 2. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ‰ Dataset actualizado guardado en 'datalake/1_LANDING_ZONE/dataset_categoria1.csv'\n",
      "ğŸ“¦ Archivo JSON guardado en 'datalake/1_LANDING_ZONE/reviews_trustpilot_empresas_categoria1.json'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ------------- CONFIG -------------\n",
    "DEBUG = False                 # True = imprime y guarda evidencia (screenshot + html)\n",
    "MAX_PAGES = 5               # mÃ¡ximo de pÃ¡ginas por empresa\n",
    "EMPRESAS_JSON = 'datalake/1_LANDING_ZONE/trustpilot_empresas_categoria1.json'\n",
    "DATASET_CSV   = 'datalake/1_LANDING_ZONE/dataset_categoria1.csv'\n",
    "SALIDA_JSON   = 'datalake/1_LANDING_ZONE/reviews_trustpilot_empresas_categoria1.json'\n",
    "\n",
    "# ------------- HELPERS -------------\n",
    "def norm_text(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "def parse_estrellas_from_alt(alt_txt: str):\n",
    "    \"\"\"\n",
    "    'Valorada con 4 estrellas sobre 5' -> '4'\n",
    "    \"\"\"\n",
    "    if not alt_txt:\n",
    "        return \"N/A\"\n",
    "    m = re.search(r\"(\\d)\\s*estrellas\", alt_txt, flags=re.I)\n",
    "    return m.group(1) if m else \"N/A\"\n",
    "\n",
    "def normalizar_dominio(pagina_web: str) -> str:\n",
    "    \"\"\"\n",
    "    Espera 'www.midominio.es' o 'midominio.es'.\n",
    "    Quita http(s):// si viene, y slashes al final.\n",
    "    \"\"\"\n",
    "    if not pagina_web:\n",
    "        return \"\"\n",
    "    pw = pagina_web.strip()\n",
    "    pw = re.sub(r\"^https?://\", \"\", pw, flags=re.I)\n",
    "    pw = pw.strip(\"/\")\n",
    "    return pw\n",
    "\n",
    "def extraer_review_uid(card):\n",
    "    \"\"\"\n",
    "    Intenta extraer el id de la reseÃ±a desde el href del tÃ­tulo: '/reviews/<id>'\n",
    "    Devuelve el segmento <id> o '' si no existe.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        a = card.find_element(By.CSS_SELECTOR, 'a[data-review-title-typography=\"true\"]')\n",
    "        href = a.get_attribute(\"href\")\n",
    "        if href:\n",
    "            m = re.search(r\"/reviews/([^/?#]+)\", href)\n",
    "            if m:\n",
    "                return m.group(1)\n",
    "    except:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "def aceptar_cookies_si_aparecen(driver, wait):\n",
    "    \"\"\"\n",
    "    Cierra la barra de cookies de Trustpilot si aparece (botÃ³n 'Entendido' o similar).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # botÃ³n con texto 'Entendido'\n",
    "        btn = WebDriverWait(driver, 3).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[.//span[normalize-space(text())='Entendido'] or normalize-space(text())='Entendido']\"))\n",
    "        )\n",
    "        btn.click()\n",
    "        time.sleep(0.5)\n",
    "        return True\n",
    "    except:\n",
    "        # otros textos tÃ­picos\n",
    "        try:\n",
    "            btn2 = WebDriverWait(driver, 3).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[normalize-space(text())='Aceptar']\"))\n",
    "            )\n",
    "            btn2.click()\n",
    "            time.sleep(0.5)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "# ------------- CARGA EMPRESAS -------------\n",
    "with open(EMPRESAS_JSON, 'r', encoding='utf-8') as f:\n",
    "    empresas_data = json.load(f)\n",
    "\n",
    "# ------------- CARGA / CREA DATASET -------------\n",
    "if os.path.exists(DATASET_CSV):\n",
    "    existing_df = pd.read_csv(DATASET_CSV)\n",
    "else:\n",
    "    existing_df = pd.DataFrame(columns=[\"id_reseÃ±a\", \"Fecha\", \"TÃ­tulo\", \"Contenido\", \"Empresa\", \"CalificaciÃ³n\"])\n",
    "\n",
    "existing_df['Fecha'] = pd.to_datetime(existing_df['Fecha'], errors='coerce')\n",
    "\n",
    "# ------------- SELENIUM -------------\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_argument(\"--window-size=1366,2400\")\n",
    "options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    \"(KHTML, like Gecko) Chrome/118.0.5993.70 Safari/537.36\"\n",
    ")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "wait = WebDriverWait(driver, 25)\n",
    "\n",
    "resultados = []\n",
    "reviews_by_company = {}\n",
    "\n",
    "try:\n",
    "    for empresa in empresas_data:\n",
    "        nombre_empresa = empresa.get(\"nombre\", \"N/A\")\n",
    "        pagina_web = normalizar_dominio(empresa.get(\"pagina_web\", \"\"))\n",
    "        if not pagina_web:\n",
    "            print(f\"âš ï¸ Empresa sin 'pagina_web': {nombre_empresa}. Omitida.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nğŸ” Verificando nuevas reseÃ±as de {nombre_empresa} ({pagina_web})...\")\n",
    "\n",
    "        # HistÃ³rico de esa empresa\n",
    "        old_df = existing_df[existing_df[\"Empresa\"] == nombre_empresa].copy()\n",
    "        if not old_df.empty:\n",
    "            old_df[\"TÃ­tulo\"] = old_df[\"TÃ­tulo\"].fillna(\"\").str.strip()\n",
    "            old_df[\"Fecha\"] = pd.to_datetime(old_df[\"Fecha\"], errors=\"coerce\")\n",
    "\n",
    "        reseÃ±as_nuevas = []\n",
    "\n",
    "        for page in range(1, MAX_PAGES + 1):\n",
    "            review_url = f\"https://es.trustpilot.com/review/{pagina_web}\" + (f\"?page={page}\" if page > 1 else \"\")\n",
    "            print(f\"ğŸŒ Accediendo a: {review_url}\")\n",
    "            driver.get(review_url)\n",
    "\n",
    "            # Aceptar cookies si estorban\n",
    "            aceptar_cookies_si_aparecen(driver, wait)\n",
    "\n",
    "            # Espera a que al menos haya 1 card de reseÃ±a en la pÃ¡gina\n",
    "            try:\n",
    "                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-testid=\"service-review-card-v2\"]')))\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ No se encontraron cards en pÃ¡gina {page}: {e}\")\n",
    "                # guarda para debug y rompe\n",
    "                if DEBUG:\n",
    "                    ss = f\"_debug_reviews_{nombre_empresa}_p{page}_nocards.png\"\n",
    "                    driver.save_screenshot(ss)\n",
    "                    with open(f\"_debug_reviews_{nombre_empresa}_p{page}_nocards.html\", \"w\", encoding=\"utf-8\") as fh:\n",
    "                        fh.write(driver.page_source)\n",
    "                    print(f\"ğŸ’¾ GuardÃ© evidencia sin cards: {ss}\")\n",
    "                break\n",
    "\n",
    "            # Forzar render con scrolls suaves (contenido lazy)\n",
    "            for _ in range(3):\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(1.2)\n",
    "\n",
    "            # Cards reales (BÃšSQUEDA GLOBAL, no dentro de un contenedor especÃ­fico)\n",
    "            cards = driver.find_elements(By.CSS_SELECTOR, 'div[data-testid=\"service-review-card-v2\"]')\n",
    "\n",
    "            if DEBUG:\n",
    "                print(f\"ğŸ” Encontradas {len(cards)} cards en pÃ¡gina {page}\")\n",
    "                try:\n",
    "                    ss_name = f\"_debug_reviews_{nombre_empresa}_p{page}.png\"\n",
    "                    html_name = f\"_debug_reviews_{nombre_empresa}_p{page}.html\"\n",
    "                    driver.save_screenshot(ss_name)\n",
    "                    with open(html_name, \"w\", encoding=\"utf-8\") as fh:\n",
    "                        fh.write(driver.page_source)\n",
    "                    print(f\"ğŸ’¾ GuardÃ© screenshot y HTML: {ss_name} / {html_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ No pude guardar debug files: {e}\")\n",
    "\n",
    "            nuevas_esta_pagina = []\n",
    "\n",
    "            for i, card in enumerate(cards, start=1):\n",
    "                try:\n",
    "                    # Fecha (ISO en <time datetime=\"...\">)\n",
    "                    try:\n",
    "                        time_el = card.find_element(By.CSS_SELECTOR, 'time[datetime]')\n",
    "                        fecha = pd.to_datetime(time_el.get_attribute(\"datetime\"), errors='coerce')\n",
    "                    except:\n",
    "                        fecha = pd.NaT\n",
    "\n",
    "                    # TÃ­tulo\n",
    "                    try:\n",
    "                        h2 = card.find_element(By.CSS_SELECTOR, 'h2[data-service-review-title-typography=\"true\"]')\n",
    "                        titulo = norm_text(h2.get_attribute(\"innerText\") or h2.text)\n",
    "                    except:\n",
    "                        titulo = \"N/A\"\n",
    "\n",
    "                    # Contenido\n",
    "                    try:\n",
    "                        p = card.find_element(By.CSS_SELECTOR, 'p[data-service-review-text-typography=\"true\"]')\n",
    "                        contenido = norm_text(p.get_attribute(\"innerText\") or p.text)\n",
    "                    except:\n",
    "                        contenido = \"Sin contenido\"\n",
    "\n",
    "                    # CalificaciÃ³n (solo nÃºmero)\n",
    "                    try:\n",
    "                        img = card.find_element(By.CSS_SELECTOR, 'img[alt*=\"Valorada con\"]')\n",
    "                        estrellas = parse_estrellas_from_alt(img.get_attribute(\"alt\"))\n",
    "                    except:\n",
    "                        estrellas = \"N/A\"\n",
    "\n",
    "                    # UID de la review (por si luego quieres deduplicar por id real)\n",
    "                    review_uid = extraer_review_uid(card)\n",
    "\n",
    "                    if DEBUG and i <= 3:\n",
    "                        print(f\"  â€¢ Card #{i}: fecha={fecha}, estrellas={estrellas}, tÃ­tulo='{titulo[:60]}', uid='{review_uid}'\")\n",
    "\n",
    "                    # DEDUP: por compatibilidad, TÃ­tulo + Fecha\n",
    "                    existe = (\n",
    "                        not old_df[\n",
    "                            (old_df[\"TÃ­tulo\"].str.strip() == titulo) &\n",
    "                            (old_df[\"Fecha\"] == fecha)\n",
    "                        ].empty\n",
    "                    )\n",
    "\n",
    "                    if not existe:\n",
    "                        nuevas_esta_pagina.append({\n",
    "                            \"Empresa\": nombre_empresa,\n",
    "                            \"TÃ­tulo\": titulo,\n",
    "                            \"Contenido\": contenido,\n",
    "                            \"Fecha\": fecha,\n",
    "                            \"CalificaciÃ³n\": estrellas\n",
    "                        })\n",
    "\n",
    "                except Exception as e_card:\n",
    "                    if DEBUG:\n",
    "                        print(f\"  â€¢ Aviso: card con error: {e_card}\")\n",
    "                    continue\n",
    "\n",
    "            if nuevas_esta_pagina:\n",
    "                reseÃ±as_nuevas.extend(nuevas_esta_pagina)\n",
    "                print(f\"âœ… {len(nuevas_esta_pagina)} reseÃ±as nuevas en pÃ¡gina {page}. Continuando...\")\n",
    "            else:\n",
    "                print(f\"ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina {page}. Deteniendo scraping para esta empresa.\")\n",
    "                break\n",
    "\n",
    "        # Consolidar por empresa\n",
    "        if reseÃ±as_nuevas:\n",
    "            df_nuevas = pd.DataFrame(reseÃ±as_nuevas)\n",
    "            df_nuevas['Fecha'] = pd.to_datetime(df_nuevas['Fecha'], errors='coerce')\n",
    "            combined_df = pd.concat([old_df, df_nuevas], ignore_index=True)\n",
    "            combined_df.drop_duplicates(subset=[\"TÃ­tulo\", \"Fecha\"], keep='first', inplace=True)\n",
    "            combined_df.sort_values(by=\"Fecha\", ascending=False, inplace=True)\n",
    "            combined_df.reset_index(drop=True, inplace=True)\n",
    "            combined_df.fillna(\"\", inplace=True)\n",
    "\n",
    "            # id_reseÃ±a por empresa (mantenemos tu formato)\n",
    "            total = len(combined_df)\n",
    "            combined_df[\"id_reseÃ±a\"] = [f\"{nombre_empresa.replace(' ', '')}_N{total - idx}\" for idx in range(total)]\n",
    "\n",
    "            reviews_by_company[nombre_empresa] = combined_df.to_dict(orient=\"records\")\n",
    "            resultados.append(combined_df)\n",
    "        else:\n",
    "            if not old_df.empty:\n",
    "                reviews_by_company[nombre_empresa] = old_df.to_dict(orient=\"records\")\n",
    "            print(f\"ğŸ“­ No se detectaron nuevas reseÃ±as para {nombre_empresa}.\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# ------------- SALIDA CSV -------------\n",
    "if resultados:\n",
    "    final_df = pd.concat([existing_df] + resultados, ignore_index=True)\n",
    "    final_df.drop_duplicates(subset=[\"TÃ­tulo\", \"Fecha\", \"Empresa\"], keep='first', inplace=True)\n",
    "    final_df.fillna(\"\", inplace=True)\n",
    "\n",
    "    # Recalcular ids por empresa en orden de fecha desc\n",
    "    final_df[\"Fecha\"] = pd.to_datetime(final_df[\"Fecha\"], errors=\"coerce\")\n",
    "    final_df.sort_values(by=[\"Empresa\", \"Fecha\"], ascending=[True, False], inplace=True)\n",
    "\n",
    "    final_df[\"id_reseÃ±a\"] = final_df.groupby(\"Empresa\").cumcount(ascending=False) + 1\n",
    "    final_df[\"id_reseÃ±a\"] = final_df.apply(\n",
    "        lambda row: f\"{row['Empresa'].replace(' ', '')}_N{row['id_reseÃ±a']}\", axis=1\n",
    "    )\n",
    "\n",
    "    final_df = final_df[[\"id_reseÃ±a\", \"Fecha\", \"TÃ­tulo\", \"Contenido\", \"Empresa\", \"CalificaciÃ³n\"]]\n",
    "\n",
    "    # Ordenar por nÃºmero de id descendente dentro de cada empresa\n",
    "    final_df[\"id_num\"] = final_df[\"id_reseÃ±a\"].str.extract(r'_N(\\d+)').astype(int)\n",
    "    final_df.sort_values(by=[\"Empresa\", \"id_num\"], ascending=[True, False], inplace=True)\n",
    "    final_df.drop(columns=[\"id_num\"], inplace=True)\n",
    "\n",
    "    os.makedirs(os.path.dirname(DATASET_CSV), exist_ok=True)\n",
    "    final_df.to_csv(DATASET_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\nğŸ‰ Dataset actualizado guardado en '{DATASET_CSV}'\")\n",
    "else:\n",
    "    print(\"\\nâœ… No hubo actualizaciones, el dataset se mantiene igual.\")\n",
    "\n",
    "# ------------- SALIDA JSON (anidado por empresa) -------------\n",
    "for empresa in empresas_data:\n",
    "    nombre_empresa = empresa.get(\"nombre\", \"N/A\")\n",
    "    empresa[\"reseÃ±as\"] = reviews_by_company.get(nombre_empresa, [])\n",
    "\n",
    "os.makedirs(os.path.dirname(SALIDA_JSON), exist_ok=True)\n",
    "with open(SALIDA_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(empresas_data, f, ensure_ascii=False, indent=4, default=str)\n",
    "\n",
    "print(f\"ğŸ“¦ Archivo JSON guardado en '{SALIDA_JSON}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoria 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap - Paginado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Heymondo Seguros de Viaje (heymondo.es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/heymondo.es\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/heymondo.es?page=2\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 2. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/heymondo.es?page=3\n",
      "âœ… 12 reseÃ±as nuevas en pÃ¡gina 3. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/heymondo.es?page=4\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 4. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Intermundial Seguros de viaje (intermundial.es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/intermundial.es\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/intermundial.es?page=2\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 2. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/intermundial.es?page=3\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 3. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/intermundial.es?page=4\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 4. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/intermundial.es?page=5\n",
      "âœ… 2 reseÃ±as nuevas en pÃ¡gina 5. Continuando...\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de IATI Seguros (iatiseguros.com)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/iatiseguros.com\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/iatiseguros.com?page=2\n",
      "âœ… 2 reseÃ±as nuevas en pÃ¡gina 2. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/iatiseguros.com?page=3\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 3. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Abbeygate Seguros EspaÃ±a (www.abbeygateinsure.com)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.abbeygateinsure.com\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 1. Deteniendo scraping para esta empresa.\n",
      "ğŸ“­ No se detectaron nuevas reseÃ±as para Abbeygate Seguros EspaÃ±a.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de MAPFRE EspaÃ±a (mapfre.es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/mapfre.es\n",
      "âœ… 10 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/mapfre.es?page=2\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 2. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Fit 2 Trip (www.fit2trip.es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.fit2trip.es\n",
      "âœ… 2 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.fit2trip.es?page=2\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 2. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Pelayo Mutua de Seguros (pelayo.com)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/pelayo.com\n",
      "âœ… 3 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/pelayo.com?page=2\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 2. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Zurich (zurich.es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/zurich.es\n",
      "âœ… 5 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/zurich.es?page=2\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 2. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Chapka Seguros (www.chapkadirect.es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.chapkadirect.es\n",
      "âœ… 8 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.chapkadirect.es?page=2\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 2. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Divina Seguros (www.divinaseguros.com)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.divinaseguros.com\n",
      "âœ… 5 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.divinaseguros.com?page=2\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 2. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ‰ Dataset actualizado guardado en 'datalake/1_LANDING_ZONE/dataset_categoria2.csv'\n",
      "ğŸ“¦ Archivo JSON guardado en 'datalake/1_LANDING_ZONE/reviews_trustpilot_empresas_categoria2.json'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ------------- CONFIG -------------\n",
    "DEBUG = False                 # True = imprime y guarda evidencia (screenshot + html)\n",
    "MAX_PAGES = 5                 # mÃ¡ximo de pÃ¡ginas por empresa\n",
    "EMPRESAS_JSON = 'datalake/1_LANDING_ZONE/trustpilot_empresas_categoria2.json'\n",
    "DATASET_CSV   = 'datalake/1_LANDING_ZONE/dataset_categoria2.csv'\n",
    "SALIDA_JSON   = 'datalake/1_LANDING_ZONE/reviews_trustpilot_empresas_categoria2.json'\n",
    "\n",
    "# ------------- HELPERS -------------\n",
    "def norm_text(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "def parse_estrellas_from_alt(alt_txt: str):\n",
    "    \"\"\"\n",
    "    'Valorada con 4 estrellas sobre 5' -> '4'\n",
    "    \"\"\"\n",
    "    if not alt_txt:\n",
    "        return \"N/A\"\n",
    "    m = re.search(r\"(\\d)\\s*estrellas\", alt_txt, flags=re.I)\n",
    "    return m.group(1) if m else \"N/A\"\n",
    "\n",
    "def normalizar_dominio(pagina_web: str) -> str:\n",
    "    \"\"\"\n",
    "    Espera 'www.midominio.es' o 'midominio.es'.\n",
    "    Quita http(s):// si viene, y slashes al final.\n",
    "    \"\"\"\n",
    "    if not pagina_web:\n",
    "        return \"\"\n",
    "    pw = pagina_web.strip()\n",
    "    pw = re.sub(r\"^https?://\", \"\", pw, flags=re.I)\n",
    "    pw = pw.strip(\"/\")\n",
    "    return pw\n",
    "\n",
    "def extraer_review_uid(card):\n",
    "    \"\"\"\n",
    "    Intenta extraer el id de la reseÃ±a desde el href del tÃ­tulo: '/reviews/<id>'\n",
    "    Devuelve el segmento <id> o '' si no existe.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        a = card.find_element(By.CSS_SELECTOR, 'a[data-review-title-typography=\"true\"]')\n",
    "        href = a.get_attribute(\"href\")\n",
    "        if href:\n",
    "            m = re.search(r\"/reviews/([^/?#]+)\", href)\n",
    "            if m:\n",
    "                return m.group(1)\n",
    "    except:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "def aceptar_cookies_si_aparecen(driver, wait):\n",
    "    \"\"\"\n",
    "    Cierra la barra de cookies de Trustpilot si aparece (botÃ³n 'Entendido' o similar).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        btn = WebDriverWait(driver, 3).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[.//span[normalize-space(text())='Entendido'] or normalize-space(text())='Entendido']\"))\n",
    "        )\n",
    "        btn.click()\n",
    "        time.sleep(0.5)\n",
    "        return True\n",
    "    except:\n",
    "        try:\n",
    "            btn2 = WebDriverWait(driver, 3).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[normalize-space(text())='Aceptar']\"))\n",
    "            )\n",
    "            btn2.click()\n",
    "            time.sleep(0.5)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "# ------------- CARGA EMPRESAS -------------\n",
    "with open(EMPRESAS_JSON, 'r', encoding='utf-8') as f:\n",
    "    empresas_data = json.load(f)\n",
    "\n",
    "# ------------- CARGA / CREA DATASET -------------\n",
    "if os.path.exists(DATASET_CSV):\n",
    "    existing_df = pd.read_csv(DATASET_CSV)\n",
    "else:\n",
    "    existing_df = pd.DataFrame(columns=[\"id_reseÃ±a\", \"Fecha\", \"TÃ­tulo\", \"Contenido\", \"Empresa\", \"CalificaciÃ³n\"])\n",
    "\n",
    "existing_df['Fecha'] = pd.to_datetime(existing_df['Fecha'], errors='coerce')\n",
    "\n",
    "# ------------- SELENIUM -------------\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_argument(\"--window-size=1366,2400\")\n",
    "options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    \"(KHTML, like Gecko) Chrome/118.0.5993.70 Safari/537.36\"\n",
    ")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "wait = WebDriverWait(driver, 25)\n",
    "\n",
    "resultados = []\n",
    "reviews_by_company = {}\n",
    "\n",
    "try:\n",
    "    for empresa in empresas_data:\n",
    "        nombre_empresa = empresa.get(\"nombre\", \"N/A\")\n",
    "        pagina_web = normalizar_dominio(empresa.get(\"pagina_web\", \"\"))\n",
    "        if not pagina_web:\n",
    "            print(f\"âš ï¸ Empresa sin 'pagina_web': {nombre_empresa}. Omitida.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nğŸ” Verificando nuevas reseÃ±as de {nombre_empresa} ({pagina_web})...\")\n",
    "\n",
    "        # HistÃ³rico de esa empresa\n",
    "        old_df = existing_df[existing_df[\"Empresa\"] == nombre_empresa].copy()\n",
    "        if not old_df.empty:\n",
    "            old_df[\"TÃ­tulo\"] = old_df[\"TÃ­tulo\"].fillna(\"\").str.strip()\n",
    "            old_df[\"Fecha\"] = pd.to_datetime(old_df[\"Fecha\"], errors=\"coerce\")\n",
    "\n",
    "        reseÃ±as_nuevas = []\n",
    "\n",
    "        for page in range(1, MAX_PAGES + 1):\n",
    "            review_url = f\"https://es.trustpilot.com/review/{pagina_web}\" + (f\"?page={page}\" if page > 1 else \"\")\n",
    "            print(f\"ğŸŒ Accediendo a: {review_url}\")\n",
    "            driver.get(review_url)\n",
    "\n",
    "            # Aceptar cookies si estorban\n",
    "            aceptar_cookies_si_aparecen(driver, wait)\n",
    "\n",
    "            # Espera a que al menos haya 1 card de reseÃ±a en la pÃ¡gina\n",
    "            try:\n",
    "                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-testid=\"service-review-card-v2\"]')))\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ No se encontraron cards en pÃ¡gina {page}: {e}\")\n",
    "                if DEBUG:\n",
    "                    ss = f\"_debug_reviews_{nombre_empresa}_p{page}_nocards.png\"\n",
    "                    driver.save_screenshot(ss)\n",
    "                    with open(f\"_debug_reviews_{nombre_empresa}_p{page}_nocards.html\", \"w\", encoding=\"utf-8\") as fh:\n",
    "                        fh.write(driver.page_source)\n",
    "                    print(f\"ğŸ’¾ GuardÃ© evidencia sin cards: {ss}\")\n",
    "                break\n",
    "\n",
    "            # Forzar render con scrolls suaves (contenido lazy)\n",
    "            for _ in range(3):\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(1.2)\n",
    "\n",
    "            # Cards reales (bÃºsqueda global)\n",
    "            cards = driver.find_elements(By.CSS_SELECTOR, 'div[data-testid=\"service-review-card-v2\"]')\n",
    "\n",
    "            if DEBUG:\n",
    "                print(f\"ğŸ” Encontradas {len(cards)} cards en pÃ¡gina {page}\")\n",
    "                try:\n",
    "                    ss_name = f\"_debug_reviews_{nombre_empresa}_p{page}.png\"\n",
    "                    html_name = f\"_debug_reviews_{nombre_empresa}_p{page}.html\"\n",
    "                    driver.save_screenshot(ss_name)\n",
    "                    with open(html_name, \"w\", encoding=\"utf-8\") as fh:\n",
    "                        fh.write(driver.page_source)\n",
    "                    print(f\"ğŸ’¾ GuardÃ© screenshot y HTML: {ss_name} / {html_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ No pude guardar debug files: {e}\")\n",
    "\n",
    "            nuevas_esta_pagina = []\n",
    "\n",
    "            for i, card in enumerate(cards, start=1):\n",
    "                try:\n",
    "                    # Fecha\n",
    "                    try:\n",
    "                        time_el = card.find_element(By.CSS_SELECTOR, 'time[datetime]')\n",
    "                        fecha = pd.to_datetime(time_el.get_attribute(\"datetime\"), errors='coerce')\n",
    "                    except:\n",
    "                        fecha = pd.NaT\n",
    "\n",
    "                    # TÃ­tulo\n",
    "                    try:\n",
    "                        h2 = card.find_element(By.CSS_SELECTOR, 'h2[data-service-review-title-typography=\"true\"]')\n",
    "                        titulo = norm_text(h2.get_attribute(\"innerText\") or h2.text)\n",
    "                    except:\n",
    "                        titulo = \"N/A\"\n",
    "\n",
    "                    # Contenido\n",
    "                    try:\n",
    "                        p = card.find_element(By.CSS_SELECTOR, 'p[data-service-review-text-typography=\"true\"]')\n",
    "                        contenido = norm_text(p.get_attribute(\"innerText\") or p.text)\n",
    "                    except:\n",
    "                        contenido = \"Sin contenido\"\n",
    "\n",
    "                    # CalificaciÃ³n (solo nÃºmero)\n",
    "                    try:\n",
    "                        img = card.find_element(By.CSS_SELECTOR, 'img[alt*=\"Valorada con\"]')\n",
    "                        estrellas = parse_estrellas_from_alt(img.get_attribute(\"alt\"))\n",
    "                    except:\n",
    "                        estrellas = \"N/A\"\n",
    "\n",
    "                    # UID de la review (opcional)\n",
    "                    review_uid = extraer_review_uid(card)\n",
    "\n",
    "                    if DEBUG and i <= 3:\n",
    "                        print(f\"  â€¢ Card #{i}: fecha={fecha}, estrellas={estrellas}, tÃ­tulo='{titulo[:60]}', uid='{review_uid}'\")\n",
    "\n",
    "                    # DEDUP: TÃ­tulo + Fecha\n",
    "                    existe = (\n",
    "                        not old_df[\n",
    "                            (old_df[\"TÃ­tulo\"].str.strip() == titulo) &\n",
    "                            (old_df[\"Fecha\"] == fecha)\n",
    "                        ].empty\n",
    "                    )\n",
    "\n",
    "                    if not existe:\n",
    "                        nuevas_esta_pagina.append({\n",
    "                            \"Empresa\": nombre_empresa,\n",
    "                            \"TÃ­tulo\": titulo,\n",
    "                            \"Contenido\": contenido,\n",
    "                            \"Fecha\": fecha,\n",
    "                            \"CalificaciÃ³n\": estrellas\n",
    "                        })\n",
    "\n",
    "                except Exception as e_card:\n",
    "                    if DEBUG:\n",
    "                        print(f\"  â€¢ Aviso: card con error: {e_card}\")\n",
    "                    continue\n",
    "\n",
    "            if nuevas_esta_pagina:\n",
    "                reseÃ±as_nuevas.extend(nuevas_esta_pagina)\n",
    "                print(f\"âœ… {len(nuevas_esta_pagina)} reseÃ±as nuevas en pÃ¡gina {page}. Continuando...\")\n",
    "            else:\n",
    "                print(f\"ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina {page}. Deteniendo scraping para esta empresa.\")\n",
    "                break\n",
    "\n",
    "        # Consolidar por empresa\n",
    "        if reseÃ±as_nuevas:\n",
    "            df_nuevas = pd.DataFrame(reseÃ±as_nuevas)\n",
    "            df_nuevas['Fecha'] = pd.to_datetime(df_nuevas['Fecha'], errors='coerce')\n",
    "            combined_df = pd.concat([old_df, df_nuevas], ignore_index=True)\n",
    "            combined_df.drop_duplicates(subset=[\"TÃ­tulo\", \"Fecha\"], keep='first', inplace=True)\n",
    "            combined_df.sort_values(by=\"Fecha\", ascending=False, inplace=True)\n",
    "            combined_df.reset_index(drop=True, inplace=True)\n",
    "            combined_df.fillna(\"\", inplace=True)\n",
    "\n",
    "            total = len(combined_df)\n",
    "            combined_df[\"id_reseÃ±a\"] = [f\"{nombre_empresa.replace(' ', '')}_N{total - idx}\" for idx in range(total)]\n",
    "\n",
    "            reviews_by_company[nombre_empresa] = combined_df.to_dict(orient=\"records\")\n",
    "            resultados.append(combined_df)\n",
    "        else:\n",
    "            if not old_df.empty:\n",
    "                reviews_by_company[nombre_empresa] = old_df.to_dict(orient=\"records\")\n",
    "            print(f\"ğŸ“­ No se detectaron nuevas reseÃ±as para {nombre_empresa}.\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# ------------- SALIDA CSV -------------\n",
    "if resultados:\n",
    "    final_df = pd.concat([existing_df] + resultados, ignore_index=True)\n",
    "    final_df.drop_duplicates(subset=[\"TÃ­tulo\", \"Fecha\", \"Empresa\"], keep='first', inplace=True)\n",
    "    final_df.fillna(\"\", inplace=True)\n",
    "\n",
    "    final_df[\"Fecha\"] = pd.to_datetime(final_df[\"Fecha\"], errors=\"coerce\")\n",
    "    final_df.sort_values(by=[\"Empresa\", \"Fecha\"], ascending=[True, False], inplace=True)\n",
    "\n",
    "    final_df[\"id_reseÃ±a\"] = final_df.groupby(\"Empresa\").cumcount(ascending=False) + 1\n",
    "    final_df[\"id_reseÃ±a\"] = final_df.apply(\n",
    "        lambda row: f\"{row['Empresa'].replace(' ', '')}_N{row['id_reseÃ±a']}\", axis=1\n",
    "    )\n",
    "\n",
    "    final_df = final_df[[\"id_reseÃ±a\", \"Fecha\", \"TÃ­tulo\", \"Contenido\", \"Empresa\", \"CalificaciÃ³n\"]]\n",
    "\n",
    "    final_df[\"id_num\"] = final_df[\"id_reseÃ±a\"].str.extract(r'_N(\\d+)').astype(int)\n",
    "    final_df.sort_values(by=[\"Empresa\", \"id_num\"], ascending=[True, False], inplace=True)\n",
    "    final_df.drop(columns=[\"id_num\"], inplace=True)\n",
    "\n",
    "    os.makedirs(os.path.dirname(DATASET_CSV), exist_ok=True)\n",
    "    final_df.to_csv(DATASET_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\nğŸ‰ Dataset actualizado guardado en '{DATASET_CSV}'\")\n",
    "else:\n",
    "    print(\"\\nâœ… No hubo actualizaciones, el dataset se mantiene igual.\")\n",
    "\n",
    "# ------------- SALIDA JSON (anidado por empresa) -------------\n",
    "for empresa in empresas_data:\n",
    "    nombre_empresa = empresa.get(\"nombre\", \"N/A\")\n",
    "    empresa[\"reseÃ±as\"] = reviews_by_company.get(nombre_empresa, [])\n",
    "\n",
    "os.makedirs(os.path.dirname(SALIDA_JSON), exist_ok=True)\n",
    "with open(SALIDA_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(empresas_data, f, ensure_ascii=False, indent=4, default=str)\n",
    "\n",
    "print(f\"ğŸ“¦ Archivo JSON guardado en '{SALIDA_JSON}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoria 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de compramostucoche.es (www.compramostucoche.es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.compramostucoche.es\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.compramostucoche.es?page=2\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 2. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.compramostucoche.es?page=3\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 3. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.compramostucoche.es?page=4\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 4. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.compramostucoche.es?page=5\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 5. Continuando...\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de OcasionPlus (ocasionplus.com)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/ocasionplus.com\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/ocasionplus.com?page=2\n",
      "âœ… 19 reseÃ±as nuevas en pÃ¡gina 2. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/ocasionplus.com?page=3\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 3. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Autohero EspaÃ±a (autohero.com/es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/autohero.com/es\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/autohero.com/es?page=2\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 2. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/autohero.com/es?page=3\n",
      "âœ… 17 reseÃ±as nuevas en pÃ¡gina 3. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/autohero.com/es?page=4\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 4. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Carwow ES (www.carwow.es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.carwow.es\n",
      "âœ… 8 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.carwow.es?page=2\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 2. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Carplus EspaÃ±a (carplus.es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/carplus.es\n",
      "âœ… 12 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/carplus.es?page=2\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 2. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Hr Motor (www.hrmotor.com)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.hrmotor.com\n",
      "âœ… 5 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.hrmotor.com?page=2\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 2. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Clicars (www.clicars.com)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.clicars.com\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.clicars.com?page=2\n",
      "âœ… 7 reseÃ±as nuevas en pÃ¡gina 2. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.clicars.com?page=3\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 3. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de CarNext.com EspaÃ±a (carnext.com/es-es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/carnext.com/es-es\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 1. Deteniendo scraping para esta empresa.\n",
      "ğŸ“­ No se detectaron nuevas reseÃ±as para CarNext.com EspaÃ±a.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Autobiz SA (autobiz-ocasion.es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/autobiz-ocasion.es\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 1. Deteniendo scraping para esta empresa.\n",
      "ğŸ“­ No se detectaron nuevas reseÃ±as para Autobiz SA.\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Garage Club (garageclub.es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/garageclub.es\n",
      "âœ… 9 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/garageclub.es?page=2\n",
      "ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina 2. Deteniendo scraping para esta empresa.\n",
      "\n",
      "ğŸ‰ Dataset actualizado guardado en 'datalake/1_LANDING_ZONE/dataset_categoria3.csv'\n",
      "ğŸ“¦ Archivo JSON guardado en 'datalake/1_LANDING_ZONE/reviews_trustpilot_empresas_categoria3.json'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ------------- CONFIG -------------\n",
    "DEBUG = False                 # True = imprime y guarda evidencia (screenshot + html)\n",
    "MAX_PAGES = 5                 # mÃ¡ximo de pÃ¡ginas por empresa\n",
    "EMPRESAS_JSON = 'datalake/1_LANDING_ZONE/trustpilot_empresas_categoria3.json'\n",
    "DATASET_CSV   = 'datalake/1_LANDING_ZONE/dataset_categoria3.csv'\n",
    "SALIDA_JSON   = 'datalake/1_LANDING_ZONE/reviews_trustpilot_empresas_categoria3.json'\n",
    "\n",
    "# ------------- HELPERS -------------\n",
    "def norm_text(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "def parse_estrellas_from_alt(alt_txt: str):\n",
    "    \"\"\"\n",
    "    'Valorada con 4 estrellas sobre 5' -> '4'\n",
    "    \"\"\"\n",
    "    if not alt_txt:\n",
    "        return \"N/A\"\n",
    "    m = re.search(r\"(\\d)\\s*estrellas\", alt_txt, flags=re.I)\n",
    "    return m.group(1) if m else \"N/A\"\n",
    "\n",
    "def normalizar_dominio(pagina_web: str) -> str:\n",
    "    \"\"\"\n",
    "    Espera 'www.midominio.es' o 'midominio.es'.\n",
    "    Quita http(s):// si viene, y slashes al final.\n",
    "    \"\"\"\n",
    "    if not pagina_web:\n",
    "        return \"\"\n",
    "    pw = pagina_web.strip()\n",
    "    pw = re.sub(r\"^https?://\", \"\", pw, flags=re.I)\n",
    "    pw = pw.strip(\"/\")\n",
    "    return pw\n",
    "\n",
    "def extraer_review_uid(card):\n",
    "    \"\"\"\n",
    "    Intenta extraer el id de la reseÃ±a desde el href del tÃ­tulo: '/reviews/<id>'\n",
    "    Devuelve el segmento <id> o '' si no existe.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        a = card.find_element(By.CSS_SELECTOR, 'a[data-review-title-typography=\"true\"]')\n",
    "        href = a.get_attribute(\"href\")\n",
    "        if href:\n",
    "            m = re.search(r\"/reviews/([^/?#]+)\", href)\n",
    "            if m:\n",
    "                return m.group(1)\n",
    "    except:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "def aceptar_cookies_si_aparecen(driver, wait):\n",
    "    \"\"\"\n",
    "    Cierra la barra de cookies de Trustpilot si aparece (botÃ³n 'Entendido' o similar).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        btn = WebDriverWait(driver, 3).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[.//span[normalize-space(text())='Entendido'] or normalize-space(text())='Entendido']\"))\n",
    "        )\n",
    "        btn.click()\n",
    "        time.sleep(0.5)\n",
    "        return True\n",
    "    except:\n",
    "        try:\n",
    "            btn2 = WebDriverWait(driver, 3).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[normalize-space(text())='Aceptar']\"))\n",
    "            )\n",
    "            btn2.click()\n",
    "            time.sleep(0.5)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "# ------------- CARGA EMPRESAS -------------\n",
    "with open(EMPRESAS_JSON, 'r', encoding='utf-8') as f:\n",
    "    empresas_data = json.load(f)\n",
    "\n",
    "# ------------- CARGA / CREA DATASET -------------\n",
    "if os.path.exists(DATASET_CSV):\n",
    "    existing_df = pd.read_csv(DATASET_CSV)\n",
    "else:\n",
    "    existing_df = pd.DataFrame(columns=[\"id_reseÃ±a\", \"Fecha\", \"TÃ­tulo\", \"Contenido\", \"Empresa\", \"CalificaciÃ³n\"])\n",
    "\n",
    "existing_df['Fecha'] = pd.to_datetime(existing_df['Fecha'], errors='coerce')\n",
    "\n",
    "# ------------- SELENIUM -------------\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_argument(\"--window-size=1366,2400\")\n",
    "options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    \"(KHTML, like Gecko) Chrome/118.0.5993.70 Safari/537.36\"\n",
    ")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "wait = WebDriverWait(driver, 25)\n",
    "\n",
    "resultados = []\n",
    "reviews_by_company = {}\n",
    "\n",
    "try:\n",
    "    for empresa in empresas_data:\n",
    "        nombre_empresa = empresa.get(\"nombre\", \"N/A\")\n",
    "        pagina_web = normalizar_dominio(empresa.get(\"pagina_web\", \"\"))\n",
    "        if not pagina_web:\n",
    "            print(f\"âš ï¸ Empresa sin 'pagina_web': {nombre_empresa}. Omitida.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nğŸ” Verificando nuevas reseÃ±as de {nombre_empresa} ({pagina_web})...\")\n",
    "\n",
    "        # HistÃ³rico de esa empresa\n",
    "        old_df = existing_df[existing_df[\"Empresa\"] == nombre_empresa].copy()\n",
    "        if not old_df.empty:\n",
    "            old_df[\"TÃ­tulo\"] = old_df[\"TÃ­tulo\"].fillna(\"\").str.strip()\n",
    "            old_df[\"Fecha\"] = pd.to_datetime(old_df[\"Fecha\"], errors=\"coerce\")\n",
    "\n",
    "        reseÃ±as_nuevas = []\n",
    "\n",
    "        for page in range(1, MAX_PAGES + 1):\n",
    "            review_url = f\"https://es.trustpilot.com/review/{pagina_web}\" + (f\"?page={page}\" if page > 1 else \"\")\n",
    "            print(f\"ğŸŒ Accediendo a: {review_url}\")\n",
    "            driver.get(review_url)\n",
    "\n",
    "            # Aceptar cookies si estorban\n",
    "            aceptar_cookies_si_aparecen(driver, wait)\n",
    "\n",
    "            # Espera a que al menos haya 1 card de reseÃ±a en la pÃ¡gina\n",
    "            try:\n",
    "                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-testid=\"service-review-card-v2\"]')))\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ No se encontraron cards en pÃ¡gina {page}: {e}\")\n",
    "                if DEBUG:\n",
    "                    ss = f\"_debug_reviews_{nombre_empresa}_p{page}_nocards.png\"\n",
    "                    driver.save_screenshot(ss)\n",
    "                    with open(f\"_debug_reviews_{nombre_empresa}_p{page}_nocards.html\", \"w\", encoding=\"utf-8\") as fh:\n",
    "                        fh.write(driver.page_source)\n",
    "                    print(f\"ğŸ’¾ GuardÃ© evidencia sin cards: {ss}\")\n",
    "                break\n",
    "\n",
    "            # Forzar render con scrolls suaves (contenido lazy)\n",
    "            for _ in range(3):\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(1.2)\n",
    "\n",
    "            # Cards reales (bÃºsqueda global)\n",
    "            cards = driver.find_elements(By.CSS_SELECTOR, 'div[data-testid=\"service-review-card-v2\"]')\n",
    "\n",
    "            if DEBUG:\n",
    "                print(f\"ğŸ” Encontradas {len(cards)} cards en pÃ¡gina {page}\")\n",
    "                try:\n",
    "                    ss_name = f\"_debug_reviews_{nombre_empresa}_p{page}.png\"\n",
    "                    html_name = f\"_debug_reviews_{nombre_empresa}_p{page}.html\"\n",
    "                    driver.save_screenshot(ss_name)\n",
    "                    with open(html_name, \"w\", encoding=\"utf-8\") as fh:\n",
    "                        fh.write(driver.page_source)\n",
    "                    print(f\"ğŸ’¾ GuardÃ© screenshot y HTML: {ss_name} / {html_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ No pude guardar debug files: {e}\")\n",
    "\n",
    "            nuevas_esta_pagina = []\n",
    "\n",
    "            for i, card in enumerate(cards, start=1):\n",
    "                try:\n",
    "                    # Fecha\n",
    "                    try:\n",
    "                        time_el = card.find_element(By.CSS_SELECTOR, 'time[datetime]')\n",
    "                        fecha = pd.to_datetime(time_el.get_attribute(\"datetime\"), errors='coerce')\n",
    "                    except:\n",
    "                        fecha = pd.NaT\n",
    "\n",
    "                    # TÃ­tulo\n",
    "                    try:\n",
    "                        h2 = card.find_element(By.CSS_SELECTOR, 'h2[data-service-review-title-typography=\"true\"]')\n",
    "                        titulo = norm_text(h2.get_attribute(\"innerText\") or h2.text)\n",
    "                    except:\n",
    "                        titulo = \"N/A\"\n",
    "\n",
    "                    # Contenido\n",
    "                    try:\n",
    "                        p = card.find_element(By.CSS_SELECTOR, 'p[data-service-review-text-typography=\"true\"]')\n",
    "                        contenido = norm_text(p.get_attribute(\"innerText\") or p.text)\n",
    "                    except:\n",
    "                        contenido = \"Sin contenido\"\n",
    "\n",
    "                    # CalificaciÃ³n (solo nÃºmero)\n",
    "                    try:\n",
    "                        img = card.find_element(By.CSS_SELECTOR, 'img[alt*=\"Valorada con\"]')\n",
    "                        estrellas = parse_estrellas_from_alt(img.get_attribute(\"alt\"))\n",
    "                    except:\n",
    "                        estrellas = \"N/A\"\n",
    "\n",
    "                    # UID de la review (opcional)\n",
    "                    review_uid = extraer_review_uid(card)\n",
    "\n",
    "                    if DEBUG and i <= 3:\n",
    "                        print(f\"  â€¢ Card #{i}: fecha={fecha}, estrellas={estrellas}, tÃ­tulo='{titulo[:60]}', uid='{review_uid}'\")\n",
    "\n",
    "                    # DEDUP: TÃ­tulo + Fecha\n",
    "                    existe = (\n",
    "                        not old_df[\n",
    "                            (old_df[\"TÃ­tulo\"].str.strip() == titulo) &\n",
    "                            (old_df[\"Fecha\"] == fecha)\n",
    "                        ].empty\n",
    "                    )\n",
    "\n",
    "                    if not existe:\n",
    "                        nuevas_esta_pagina.append({\n",
    "                            \"Empresa\": nombre_empresa,\n",
    "                            \"TÃ­tulo\": titulo,\n",
    "                            \"Contenido\": contenido,\n",
    "                            \"Fecha\": fecha,\n",
    "                            \"CalificaciÃ³n\": estrellas\n",
    "                        })\n",
    "\n",
    "                except Exception as e_card:\n",
    "                    if DEBUG:\n",
    "                        print(f\"  â€¢ Aviso: card con error: {e_card}\")\n",
    "                    continue\n",
    "\n",
    "            if nuevas_esta_pagina:\n",
    "                reseÃ±as_nuevas.extend(nuevas_esta_pagina)\n",
    "                print(f\"âœ… {len(nuevas_esta_pagina)} reseÃ±as nuevas en pÃ¡gina {page}. Continuando...\")\n",
    "            else:\n",
    "                print(f\"ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina {page}. Deteniendo scraping para esta empresa.\")\n",
    "                break\n",
    "\n",
    "        # Consolidar por empresa\n",
    "        if reseÃ±as_nuevas:\n",
    "            df_nuevas = pd.DataFrame(reseÃ±as_nuevas)\n",
    "            df_nuevas['Fecha'] = pd.to_datetime(df_nuevas['Fecha'], errors='coerce')\n",
    "            combined_df = pd.concat([old_df, df_nuevas], ignore_index=True)\n",
    "            combined_df.drop_duplicates(subset=[\"TÃ­tulo\", \"Fecha\"], keep='first', inplace=True)\n",
    "            combined_df.sort_values(by=\"Fecha\", ascending=False, inplace=True)\n",
    "            combined_df.reset_index(drop=True, inplace=True)\n",
    "            combined_df.fillna(\"\", inplace=True)\n",
    "\n",
    "            total = len(combined_df)\n",
    "            combined_df[\"id_reseÃ±a\"] = [f\"{nombre_empresa.replace(' ', '')}_N{total - idx}\" for idx in range(total)]\n",
    "\n",
    "            reviews_by_company[nombre_empresa] = combined_df.to_dict(orient=\"records\")\n",
    "            resultados.append(combined_df)\n",
    "        else:\n",
    "            if not old_df.empty:\n",
    "                reviews_by_company[nombre_empresa] = old_df.to_dict(orient=\"records\")\n",
    "            print(f\"ğŸ“­ No se detectaron nuevas reseÃ±as para {nombre_empresa}.\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# ------------- SALIDA CSV -------------\n",
    "if resultados:\n",
    "    final_df = pd.concat([existing_df] + resultados, ignore_index=True)\n",
    "    final_df.drop_duplicates(subset=[\"TÃ­tulo\", \"Fecha\", \"Empresa\"], keep='first', inplace=True)\n",
    "    final_df.fillna(\"\", inplace=True)\n",
    "\n",
    "    final_df[\"Fecha\"] = pd.to_datetime(final_df[\"Fecha\"], errors=\"coerce\")\n",
    "    final_df.sort_values(by=[\"Empresa\", \"Fecha\"], ascending=[True, False], inplace=True)\n",
    "\n",
    "    final_df[\"id_reseÃ±a\"] = final_df.groupby(\"Empresa\").cumcount(ascending=False) + 1\n",
    "    final_df[\"id_reseÃ±a\"] = final_df.apply(\n",
    "        lambda row: f\"{row['Empresa'].replace(' ', '')}_N{row['id_reseÃ±a']}\", axis=1\n",
    "    )\n",
    "\n",
    "    final_df = final_df[[\"id_reseÃ±a\", \"Fecha\", \"TÃ­tulo\", \"Contenido\", \"Empresa\", \"CalificaciÃ³n\"]]\n",
    "\n",
    "    final_df[\"id_num\"] = final_df[\"id_reseÃ±a\"].str.extract(r'_N(\\d+)').astype(int)\n",
    "    final_df.sort_values(by=[\"Empresa\", \"id_num\"], ascending=[True, False], inplace=True)\n",
    "    final_df.drop(columns=[\"id_num\"], inplace=True)\n",
    "\n",
    "    os.makedirs(os.path.dirname(DATASET_CSV), exist_ok=True)\n",
    "    final_df.to_csv(DATASET_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\nğŸ‰ Dataset actualizado guardado en '{DATASET_CSV}'\")\n",
    "else:\n",
    "    print(\"\\nâœ… No hubo actualizaciones, el dataset se mantiene igual.\")\n",
    "\n",
    "# ------------- SALIDA JSON (anidado por empresa) -------------\n",
    "for empresa in empresas_data:\n",
    "    nombre_empresa = empresa.get(\"nombre\", \"N/A\")\n",
    "    empresa[\"reseÃ±as\"] = reviews_by_company.get(nombre_empresa, [])\n",
    "\n",
    "os.makedirs(os.path.dirname(SALIDA_JSON), exist_ok=True)\n",
    "with open(SALIDA_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(empresas_data, f, ensure_ascii=False, indent=4, default=str)\n",
    "\n",
    "print(f\"ğŸ“¦ Archivo JSON guardado en '{SALIDA_JSON}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoria 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Bershka (www.bershka.com)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.bershka.com\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.bershka.com?page=2\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 2. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.bershka.com?page=3\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 3. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.bershka.com?page=4\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 4. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.bershka.com?page=5\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 5. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.bershka.com?page=6\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 6. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.bershka.com?page=7\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 7. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.bershka.com?page=8\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 8. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.bershka.com?page=9\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 9. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.bershka.com?page=10\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 10. Continuando...\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de latostadora (latostadora.com)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/latostadora.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DANIEL\\AppData\\Local\\Temp\\ipykernel_16840\\2139895892.py:247: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  combined_df.fillna(\"\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/latostadora.com?page=2\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 2. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/latostadora.com?page=3\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 3. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/latostadora.com?page=4\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 4. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/latostadora.com?page=5\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 5. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/latostadora.com?page=6\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 6. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/latostadora.com?page=7\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 7. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/latostadora.com?page=8\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 8. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/latostadora.com?page=9\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 9. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/latostadora.com?page=10\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 10. Continuando...\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Hawkers (hawkersco.com)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/hawkersco.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DANIEL\\AppData\\Local\\Temp\\ipykernel_16840\\2139895892.py:247: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  combined_df.fillna(\"\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/hawkersco.com?page=2\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 2. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/hawkersco.com?page=3\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 3. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/hawkersco.com?page=4\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 4. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/hawkersco.com?page=5\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 5. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/hawkersco.com?page=6\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 6. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/hawkersco.com?page=7\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 7. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/hawkersco.com?page=8\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 8. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/hawkersco.com?page=9\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 9. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/hawkersco.com?page=10\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 10. Continuando...\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Dressinn (dressinn.com)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/dressinn.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DANIEL\\AppData\\Local\\Temp\\ipykernel_16840\\2139895892.py:247: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  combined_df.fillna(\"\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/dressinn.com?page=2\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 2. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/dressinn.com?page=3\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 3. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/dressinn.com?page=4\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 4. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/dressinn.com?page=5\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 5. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/dressinn.com?page=6\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 6. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/dressinn.com?page=7\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 7. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/dressinn.com?page=8\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 8. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/dressinn.com?page=9\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 9. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/dressinn.com?page=10\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 10. Continuando...\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de ZARA (www.zara.com)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.zara.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DANIEL\\AppData\\Local\\Temp\\ipykernel_16840\\2139895892.py:247: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  combined_df.fillna(\"\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.zara.com?page=2\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 2. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.zara.com?page=3\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 3. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.zara.com?page=4\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 4. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.zara.com?page=5\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 5. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.zara.com?page=6\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 6. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.zara.com?page=7\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 7. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.zara.com?page=8\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 8. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.zara.com?page=9\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 9. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/www.zara.com?page=10\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 10. Continuando...\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de TWOTHIRDS (twothirds.com)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/twothirds.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DANIEL\\AppData\\Local\\Temp\\ipykernel_16840\\2139895892.py:247: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  combined_df.fillna(\"\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/twothirds.com?page=2\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 2. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/twothirds.com?page=3\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 3. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/twothirds.com?page=4\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 4. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/twothirds.com?page=5\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 5. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/twothirds.com?page=6\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 6. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/twothirds.com?page=7\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 7. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/twothirds.com?page=8\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 8. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/twothirds.com?page=9\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 9. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/twothirds.com?page=10\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 10. Continuando...\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Streetprorunning (streetprorunning.com)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/streetprorunning.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DANIEL\\AppData\\Local\\Temp\\ipykernel_16840\\2139895892.py:247: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  combined_df.fillna(\"\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/streetprorunning.com?page=2\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 2. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/streetprorunning.com?page=3\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 3. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/streetprorunning.com?page=4\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 4. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/streetprorunning.com?page=5\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 5. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/streetprorunning.com?page=6\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 6. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/streetprorunning.com?page=7\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 7. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/streetprorunning.com?page=8\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 8. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/streetprorunning.com?page=9\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 9. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/streetprorunning.com?page=10\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 10. Continuando...\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Scuffers (scuffers.es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/scuffers.es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DANIEL\\AppData\\Local\\Temp\\ipykernel_16840\\2139895892.py:247: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  combined_df.fillna(\"\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/scuffers.es?page=2\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 2. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/scuffers.es?page=3\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 3. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/scuffers.es?page=4\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 4. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/scuffers.es?page=5\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 5. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/scuffers.es?page=6\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 6. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/scuffers.es?page=7\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 7. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/scuffers.es?page=8\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 8. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/scuffers.es?page=9\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 9. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/scuffers.es?page=10\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 10. Continuando...\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Funidelia (funidelia.com)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/funidelia.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DANIEL\\AppData\\Local\\Temp\\ipykernel_16840\\2139895892.py:247: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  combined_df.fillna(\"\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/funidelia.com?page=2\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 2. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/funidelia.com?page=3\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 3. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/funidelia.com?page=4\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 4. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/funidelia.com?page=5\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 5. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/funidelia.com?page=6\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 6. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/funidelia.com?page=7\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 7. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/funidelia.com?page=8\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 8. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/funidelia.com?page=9\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 9. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/funidelia.com?page=10\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 10. Continuando...\n",
      "\n",
      "ğŸ” Verificando nuevas reseÃ±as de Vinted ES (vinted.es)...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/vinted.es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DANIEL\\AppData\\Local\\Temp\\ipykernel_16840\\2139895892.py:247: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  combined_df.fillna(\"\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 1. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/vinted.es?page=2\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 2. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/vinted.es?page=3\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 3. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/vinted.es?page=4\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 4. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/vinted.es?page=5\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 5. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/vinted.es?page=6\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 6. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/vinted.es?page=7\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 7. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/vinted.es?page=8\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 8. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/vinted.es?page=9\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 9. Continuando...\n",
      "ğŸŒ Accediendo a: https://es.trustpilot.com/review/vinted.es?page=10\n",
      "âœ… 20 reseÃ±as nuevas en pÃ¡gina 10. Continuando...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DANIEL\\AppData\\Local\\Temp\\ipykernel_16840\\2139895892.py:247: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  combined_df.fillna(\"\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‰ Dataset actualizado guardado en 'datalake/1_LANDING_ZONE/dataset_categoria4.csv'\n",
      "ğŸ“¦ Archivo JSON guardado en 'datalake/1_LANDING_ZONE/reviews_trustpilot_empresas_categoria4.json'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ------------- CONFIG -------------\n",
    "DEBUG = False                 # True = imprime y guarda evidencia (screenshot + html)\n",
    "MAX_PAGES = 10                 # mÃ¡ximo de pÃ¡ginas por empresa\n",
    "EMPRESAS_JSON = 'datalake/1_LANDING_ZONE/trustpilot_empresas_categoria4.json'\n",
    "DATASET_CSV   = 'datalake/1_LANDING_ZONE/dataset_categoria4.csv'\n",
    "SALIDA_JSON   = 'datalake/1_LANDING_ZONE/reviews_trustpilot_empresas_categoria4.json'\n",
    "\n",
    "# ------------- HELPERS -------------\n",
    "def norm_text(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "def parse_estrellas_from_alt(alt_txt: str):\n",
    "    \"\"\"\n",
    "    'Valorada con 4 estrellas sobre 5' -> '4'\n",
    "    \"\"\"\n",
    "    if not alt_txt:\n",
    "        return \"N/A\"\n",
    "    m = re.search(r\"(\\d)\\s*estrellas\", alt_txt, flags=re.I)\n",
    "    return m.group(1) if m else \"N/A\"\n",
    "\n",
    "def normalizar_dominio(pagina_web: str) -> str:\n",
    "    \"\"\"\n",
    "    Espera 'www.midominio.es' o 'midominio.es'.\n",
    "    Quita http(s):// si viene, y slashes al final.\n",
    "    \"\"\"\n",
    "    if not pagina_web:\n",
    "        return \"\"\n",
    "    pw = pagina_web.strip()\n",
    "    pw = re.sub(r\"^https?://\", \"\", pw, flags=re.I)\n",
    "    pw = pw.strip(\"/\")\n",
    "    return pw\n",
    "\n",
    "def extraer_review_uid(card):\n",
    "    \"\"\"\n",
    "    Intenta extraer el id de la reseÃ±a desde el href del tÃ­tulo: '/reviews/<id>'\n",
    "    Devuelve el segmento <id> o '' si no existe.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        a = card.find_element(By.CSS_SELECTOR, 'a[data-review-title-typography=\"true\"]')\n",
    "        href = a.get_attribute(\"href\")\n",
    "        if href:\n",
    "            m = re.search(r\"/reviews/([^/?#]+)\", href)\n",
    "            if m:\n",
    "                return m.group(1)\n",
    "    except:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "def aceptar_cookies_si_aparecen(driver, wait):\n",
    "    \"\"\"\n",
    "    Cierra la barra de cookies de Trustpilot si aparece (botÃ³n 'Entendido' o similar).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        btn = WebDriverWait(driver, 3).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[.//span[normalize-space(text())='Entendido'] or normalize-space(text())='Entendido']\"))\n",
    "        )\n",
    "        btn.click()\n",
    "        time.sleep(0.5)\n",
    "        return True\n",
    "    except:\n",
    "        try:\n",
    "            btn2 = WebDriverWait(driver, 3).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[normalize-space(text())='Aceptar']\"))\n",
    "            )\n",
    "            btn2.click()\n",
    "            time.sleep(0.5)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "# ------------- CARGA EMPRESAS -------------\n",
    "with open(EMPRESAS_JSON, 'r', encoding='utf-8') as f:\n",
    "    empresas_data = json.load(f)\n",
    "\n",
    "# ------------- CARGA / CREA DATASET -------------\n",
    "if os.path.exists(DATASET_CSV):\n",
    "    existing_df = pd.read_csv(DATASET_CSV)\n",
    "else:\n",
    "    existing_df = pd.DataFrame(columns=[\"id_reseÃ±a\", \"Fecha\", \"TÃ­tulo\", \"Contenido\", \"Empresa\", \"CalificaciÃ³n\"])\n",
    "\n",
    "existing_df['Fecha'] = pd.to_datetime(existing_df['Fecha'], errors='coerce')\n",
    "\n",
    "# ------------- SELENIUM -------------\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_argument(\"--window-size=1366,2400\")\n",
    "options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    \"(KHTML, like Gecko) Chrome/118.0.5993.70 Safari/537.36\"\n",
    ")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "wait = WebDriverWait(driver, 25)\n",
    "\n",
    "resultados = []\n",
    "reviews_by_company = {}\n",
    "\n",
    "try:\n",
    "    for empresa in empresas_data:\n",
    "        nombre_empresa = empresa.get(\"nombre\", \"N/A\")\n",
    "        pagina_web = normalizar_dominio(empresa.get(\"pagina_web\", \"\"))\n",
    "        if not pagina_web:\n",
    "            print(f\"âš ï¸ Empresa sin 'pagina_web': {nombre_empresa}. Omitida.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nğŸ” Verificando nuevas reseÃ±as de {nombre_empresa} ({pagina_web})...\")\n",
    "\n",
    "        # HistÃ³rico de esa empresa\n",
    "        old_df = existing_df[existing_df[\"Empresa\"] == nombre_empresa].copy()\n",
    "        if not old_df.empty:\n",
    "            old_df[\"TÃ­tulo\"] = old_df[\"TÃ­tulo\"].fillna(\"\").str.strip()\n",
    "            old_df[\"Fecha\"] = pd.to_datetime(old_df[\"Fecha\"], errors=\"coerce\")\n",
    "\n",
    "        reseÃ±as_nuevas = []\n",
    "\n",
    "        for page in range(1, MAX_PAGES + 1):\n",
    "            review_url = f\"https://es.trustpilot.com/review/{pagina_web}\" + (f\"?page={page}\" if page > 1 else \"\")\n",
    "            print(f\"ğŸŒ Accediendo a: {review_url}\")\n",
    "            driver.get(review_url)\n",
    "\n",
    "            # Aceptar cookies si estorban\n",
    "            aceptar_cookies_si_aparecen(driver, wait)\n",
    "\n",
    "            # Espera a que al menos haya 1 card de reseÃ±a en la pÃ¡gina\n",
    "            try:\n",
    "                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-testid=\"service-review-card-v2\"]')))\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ No se encontraron cards en pÃ¡gina {page}: {e}\")\n",
    "                if DEBUG:\n",
    "                    ss = f\"_debug_reviews_{nombre_empresa}_p{page}_nocards.png\"\n",
    "                    driver.save_screenshot(ss)\n",
    "                    with open(f\"_debug_reviews_{nombre_empresa}_p{page}_nocards.html\", \"w\", encoding=\"utf-8\") as fh:\n",
    "                        fh.write(driver.page_source)\n",
    "                    print(f\"ğŸ’¾ GuardÃ© evidencia sin cards: {ss}\")\n",
    "                break\n",
    "\n",
    "            # Forzar render con scrolls suaves (contenido lazy)\n",
    "            for _ in range(3):\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(1.2)\n",
    "\n",
    "            # Cards reales (bÃºsqueda global)\n",
    "            cards = driver.find_elements(By.CSS_SELECTOR, 'div[data-testid=\"service-review-card-v2\"]')\n",
    "\n",
    "            if DEBUG:\n",
    "                print(f\"ğŸ” Encontradas {len(cards)} cards en pÃ¡gina {page}\")\n",
    "                try:\n",
    "                    ss_name = f\"_debug_reviews_{nombre_empresa}_p{page}.png\"\n",
    "                    html_name = f\"_debug_reviews_{nombre_empresa}_p{page}.html\"\n",
    "                    driver.save_screenshot(ss_name)\n",
    "                    with open(html_name, \"w\", encoding=\"utf-8\") as fh:\n",
    "                        fh.write(driver.page_source)\n",
    "                    print(f\"ğŸ’¾ GuardÃ© screenshot y HTML: {ss_name} / {html_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ No pude guardar debug files: {e}\")\n",
    "\n",
    "            nuevas_esta_pagina = []\n",
    "\n",
    "            for i, card in enumerate(cards, start=1):\n",
    "                try:\n",
    "                    # Fecha\n",
    "                    try:\n",
    "                        time_el = card.find_element(By.CSS_SELECTOR, 'time[datetime]')\n",
    "                        fecha = pd.to_datetime(time_el.get_attribute(\"datetime\"), errors='coerce')\n",
    "                    except:\n",
    "                        fecha = pd.NaT\n",
    "\n",
    "                    # TÃ­tulo\n",
    "                    try:\n",
    "                        h2 = card.find_element(By.CSS_SELECTOR, 'h2[data-service-review-title-typography=\"true\"]')\n",
    "                        titulo = norm_text(h2.get_attribute(\"innerText\") or h2.text)\n",
    "                    except:\n",
    "                        titulo = \"N/A\"\n",
    "\n",
    "                    # Contenido\n",
    "                    try:\n",
    "                        p = card.find_element(By.CSS_SELECTOR, 'p[data-service-review-text-typography=\"true\"]')\n",
    "                        contenido = norm_text(p.get_attribute(\"innerText\") or p.text)\n",
    "                    except:\n",
    "                        contenido = \"Sin contenido\"\n",
    "\n",
    "                    # CalificaciÃ³n (solo nÃºmero)\n",
    "                    try:\n",
    "                        img = card.find_element(By.CSS_SELECTOR, 'img[alt*=\"Valorada con\"]')\n",
    "                        estrellas = parse_estrellas_from_alt(img.get_attribute(\"alt\"))\n",
    "                    except:\n",
    "                        estrellas = \"N/A\"\n",
    "\n",
    "                    # UID de la review (opcional)\n",
    "                    review_uid = extraer_review_uid(card)\n",
    "\n",
    "                    if DEBUG and i <= 3:\n",
    "                        print(f\"  â€¢ Card #{i}: fecha={fecha}, estrellas={estrellas}, tÃ­tulo='{titulo[:60]}', uid='{review_uid}'\")\n",
    "\n",
    "                    # DEDUP: TÃ­tulo + Fecha\n",
    "                    existe = (\n",
    "                        not old_df[\n",
    "                            (old_df[\"TÃ­tulo\"].str.strip() == titulo) &\n",
    "                            (old_df[\"Fecha\"] == fecha)\n",
    "                        ].empty\n",
    "                    )\n",
    "\n",
    "                    if not existe:\n",
    "                        nuevas_esta_pagina.append({\n",
    "                            \"Empresa\": nombre_empresa,\n",
    "                            \"TÃ­tulo\": titulo,\n",
    "                            \"Contenido\": contenido,\n",
    "                            \"Fecha\": fecha,\n",
    "                            \"CalificaciÃ³n\": estrellas\n",
    "                        })\n",
    "\n",
    "                except Exception as e_card:\n",
    "                    if DEBUG:\n",
    "                        print(f\"  â€¢ Aviso: card con error: {e_card}\")\n",
    "                    continue\n",
    "\n",
    "            if nuevas_esta_pagina:\n",
    "                reseÃ±as_nuevas.extend(nuevas_esta_pagina)\n",
    "                print(f\"âœ… {len(nuevas_esta_pagina)} reseÃ±as nuevas en pÃ¡gina {page}. Continuando...\")\n",
    "            else:\n",
    "                print(f\"ğŸ›‘ No hay reseÃ±as nuevas en pÃ¡gina {page}. Deteniendo scraping para esta empresa.\")\n",
    "                break\n",
    "\n",
    "        # Consolidar por empresa\n",
    "        if reseÃ±as_nuevas:\n",
    "            df_nuevas = pd.DataFrame(reseÃ±as_nuevas)\n",
    "            df_nuevas['Fecha'] = pd.to_datetime(df_nuevas['Fecha'], errors='coerce')\n",
    "            combined_df = pd.concat([old_df, df_nuevas], ignore_index=True)\n",
    "            combined_df.drop_duplicates(subset=[\"TÃ­tulo\", \"Fecha\"], keep='first', inplace=True)\n",
    "            combined_df.sort_values(by=\"Fecha\", ascending=False, inplace=True)\n",
    "            combined_df.reset_index(drop=True, inplace=True)\n",
    "            combined_df.fillna(\"\", inplace=True)\n",
    "\n",
    "            total = len(combined_df)\n",
    "            combined_df[\"id_reseÃ±a\"] = [f\"{nombre_empresa.replace(' ', '')}_N{total - idx}\" for idx in range(total)]\n",
    "\n",
    "            reviews_by_company[nombre_empresa] = combined_df.to_dict(orient=\"records\")\n",
    "            resultados.append(combined_df)\n",
    "        else:\n",
    "            if not old_df.empty:\n",
    "                reviews_by_company[nombre_empresa] = old_df.to_dict(orient=\"records\")\n",
    "            print(f\"ğŸ“­ No se detectaron nuevas reseÃ±as para {nombre_empresa}.\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# ------------- SALIDA CSV -------------\n",
    "if resultados:\n",
    "    final_df = pd.concat([existing_df] + resultados, ignore_index=True)\n",
    "    final_df.drop_duplicates(subset=[\"TÃ­tulo\", \"Fecha\", \"Empresa\"], keep='first', inplace=True)\n",
    "    final_df.fillna(\"\", inplace=True)\n",
    "\n",
    "    final_df[\"Fecha\"] = pd.to_datetime(final_df[\"Fecha\"], errors=\"coerce\")\n",
    "    final_df.sort_values(by=[\"Empresa\", \"Fecha\"], ascending=[True, False], inplace=True)\n",
    "\n",
    "    final_df[\"id_reseÃ±a\"] = final_df.groupby(\"Empresa\").cumcount(ascending=False) + 1\n",
    "    final_df[\"id_reseÃ±a\"] = final_df.apply(\n",
    "        lambda row: f\"{row['Empresa'].replace(' ', '')}_N{row['id_reseÃ±a']}\", axis=1\n",
    "    )\n",
    "\n",
    "    final_df = final_df[[\"id_reseÃ±a\", \"Fecha\", \"TÃ­tulo\", \"Contenido\", \"Empresa\", \"CalificaciÃ³n\"]]\n",
    "\n",
    "    final_df[\"id_num\"] = final_df[\"id_reseÃ±a\"].str.extract(r'_N(\\d+)').astype(int)\n",
    "    final_df.sort_values(by=[\"Empresa\", \"id_num\"], ascending=[True, False], inplace=True)\n",
    "    final_df.drop(columns=[\"id_num\"], inplace=True)\n",
    "\n",
    "    os.makedirs(os.path.dirname(DATASET_CSV), exist_ok=True)\n",
    "    final_df.to_csv(DATASET_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\nğŸ‰ Dataset actualizado guardado en '{DATASET_CSV}'\")\n",
    "else:\n",
    "    print(\"\\nâœ… No hubo actualizaciones, el dataset se mantiene igual.\")\n",
    "\n",
    "# ------------- SALIDA JSON (anidado por empresa) -------------\n",
    "for empresa in empresas_data:\n",
    "    nombre_empresa = empresa.get(\"nombre\", \"N/A\")\n",
    "    empresa[\"reseÃ±as\"] = reviews_by_company.get(nombre_empresa, [])\n",
    "\n",
    "os.makedirs(os.path.dirname(SALIDA_JSON), exist_ok=True)\n",
    "with open(SALIDA_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(empresas_data, f, ensure_ascii=False, indent=4, default=str)\n",
    "\n",
    "print(f\"ğŸ“¦ Archivo JSON guardado en '{SALIDA_JSON}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Unir los dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV fusionado guardado como dataset_reviews.csv\n",
      "JSON fusionado guardado como dataset_reviews.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Cargar los datasets CSV\n",
    "df1 = pd.read_csv(\"datalake/1_LANDING_ZONE/dataset_categoria1.csv\")\n",
    "df2 = pd.read_csv(\"datalake/1_LANDING_ZONE/dataset_categoria2.csv\")\n",
    "df3 = pd.read_csv(\"datalake/1_LANDING_ZONE/dataset_categoria3.csv\") \n",
    "df4 = pd.read_csv(\"datalake/1_LANDING_ZONE/dataset_categoria4.csv\") \n",
    " # Nuevo dataset\n",
    "\n",
    "# Agregar columna 'categoria'\n",
    "df1[\"categoria\"] = \"Banco\"\n",
    "df2[\"categoria\"] = \"Seguros de viaje\"\n",
    "df3[\"categoria\"] = \"Concesionario de autos\"  # Nueva categorÃ­a\n",
    "df4[\"categoria\"] = \"Tienda de ropa\"  # Nueva categorÃ­a\n",
    "\n",
    "# Reordenar las columnas para que 'categoria' aparezca al inicio\n",
    "def reorder_columns(df):\n",
    "    return df[[\"categoria\"] + [col for col in df.columns if col != \"categoria\"]]\n",
    "\n",
    "df1 = reorder_columns(df1)\n",
    "df2 = reorder_columns(df2)\n",
    "df3 = reorder_columns(df3)\n",
    "df4 = reorder_columns(df4)\n",
    "\n",
    "# Combinar todos los DataFrames\n",
    "merged_csv = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "# Guardar el resultado en un nuevo archivo CSV\n",
    "merged_csv.to_csv(\"datalake/1_LANDING_ZONE/dataset_reviews.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"CSV fusionado guardado como dataset_reviews.csv\")\n",
    "\n",
    "################### JSON ###################\n",
    "\n",
    "# Cargar los archivos JSON de entrada\n",
    "with open(\"datalake/1_LANDING_ZONE/reviews_trustpilot_empresas_categoria1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data1 = json.load(f)\n",
    "with open(\"datalake/1_LANDING_ZONE/reviews_trustpilot_empresas_categoria2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data2 = json.load(f)\n",
    "with open(\"datalake/1_LANDING_ZONE/reviews_trustpilot_empresas_categoria3.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data3 = json.load(f)\n",
    "with open(\"datalake/1_LANDING_ZONE/reviews_trustpilot_empresas_categoria4.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data4 = json.load(f)\n",
    " \n",
    "\n",
    "# Agregar la clave 'categoria' a cada empresa\n",
    "for company in data1:\n",
    "    company[\"categoria\"] = \"Bancos\"\n",
    "for company in data2:\n",
    "    company[\"categoria\"] = \"Seguro de viajes\"\n",
    "for company in data3:\n",
    "    company[\"categoria\"] = \"Concesionario de autos\"\n",
    "for company in data4:\n",
    "    company[\"categoria\"] = \"Tienda de ropa\"\n",
    "\n",
    "\n",
    "# Combinar las listas de empresas\n",
    "merged_data = data1 + data2 + data3 + data4\n",
    "\n",
    "# Reordenar las claves para que 'categoria' aparezca al inicio\n",
    "ordered_merged_data = []\n",
    "for company in merged_data:\n",
    "    ordered_company = OrderedDict()\n",
    "    ordered_company[\"categoria\"] = company.get(\"categoria\", \"\")\n",
    "    for key in company:\n",
    "        if key != \"categoria\":\n",
    "            ordered_company[key] = company[key]\n",
    "    ordered_merged_data.append(ordered_company)\n",
    "\n",
    "# Guardar el JSON combinado en un nuevo archivo\n",
    "with open(\"datalake/1_LANDING_ZONE/dataset_reviews.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ordered_merged_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"JSON fusionado guardado como dataset_reviews.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Data Cleaning & Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos limpios y validados guardados en datalake/2_REFINED_ZONE/dataset_reviews_limpio.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from datetime import datetime, timezone\n",
    "import pytz\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Definir rutas de archivos\n",
    "input_file = \"datalake/1_LANDING_ZONE/dataset_reviews.json\"\n",
    "output_file = \"datalake/2_REFINED_ZONE/dataset_reviews_limpio.json\"\n",
    "\n",
    "# Configurar zona horaria de Bolivia (America/La_Paz)\n",
    "bolivia_tz = pytz.timezone(\"America/La_Paz\")\n",
    "\n",
    "def clean_puntuacion(puntuacion):\n",
    "    \"\"\"\n",
    "    Devuelve la puntuaciÃ³n como float (>0) o \"\" si no es vÃ¡lida.\n",
    "    - Soporta float/int directamente.\n",
    "    - Si es str, elimina 'TrustScore', cambia coma por punto y castea a float.\n",
    "    \"\"\"\n",
    "    if puntuacion is None:\n",
    "        return \"\"\n",
    "    # Si ya es numÃ©rico\n",
    "    if isinstance(puntuacion, (int, float)):\n",
    "        try:\n",
    "            val = float(puntuacion)\n",
    "            return val if val > 0 else \"\"\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    # Si es texto\n",
    "    if isinstance(puntuacion, str):\n",
    "        cleaned = puntuacion.replace(\"TrustScore\", \"\").strip().replace(\",\", \".\")\n",
    "        # Si vienen otras palabras, intenta extraer el primer nÃºmero\n",
    "        m = re.search(r\"(\\d+(?:\\.\\d+)?)\", cleaned)\n",
    "        if m:\n",
    "            try:\n",
    "                val = float(m.group(1))\n",
    "                return val if val > 0 else \"\"\n",
    "            except ValueError:\n",
    "                return \"\"\n",
    "        try:\n",
    "            val = float(cleaned)\n",
    "            return val if val > 0 else \"\"\n",
    "        except ValueError:\n",
    "            return \"\"\n",
    "    # Cualquier otro tipo\n",
    "    return \"\"\n",
    "\n",
    "def convert_datetime(dt_str):\n",
    "    \"\"\"\n",
    "    Convierte a zona Bolivia y retorna (fecha_local, hora_local, iso_bolivia).\n",
    "    Acepta strings ISO como '2025-11-08 13:51:34+00:00' o '2025-11-08T13:51:34+00:00'.\n",
    "    Si es naive, asume UTC.\n",
    "    \"\"\"\n",
    "    if not dt_str:\n",
    "        return \"\", \"\", \"\"\n",
    "    s = str(dt_str)\n",
    "    # Normaliza separador\n",
    "    if \" \" in s and \"T\" not in s:\n",
    "        s = s.replace(\" \", \"T\", 1)\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(s)\n",
    "    except Exception:\n",
    "        return \"\", \"\", \"\"\n",
    "    # Si es naive, asumimos UTC\n",
    "    if dt.tzinfo is None:\n",
    "        dt = dt.replace(tzinfo=timezone.utc)\n",
    "    try:\n",
    "        dt_bolivia = dt.astimezone(bolivia_tz)\n",
    "        fecha_local = dt_bolivia.strftime(\"%Y-%m-%d\")\n",
    "        hora_local = dt_bolivia.strftime(\"%H:%M:%S\")\n",
    "        return fecha_local, hora_local, dt_bolivia.isoformat()\n",
    "    except Exception:\n",
    "        return \"\", \"\", \"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    MinÃºsculas, quita acentos, remueve caracteres no alfanumÃ©ricos (y emojis),\n",
    "    colapsa espacios.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Cargar el archivo JSON de entrada\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Procesar cada empresa en el dataset\n",
    "for company in data:\n",
    "    # Asegurar que los campos de nivel empresa sean textos (cuando apliquen)\n",
    "    for key in [\"categoria\", \"nombre\", \"ubicacion\", \"pagina_web\"]:\n",
    "        if key in company and company[key] is not None:\n",
    "            company[key] = str(company[key]).strip()\n",
    "\n",
    "    # Si la ubicaciÃ³n es \"N/A\", reemplazar por \"sin ubicacion\"\n",
    "    if company.get(\"ubicacion\", \"\").strip().upper() == \"N/A\":\n",
    "        company[\"ubicacion\"] = \"sin ubicacion\"\n",
    "\n",
    "    # Limpiar el campo 'puntuacion' (ahora puede ser float)\n",
    "    if \"puntuacion\" in company:\n",
    "        company[\"puntuacion\"] = clean_puntuacion(company[\"puntuacion\"])\n",
    "\n",
    "    # Procesar las reseÃ±as\n",
    "    if \"reseÃ±as\" in company and isinstance(company[\"reseÃ±as\"], list):\n",
    "        for review in company[\"reseÃ±as\"]:\n",
    "            # Normaliza campos base a string\n",
    "            for key in [\"id_reseÃ±a\", \"TÃ­tulo\", \"Contenido\", \"Empresa\"]:\n",
    "                if key in review and review[key] is not None:\n",
    "                    review[key] = str(review[key]).strip()\n",
    "\n",
    "            # Limpiar TÃ­tulo y Contenido\n",
    "            if \"TÃ­tulo\" in review:\n",
    "                review[\"TÃ­tulo\"] = clean_text(review[\"TÃ­tulo\"])\n",
    "            if \"Contenido\" in review:\n",
    "                review[\"Contenido\"] = clean_text(review[\"Contenido\"])\n",
    "\n",
    "            # Convertir fecha a zona Bolivia\n",
    "            if \"Fecha\" in review and review[\"Fecha\"]:\n",
    "                fecha_local, hora_local, iso_bolivia = convert_datetime(review[\"Fecha\"])\n",
    "                # Usa minÃºsculas para consistencia con tu dashboard\n",
    "                review[\"fecha_local\"] = fecha_local\n",
    "                review[\"hora_local\"] = hora_local\n",
    "                # Si te sirve conservar el ISO local, puedes guardarlo en otro campo:\n",
    "                # review[\"fecha_iso_bolivia\"] = iso_bolivia\n",
    "\n",
    "            # Validar \"CalificaciÃ³n\": entero > 0, tolerando 'N/A', '', etc.\n",
    "            if \"CalificaciÃ³n\" in review:\n",
    "                try:\n",
    "                    cal = int(str(review[\"CalificaciÃ³n\"]).strip())\n",
    "                    review[\"CalificaciÃ³n\"] = cal if cal > 0 else \"\"\n",
    "                except Exception:\n",
    "                    review[\"CalificaciÃ³n\"] = \"\"\n",
    "\n",
    "            # Eliminar campos no requeridos\n",
    "            review.pop(\"Fecha\", None)     # ya convertida a fecha_local/hora_local\n",
    "            review.pop(\"Empresa\", None)   # evitar redundancia\n",
    "\n",
    "# Guardar los datos limpios en formato JSON\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4, default=str)\n",
    "\n",
    "print(f\"Datos limpios y validados guardados en {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subir en la base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Data Lake Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!streamlit run datalake/3_CONSUMPTION_ZONE/app.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd C:\\Users\\DANIEL\\Downloads\\M7_PROYECTO_DANIEL_SANCHEZ\\M7_PROYECTO_DANIEL_SANCHEZ\n",
    "#streamlit run datalake\\3_CONSUMPTION_ZONE\\app.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
